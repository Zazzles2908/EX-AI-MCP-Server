# ğŸ§ª EX-AI MCP Server - Comprehensive Testing Strategy

**Date:** 2025-10-05  
**Version:** 2.0  
**Status:** Active

---

## ğŸ“‹ OVERVIEW

The EX-AI MCP Server uses a **dual testing strategy** to ensure comprehensive validation of both the MCP protocol layer and the underlying provider APIs.

### Two Testing Systems

1. **MCP Integration Tests** (`tests/` directory)
   - Tests the complete MCP stack (protocol â†’ server â†’ tools â†’ providers)
   - Uses pytest framework
   - 40+ existing test files

2. **Provider API Tests** (`tool_validation_suite/` directory)
   - Tests provider APIs directly (bypasses MCP layer)
   - Uses custom test runner with GLM Watcher
   - 70% complete (utilities done, test scripts needed)

---

## ğŸ¯ WHEN TO USE EACH SYSTEM

### Use `tests/` (MCP Integration) When:

âœ… Testing MCP protocol compliance  
âœ… Testing server startup/shutdown  
âœ… Testing tool registration and discovery  
âœ… Testing WebSocket daemon + shim  
âœ… Testing configuration loading  
âœ… Testing routing logic  
âœ… Testing end-to-end workflows through MCP  
âœ… Testing both stdio and WebSocket modes  
âœ… Testing concurrent client connections  

**Example:** "Does the chat tool work when called through MCP?"

### Use `tool_validation_suite/` (Provider API) When:

âœ… Testing direct provider API integration  
âœ… Testing feature activation (web search, file upload, thinking mode)  
âœ… Testing conversation management and platform isolation  
âœ… Testing cost tracking and limits  
âœ… Benchmarking performance  
âœ… Validating provider-specific behavior  
âœ… Getting independent validation via GLM Watcher  

**Example:** "Does Kimi web search activate correctly with the right parameters?"

---

## ğŸ—ï¸ ARCHITECTURE COMPARISON

### MCP Integration Tests (tests/)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FULL STACK TESTING                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Test Script (pytest)                                        â”‚
â”‚      â†“                                                        â”‚
â”‚  MCP Client (stdio or WebSocket)                             â”‚
â”‚      â†“                                                        â”‚
â”‚  server.py / ws_server.py                                    â”‚
â”‚      â†“                                                        â”‚
â”‚  MCP Handlers (handle_call_tool, etc.)                       â”‚
â”‚      â†“                                                        â”‚
â”‚  Tool Registry & Tool Execution                              â”‚
â”‚      â†“                                                        â”‚
â”‚  Provider APIs (Kimi/GLM)                                    â”‚
â”‚      â†“                                                        â”‚
â”‚  Response Validation                                         â”‚
â”‚                                                               â”‚
â”‚  âœ… Tests: Protocol, Server, Tools, Providers                â”‚
â”‚  âœ… Modes: stdio and WebSocket                               â”‚
â”‚  âœ… Framework: pytest                                        â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Provider API Tests (tool_validation_suite/)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PROVIDER API TESTING                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Test Script (custom)                                        â”‚
â”‚      â†“                                                        â”‚
â”‚  APIClient (utils/api_client.py)                             â”‚
â”‚      â†“                                                        â”‚
â”‚  Provider APIs (Kimi/GLM) - DIRECT                           â”‚
â”‚      â†“                                                        â”‚
â”‚  Response Validation                                         â”‚
â”‚      â†“                                                        â”‚
â”‚  GLM Watcher (Independent Validation)                        â”‚
â”‚      â†“                                                        â”‚
â”‚  Cost Tracking & Performance Monitoring                      â”‚
â”‚                                                               â”‚
â”‚  âœ… Tests: Provider APIs, Features, Cost, Performance        â”‚
â”‚  âœ… Validation: Independent GLM Watcher                      â”‚
â”‚  âœ… Framework: Custom TestRunner                             â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š COVERAGE MATRIX

### What Each System Tests

| Component | MCP Tests (tests/) | Provider Tests (tool_val) |
|-----------|-------------------|---------------------------|
| **MCP Protocol** | âœ… 80% | âŒ 0% (bypassed) |
| **Server Handlers** | âœ… 75% | âŒ 0% (bypassed) |
| **Tool Registration** | âœ… 70% | âŒ 0% (bypassed) |
| **WebSocket Daemon** | âœ… 60% | âŒ 0% (bypassed) |
| **Routing Logic** | âœ… 80% | âŒ 0% (bypassed) |
| **Configuration** | âœ… 85% | âŒ 0% (bypassed) |
| **Provider APIs** | âœ… 70% (through MCP) | â³ 0% (direct) |
| **Feature Activation** | âš ï¸ 40% | â³ 0% (comprehensive) |
| **Cost Tracking** | âŒ 0% | âœ… 100% |
| **Performance** | âš ï¸ 30% | âœ… 100% |
| **Platform Isolation** | âŒ 0% | âœ… 100% |
| **Independent Validation** | âŒ 0% | âœ… 100% (GLM Watcher) |

**Combined Coverage:** ~85% (when both systems complete)

---

## ğŸš€ RUNNING TESTS

### Run MCP Integration Tests

```bash
# All MCP tests
python run_tests.py --category all

# Specific categories
python run_tests.py --category mcp_protocol
python run_tests.py --category routing
python run_tests.py --category providers
python run_tests.py --category e2e

# With pytest directly
pytest tests/ -v
pytest tests/phase8/ -v
pytest tests/week3/ -v

# Specific markers
pytest -m integration
pytest -m e2e
pytest -m performance
```

### Run Provider API Tests

```bash
# All provider tests
cd tool_validation_suite
python scripts/run_all_tests.py

# Specific categories
python scripts/run_core_tests.py
python scripts/run_provider_tests.py --provider kimi
python scripts/run_provider_tests.py --provider glm

# Specific tool
python scripts/run_all_tests.py --tool chat
python scripts/run_all_tests.py --tool analyze
```

### Run Both Systems

```bash
# Unified test runner (to be created)
python run_all_tests_unified.py

# Or run separately
python run_tests.py --category all
cd tool_validation_suite && python scripts/run_all_tests.py
```

---

## ğŸ“ DIRECTORY STRUCTURE

```
EX-AI-MCP-Server/
â”œâ”€â”€ tests/                          # MCP Integration Tests
â”‚   â”œâ”€â”€ phase2/                     # Phase 2 tests
â”‚   â”œâ”€â”€ phase3/                     # Phase 3 tests
â”‚   â”œâ”€â”€ phase4/                     # Phase 4 tests
â”‚   â”œâ”€â”€ phase5/                     # Phase 5 tests
â”‚   â”œâ”€â”€ phase6/                     # Phase 6 tests
â”‚   â”œâ”€â”€ phase7/                     # Phase 7 tests
â”‚   â”œâ”€â”€ phase8/                     # Phase 8 tests
â”‚   â”œâ”€â”€ week1/                      # Week 1 tests
â”‚   â”œâ”€â”€ week2/                      # Week 2 tests
â”‚   â”œâ”€â”€ week3/                      # Week 3 tests
â”‚   â””â”€â”€ docs/                       # Documentation tests
â”‚
â”œâ”€â”€ tool_validation_suite/          # Provider API Tests
â”‚   â”œâ”€â”€ config/                     # Test configuration
â”‚   â”œâ”€â”€ scripts/                    # Test runners
â”‚   â”œâ”€â”€ utils/                      # Test utilities (11 modules)
â”‚   â”œâ”€â”€ tests/                      # Test scripts (to be created)
â”‚   â”‚   â”œâ”€â”€ core_tools/            # 15 core tool tests
â”‚   â”‚   â”œâ”€â”€ advanced_tools/        # 7 advanced tool tests
â”‚   â”‚   â”œâ”€â”€ provider_tools/        # 8 provider tool tests
â”‚   â”‚   â””â”€â”€ integration/           # 6 integration tests
â”‚   â””â”€â”€ results/                    # Test results and reports
â”‚
â”œâ”€â”€ pytest.ini                      # pytest configuration
â”œâ”€â”€ run_tests.py                    # MCP test runner
â””â”€â”€ TESTING_STRATEGY.md            # This file
```

---

## ğŸ”§ TEST DEVELOPMENT GUIDELINES

### For MCP Integration Tests (tests/)

1. **Use pytest framework**
   ```python
   import pytest
   
   @pytest.mark.integration
   async def test_chat_through_mcp():
       from tools.chat import ChatTool
       tool = ChatTool()
       # Test through MCP protocol
   ```

2. **Test both execution modes**
   - stdio mode (direct server.py)
   - WebSocket mode (daemon + shim)

3. **Use appropriate markers**
   - `@pytest.mark.unit` - Unit tests
   - `@pytest.mark.integration` - Integration tests
   - `@pytest.mark.e2e` - End-to-end tests
   - `@pytest.mark.performance` - Performance tests

4. **Follow existing patterns**
   - Look at tests/phase8/ for examples
   - Use monkeypatch for environment variables
   - Mock external dependencies when appropriate

### For Provider API Tests (tool_validation_suite/)

1. **Use TestRunner framework**
   ```python
   from utils import TestRunner
   
   def test_chat_basic(api_client, **kwargs):
       return api_client.call_kimi(
           model="kimi-k2-0905-preview",
           messages=[{"role": "user", "content": "test"}],
           tool_name="chat",
           variation="basic_functionality"
       )
   ```

2. **Test all 12 variations**
   - basic_functionality
   - edge_cases
   - error_handling
   - file_handling
   - model_selection
   - continuation
   - timeout_handling
   - progress_reporting
   - web_search
   - file_upload
   - conversation_id_persistence
   - conversation_id_isolation

3. **Use real API calls**
   - No mocks (tests actual provider behavior)
   - Track costs with PromptCounter
   - Monitor performance with PerformanceMonitor

4. **Leverage GLM Watcher**
   - Independent validation of test results
   - Quality scores and suggestions
   - Anomaly detection

---

## ğŸ“ˆ SUCCESS CRITERIA

### MCP Integration Tests

âœ… 90%+ pass rate  
âœ… All tools discoverable via MCP  
âœ… Both stdio and WebSocket modes work  
âœ… No protocol violations  
âœ… Configuration loads correctly  
âœ… Routing logic works as expected  

### Provider API Tests

âœ… 90%+ pass rate  
âœ… All 30 tools tested with 12 variations  
âœ… Cost under $5 for full suite  
âœ… GLM Watcher observations collected  
âœ… Performance metrics within thresholds  
âœ… Platform isolation verified  

### Combined

âœ… 85%+ overall system coverage  
âœ… Both daemon and MCP modes validated  
âœ… No critical bugs detected  
âœ… All reports generated successfully  

---

## ğŸ› DEBUGGING FAILED TESTS

### MCP Integration Test Failures

1. **Check server logs**
   ```bash
   tail -f logs/server.log
   tail -f logs/toolcalls.jsonl
   ```

2. **Verify configuration**
   ```bash
   python scripts/diagnose_mcp.py
   ```

3. **Test WebSocket daemon**
   ```bash
   python scripts/ws/ws_status.py
   python scripts/ws/ws_chat_once.py kimi-k2-0905-preview "test"
   ```

4. **Run specific test with verbose output**
   ```bash
   pytest tests/phase8/test_workflows_end_to_end.py -v --tb=long
   ```

### Provider API Test Failures

1. **Check API keys**
   ```bash
   cd tool_validation_suite
   python scripts/validate_setup.py
   ```

2. **Review test logs**
   ```bash
   cat tool_validation_suite/results/latest/test_logs/*.log
   ```

3. **Check GLM Watcher observations**
   ```bash
   cat tool_validation_suite/results/latest/watcher_observations/*.json
   ```

4. **Review cost tracking**
   ```bash
   cat tool_validation_suite/results/latest/cost_summary.json
   ```

---

## ğŸ“š RELATED DOCUMENTATION

**MCP Integration Tests:**
- `tests/` directory - Existing test files
- `pytest.ini` - pytest configuration
- `run_tests.py` - Test runner

**Provider API Tests:**
- `tool_validation_suite/NEXT_AGENT_HANDOFF.md` - Complete context
- `tool_validation_suite/ARCHITECTURE.md` - System design
- `tool_validation_suite/TESTING_GUIDE.md` - How to run tests
- `tool_validation_suite/CORRECTED_AUDIT_FINDINGS.md` - Audit results

**General:**
- `TESTING_STRATEGY.md` - This file
- `docs/reviews/augment_code_review/03_testing/` - Testing documentation

---

## âœ… NEXT STEPS

1. **Complete tool_validation_suite test scripts** (4-6 hours)
2. **Integrate both testing systems** (1-2 hours)
3. **Run full test suite** (1-2 hours)
4. **Analyze results and fix issues** (2-3 hours)
5. **Document findings** (1 hour)

**Total Time:** 9-14 hours

---

**Testing Strategy Active** âœ…  
**Last Updated:** 2025-10-05  
**Maintained By:** EX-AI MCP Server Team

