# TIMEOUT CASCADE ANALYSIS - OPTION D FAILURE

**Date:** 2025-10-16 09:00-09:08
**Status:** üî¥ CRITICAL ISSUE IDENTIFIED
**Impact:** EXAI chat tool experiencing cascading timeouts and fallback failures

---

## üî¥ WHAT HAPPENED

When attempting **Option D** (Performance Tracking System implementation), the EXAI chat tool experienced a **cascading timeout failure** that took **4 minutes** to resolve through multiple fallback attempts.

---

## üìä TIMELINE OF EVENTS

### 09:00:07 - Initial Call (GLM-4.6)
```
Tool: chat
Model: glm-4.6 (GLM provider)
Web Search: Enabled
Prompt: Performance tracking system implementation (1,885 tokens)
```

### 09:00:08 - GLM Streaming Started
```
HTTP Request: POST https://api.z.ai/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
```

### 09:00:41 - **GLM TIMEOUT (33 seconds)**
```
ERROR: GLM generate_content failed: GLM SDK streaming failed: The read operation timed out
WARNING: Explicit model call failed; entering fallback chain
```

### 09:00:41 - Fallback #1: Kimi K2-0905-preview
```
INFO: Using fallback chain for category FAST_RESPONSE
Model: kimi-k2-0905-preview
Web Search: Enabled ($web_search)
```

### 09:01:11 - **Kimi Retry #1 (30 seconds)**
```
INFO: Retrying request to /chat/completions in 0.485412 seconds
```

### 09:01:42 - **Kimi Retry #2 (31 seconds)**
```
INFO: Retrying request to /chat/completions in 0.914305 seconds
```

### 09:02:13 - **Kimi K2-0905 TIMEOUT (92 seconds total)**
```
ERROR: OpenAI Compatible chat completion for kimi-k2-0905-preview error after 1 attempt: Request timed out.
```

### 09:02:13 - Fallback #2: Kimi K2-0711-preview
```
Model: kimi-k2-0711-preview
Web Search: Enabled ($web_search)
```

### 09:03:45 - **Kimi K2-0711 TOKEN LIMIT EXCEEDED (92 seconds)**
```
ERROR: OpenAI Compatible chat completion for moonshot-v1-8k error after 1 attempt: 
Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 8192'}}
```

**CRITICAL:** System prompt was **81,192 tokens** - exceeded moonshot-v1-8k limit!

### 09:03:45 - Fallback #3: Moonshot-v1-32k
```
Model: moonshot-v1-32k
Web Search: Enabled ($web_search)
```

### 09:04:14 - **SUCCESS (29 seconds)**
```
HTTP Request: POST https://api.moonshot.ai/v1/chat/completions "HTTP/1.1 200 OK"
INFO: Received response from kimi API for chat
```

### 09:04:15 - **DUPLICATE CALL DETECTED**
```
Same prompt sent again immediately after success
Tool: chat (same 1,885 token prompt)
Model: glm-4.6
```

### 09:04:16 - GLM Streaming Started (Again)
```
HTTP Request: POST https://api.z.ai/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
```

### 09:08:09 - **SECOND CALL SUCCESS (3m 53s)**
```
INFO: Received response from glm API for chat
Conversation ID: 1a0bb0a7-ae89-47d3-bcbd-72d3b381ce18
```

---

## üîç ROOT CAUSE ANALYSIS

### 1. **System Prompt Bloat (81,192 tokens)**
The system prompt sent to Kimi models was **81,192 tokens** - this is:
- **10x larger** than moonshot-v1-8k limit (8,192 tokens)
- **2.5x larger** than moonshot-v1-32k limit (32,768 tokens)
- **Likely caused by** accumulated conversation history or repeated instructions

### 2. **GLM Streaming Timeout (33 seconds)**
GLM-4.6 streaming timed out after 33 seconds:
- Web search enabled with complex prompt
- Possible network latency or API slowdown
- No retry mechanism for GLM streaming failures

### 3. **Kimi Fallback Timeouts (92 seconds each)**
Both Kimi K2 models timed out:
- kimi-k2-0905-preview: 92 seconds (2 retries)
- kimi-k2-0711-preview: Token limit exceeded before timeout

### 4. **Duplicate Call Issue**
After successful moonshot-v1-32k response, the **same prompt was sent again** to GLM-4.6:
- Suggests client-side retry or duplicate request
- Took additional 3m 53s to complete
- Created unnecessary load and delay

---

## üí• IMPACT

**Total Time:** 8 minutes 2 seconds (09:00:07 - 09:08:09)
- First attempt: 4 minutes 7 seconds (failed 3 times, succeeded on 4th)
- Second attempt: 3 minutes 53 seconds (succeeded)

**Cascading Failures:**
1. GLM-4.6 timeout (33s)
2. Kimi K2-0905 timeout (92s)
3. Kimi K2-0711 token limit (92s)
4. Moonshot-v1-32k success (29s)
5. **Duplicate GLM-4.6 call** (233s)

**User Experience:**
- 8+ minute wait for simple chat response
- Multiple timeout errors
- Unclear why it's taking so long
- Appears "stuck" or "frozen"

---

## üö® CRITICAL ISSUES IDENTIFIED

### Issue #1: System Prompt Bloat (CRITICAL)
**Problem:** System prompt is 81,192 tokens (10x normal size)
**Impact:** Exceeds token limits, causes failures, wastes tokens/money
**Root Cause:** Likely accumulated conversation history or repeated instructions
**Fix Required:** Investigate system prompt generation and trim to <2,000 tokens

### Issue #2: No Timeout Configuration (HIGH)
**Problem:** Hardcoded 30-second timeouts insufficient for web search + complex prompts
**Impact:** Premature timeouts trigger unnecessary fallbacks
**Fix Required:** Implement adaptive timeouts based on prompt complexity and web search

### Issue #3: Fallback Chain Too Aggressive (MEDIUM)
**Problem:** Fallback chain tries 4 models before giving up
**Impact:** 4+ minute delays before user sees error
**Fix Required:** Limit fallback attempts to 2 models max, fail faster

### Issue #4: Duplicate Requests (MEDIUM)
**Problem:** Same prompt sent twice after first success
**Impact:** Wastes time, tokens, and API quota
**Fix Required:** Investigate client-side retry logic or duplicate detection

### Issue #5: No Progress Indicators (LOW)
**Problem:** User has no visibility into fallback attempts or progress
**Impact:** Appears frozen, user doesn't know what's happening
**Fix Required:** Add progress indicators for long-running operations

---

## üîß RECOMMENDED FIXES

### Immediate (Today):
1. **Investigate system prompt bloat** - Find why it's 81K tokens
2. **Add timeout configuration** - Make timeouts configurable per model
3. **Limit fallback chain** - Max 2 fallback attempts

### Short-term (This Week):
4. **Add duplicate detection** - Prevent same prompt being sent twice
5. **Implement progress indicators** - Show fallback attempts to user
6. **Add token limit validation** - Check before sending to API

### Long-term (Next Sprint):
7. **Implement performance tracking** - Track timeout patterns
8. **Add circuit breaker** - Temporarily disable failing providers
9. **Optimize system prompts** - Reduce token usage across all tools

---

## üìù LESSONS LEARNED

1. **System prompts must be monitored** - 81K tokens is unacceptable
2. **Timeouts need to be adaptive** - Web search + complex prompts need more time
3. **Fallback chains need limits** - 4+ minutes is too long to wait
4. **Duplicate detection is essential** - Prevents wasted API calls
5. **User visibility is critical** - Show what's happening during long operations

---

## üéØ NEXT STEPS

**Priority 1: Fix System Prompt Bloat**
- Investigate why system prompt is 81K tokens
- Identify source of bloat (conversation history? repeated instructions?)
- Implement trimming/compression to <2K tokens

**Priority 2: Add Timeout Configuration**
- Move timeouts from hardcoded to .env configuration
- Implement adaptive timeouts based on prompt complexity
- Add separate timeouts for streaming vs non-streaming

**Priority 3: Limit Fallback Chain**
- Reduce fallback attempts from 4 to 2
- Fail faster with clear error messages
- Add circuit breaker for repeatedly failing providers

---

**Document Status:** UPDATED - FIXES DEPLOYED
**Next Action:** Test fixes and validate token count reduction
**Owner:** EXAI Development Team
**Severity:** üü° MEDIUM - Fixes deployed, awaiting validation

---

## ‚úÖ FIXES DEPLOYED (2025-10-16 09:24:49)

### Fix 1: System Prompt Duplication Bug - DEPLOYED ‚úÖ
**File**: `tools/simple/base.py` line 1133
**Problem**: System prompt was being sent TWICE - once in system role, once in user role
**Root Cause**: `build_standard_prompt()` method incorrectly included system_prompt in user message

**Fix Applied**:
```python
# BEFORE (BUGGY):
full_prompt = f"""{system_prompt}{websearch_instruction}
=== USER REQUEST ===
{user_content}
=== END REQUEST ==="""

# AFTER (FIXED):
# FIXED 2025-10-16: Do NOT include system prompt in user message!
# System prompt goes in the system role, user content goes in user role
full_prompt = f"""{websearch_instruction}
=== USER REQUEST ===
{user_content}
=== END REQUEST ==="""
```

**Expected Impact**: Token count reduction from 81K ‚Üí ~2-3K tokens

### Fix 2: Simplified Fallback Chain - DEPLOYED ‚úÖ
**File**: `src/providers/registry_selection.py`
**Problem**: Aggressive fallback chain pulling ALL models from ALL providers
**Impact**: 8+ minute cascading failures (GLM ‚Üí Kimi K2-0905 ‚Üí Kimi K2-0711 ‚Üí moonshot-v1-8k ‚Üí moonshot-v1-32k)

**Fix Applied**:
- Implemented category-specific fallback chains
- `FAST_RESPONSE` category now ONLY tries GLM models: `["glm-4.6", "glm-4.5-flash", "glm-4.5"]`
- No cross-provider fallback for fast response tools
- Limited fallback attempts to **maximum 2 attempts** (down from unlimited)

**Expected Impact**: Fast failure (2 attempts max) instead of 8-minute cascades

### Deployment Status
- ‚úÖ Code fixed in both files
- ‚úÖ Docker image rebuilt (3.7s)
- ‚úÖ Container restarted successfully
- ‚úÖ Server running on ws://0.0.0.0:8079
- ‚è≥ Awaiting validation testing

---

## üîç ADDITIONAL FINDINGS

### Redis Configuration
- **Container Name**: `exai-redis` (not `redis`)
- **Status**: Running Redis 7.4.6 on port 6379
- **Persistence**: Using AOF (Append-Only File)
- **Memory Usage**: ~1 MB
- **Security Warning**: Detected possible SECURITY ATTACK (POST/Host commands) at 09:16:08 - likely from health check
- **CRITICAL**: Redis is running but **NOT USED** for conversation storage yet

### Conversation Storage Architecture
**Current Implementation** (`src/conversation/history_store.py`):
- In-memory dictionary: `_mem: Dict[str, List[Dict[str, Any]]]`
- JSONL persistence: `logs/conversation/<continuation_id>.jsonl`
- Loads last 6 turns from memory or disk
- **NO REDIS INTEGRATION**

**Implications**:
- The 81K token issue was **NOT** caused by Redis dumping everything
- The issue was the system prompt duplication bug in `build_standard_prompt()`
- Redis integration is still pending as part of Supabase track (Option C)
- User was correct: the fix was NOT to trim prompts, but to fix the underlying bug

---

## üéØ UPDATED NEXT STEPS

### 1. Test System Prompt Fix (IMMEDIATE)
- Make a simple EXAI chat call
- Verify system prompt is no longer duplicated in logs
- Check token count is reasonable (~2-3K instead of 81K)
- Verify logs show only one system prompt in payload
- Confirm no cascading timeouts

### 2. Test Fallback Chain Fix
- Verify fallback chain only tries 2 models maximum
- Verify FAST_RESPONSE category only tries GLM models
- Check logs to confirm no cascading timeouts
- Validate fast failure behavior

### 3. Complete Option D - Performance Tracking System
- After fixes are validated, retry Option D implementation
- Use EXAI chat tool with GLM-4.6 and web search enabled
- Design Supabase schema for performance tracking
- Implement Python PerformanceTracker class

### 4. Fix File Upload Functionality
- File upload is broken/incomplete
- This forces large amounts of data to be sent in prompts
- Need to fix file upload to reduce prompt bloat
- User mentioned this multiple times as a priority

### 5. Integrate Redis for Conversation Storage
- Redis container is running but not being used
- Need to integrate Redis for conversation persistence
- This is part of Supabase integration track (Option C)

---

## üìö LESSONS LEARNED (UPDATED)

1. **No Truncation**: User explicitly stated NO TRUNCATION of prompts - vital information would be lost
2. **Root Cause Analysis**: Don't assume the easy answer (reduce prompt size) - investigate the actual bug
3. **System Prompt Duplication**: The bug was in `build_standard_prompt()` including system prompt in user message
4. **Redis Not Used**: Redis container is running but conversation storage is still in-memory + JSONL
5. **Fallback Chain Too Aggressive**: Cross-provider fallback for fast response tools caused 8-minute cascades
6. **User Guidance**: User correctly identified that the issue was NOT about trimming prompts, but about fixing the underlying bug
7. **Self-Awareness**: Need to be more aware of actions taken - focus on fixing actual problems, not just creating documentation

