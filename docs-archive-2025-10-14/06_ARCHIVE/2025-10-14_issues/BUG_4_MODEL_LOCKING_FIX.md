# Bug #4: Model Locking in Continuations Fix
**Date:** 2025-10-14 (14th October 2025)  
**Priority:** üî¥ CRITICAL  
**Status:** ‚úÖ FIXED

---

## üêõ Bug Description

**Problem:** Model switches mid-conversation during continuations, breaking conversation context and consistency.

**User Report:**
> "Model switching mid-conversation"

**Impact:** 
- Conversation context lost when model changes
- Inconsistent responses across turns
- User confusion about which model is being used
- Potential quality degradation if switching from quality to speed model

---

## üîç Root Cause Analysis

### The Flow

**Step 1: Continuation Setup** ‚úÖ CORRECT
```python
# File: src/server/context/thread_context.py (lines 187-195)
model_from_args = arguments.get("model")
if not model_from_args and context.turns:
    # Find the last assistant turn to get the model used
    for turn in reversed(context.turns):
        if turn.role == "assistant" and turn.model_name:
            arguments["model"] = turn.model_name  # ‚úÖ Sets model from previous turn
            logger.debug(f"Using model from previous turn: {turn.model_name}")
            break
```

**Step 2: Model Routing** ‚ùå OVERRIDES IT!
```python
# File: src/server/handlers/request_handler.py (lines 105-115)
requested_model = arguments.get("model") or os.getenv("DEFAULT_MODEL", "glm-4.5-flash")
routed_model = _route_auto_model(name, requested_model, arguments)  # ‚ùå Can override!
model_name = routed_model or requested_model

# Propagate routed model to arguments
arguments["model"] = model_name  # ‚ùå Overwrites continuation model!
```

**Step 3: Auto-Routing Logic** ‚ùå DOESN'T KNOW ABOUT CONTINUATION
```python
# File: src/server/handlers/request_handler_model_resolution.py (lines 44-111)
def _route_auto_model(tool_name: str, requested: str | None, args: Dict[str, Any]) -> str | None:
    req = (requested or "").strip().lower()
    if req and req != "auto":
        return requested  # Only respects explicit non-auto models
    
    # ‚ùå PROBLEM: Doesn't check if this is a continuation!
    # Routes based on tool_name, step_number, etc.
    # Can return different model than continuation model
```

### The Problem

**Scenario:**
1. User starts conversation with `kimi-thinking-preview`
2. First turn completes, model stored in thread
3. User continues conversation (no model specified)
4. `thread_context.py` sets `model = "kimi-thinking-preview"` ‚úÖ
5. `request_handler.py` calls `_route_auto_model("chat", "kimi-thinking-preview", args)`
6. `_route_auto_model()` sees tool="chat" and returns `"glm-4.5-flash"` ‚ùå
7. Model switches from Kimi to GLM mid-conversation! ‚ùå

**Root Cause:** No mechanism to tell routing logic "this model is locked by continuation"

---

## üîß Fix Implementation

### Fix #1: Add Model Lock Flag

**File:** `src/server/context/thread_context.py`  
**Lines:** 187-198

**Before:**
```python
if not model_from_args and context.turns:
    for turn in reversed(context.turns):
        if turn.role == "assistant" and turn.model_name:
            arguments["model"] = turn.model_name
            logger.debug(f"Using model from previous turn: {turn.model_name}")
            break
```

**After:**
```python
if not model_from_args and context.turns:
    for turn in reversed(context.turns):
        if turn.role == "assistant" and turn.model_name:
            arguments["model"] = turn.model_name
            # CRITICAL FIX (Bug #4): Lock model to prevent routing override
            # This ensures the model stays consistent across conversation turns
            arguments["_model_locked_by_continuation"] = True
            logger.debug(f"Using model from previous turn: {turn.model_name} (locked)")
            break
```

**Rationale:**
- Add internal flag `_model_locked_by_continuation` to signal routing logic
- Flag is only set when model comes from previous turn (not user-specified)
- Underscore prefix indicates internal/private parameter

---

### Fix #2: Respect Model Lock in Routing

**File:** `src/server/handlers/request_handler_model_resolution.py`  
**Lines:** 44-71

**Before:**
```python
def _route_auto_model(tool_name: str, requested: str | None, args: Dict[str, Any]) -> str | None:
    try:
        req = (requested or "").strip().lower()
        if req and req != "auto":
            return requested  # explicit model respected
        
        # Route based on tool_name, step_number, etc.
```

**After:**
```python
def _route_auto_model(tool_name: str, requested: str | None, args: Dict[str, Any]) -> str | None:
    try:
        # CRITICAL FIX (Bug #4): Respect model lock from continuation
        # When a conversation is continued, preserve the model from previous turn
        if args.get("_model_locked_by_continuation"):
            logger.debug(f"[MODEL_ROUTING] Model locked by continuation - skipping auto-routing")
            return requested  # Skip routing, use continuation model
        
        req = (requested or "").strip().lower()
        if req and req != "auto":
            return requested  # explicit model respected
        
        # Route based on tool_name, step_number, etc.
```

**Rationale:**
- Check for lock flag FIRST, before any routing logic
- If locked, return requested model immediately (skip all routing)
- Log the decision for debugging

---

## ‚úÖ Verification Plan

### Test Case 1: Continuation Preserves Model
```python
# Turn 1: Start with kimi-thinking-preview
request_1 = {
    "name": "chat_EXAI-WS",
    "arguments": {
        "prompt": "Explain quantum computing",
        "model": "kimi-thinking-preview"
    }
}
# Response includes continuation_id

# Turn 2: Continue (no model specified)
request_2 = {
    "name": "chat_EXAI-WS",
    "arguments": {
        "prompt": "What about quantum entanglement?",
        "continuation_id": "<from_turn_1>"
        # No model specified - should use kimi-thinking-preview
    }
}
```

**Expected:** Turn 2 uses `kimi-thinking-preview` (same as Turn 1)  
**Actual (before fix):** Turn 2 uses `glm-4.5-flash` (routing override) ‚ùå  
**Actual (after fix):** Turn 2 uses `kimi-thinking-preview` (locked) ‚úÖ

### Test Case 2: User Can Override Model
```python
# Turn 1: Start with kimi-thinking-preview
request_1 = {
    "name": "chat_EXAI-WS",
    "arguments": {
        "prompt": "Explain quantum computing",
        "model": "kimi-thinking-preview"
    }
}

# Turn 2: User explicitly changes model
request_2 = {
    "name": "chat_EXAI-WS",
    "arguments": {
        "prompt": "What about quantum entanglement?",
        "continuation_id": "<from_turn_1>",
        "model": "glm-4.6"  # User explicitly changes model
    }
}
```

**Expected:** Turn 2 uses `glm-4.6` (user override)  
**Actual:** Turn 2 uses `glm-4.6` ‚úÖ

### Test Case 3: New Conversation (No Lock)
```python
# New conversation (no continuation_id)
request = {
    "name": "chat_EXAI-WS",
    "arguments": {
        "prompt": "Hello",
        # No model, no continuation_id
    }
}
```

**Expected:** Uses routing logic (glm-4.5-flash for chat)  
**Actual:** Uses routing logic ‚úÖ

---

## üìä Impact Assessment

**Severity:** üî¥ CRITICAL

**Affected Components:**
- ‚úÖ Conversation continuations (all tools)
- ‚úÖ Multi-turn conversations
- ‚úÖ Model routing logic

**User Impact:**
- Consistent model usage across conversation turns
- Predictable behavior in continuations
- Better conversation quality

**Breaking Changes:** None - This is a bug fix that restores expected behavior

---

## üöÄ Implementation Steps

1. [x] Identify root cause (routing override)
2. [x] Add model lock flag in thread_context.py
3. [x] Respect lock flag in routing logic
4. [x] Create test script ‚úÖ COMPLETE
5. [ ] Run tests to verify fix (requires server running)
6. [ ] Update documentation
7. [ ] Create evidence file

---

## üìù Related Files

**Bug Fix:**
- `src/server/context/thread_context.py` (lines 187-198)
- `src/server/handlers/request_handler_model_resolution.py` (lines 44-71)

**Test Script:**
- `scripts/testing/test_model_locking.py` ‚úÖ CREATED

**Documentation:**
- `docs/05_ISSUES/BUG_4_MODEL_LOCKING_FIX.md` (this file)

**Evidence (after fix):**
- `docs/04_TESTING/BUG_4_MODEL_LOCKING_EVIDENCE.md`

---

## üéì Key Insights

### Why This Bug Existed

1. **Separation of Concerns:** Thread context and routing logic are in different modules
2. **No Communication:** No mechanism for thread context to tell routing "don't override this"
3. **Routing Priority:** Routing logic runs AFTER thread context, so it can override

### How the Fix Works

1. **Internal Flag:** Use `_model_locked_by_continuation` as communication channel
2. **Early Exit:** Routing checks flag FIRST, before any logic
3. **Minimal Impact:** Only affects continuations, doesn't change other behavior

### Future Prevention

- Document internal flags and their purposes
- Add tests for continuation model persistence
- Consider adding model lock to ConversationTurn model

---

**Status:** ‚úÖ FIXED - Ready for testing
**Next Step:** Run test script with server running

