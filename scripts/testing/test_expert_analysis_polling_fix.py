"""
Test Script: Expert Analysis Polling Fix Verification
======================================================

Purpose:
    Verify that the expert analysis polling fix (100ms polling interval)
    correctly detects task completion immediately instead of hanging.

Root Cause Fixed:
    - Polling loop was sleeping for 5 seconds between task.done() checks
    - Provider could complete during sleep
    - Client timeout would trigger before next poll
    - Tool cancelled before completion detected

Fix Applied:
    - Changed polling interval from 5 seconds to 0.1 seconds
    - Kept progress heartbeat at 5 seconds to avoid spam
    - Task completion now detected within 100ms

Files Modified:
    - tools/workflow/expert_analysis.py (lines 505-661)

Test Strategy:
    1. Test chat tool (baseline - no expert analysis)
    2. Test analyze tool (should now complete quickly)
    3. Test codereview tool (should complete quickly)
    4. Measure completion times
    5. Verify no TOOL_CANCELLED messages

Expected Results:
    - All tools complete successfully
    - Completion time < 30 seconds for simple tasks
    - No timeout/cancellation errors
    - Expert analysis field present in results

Author: Augment Agent
Date: 2025-10-13
"""

import asyncio
import json
import sys
import time
from pathlib import Path
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from tools.chat import ChatTool
from tools.workflows.analyze import AnalyzeTool
from tools.workflows.codereview import CodeReviewTool


def print_header(title):
    """Print formatted header"""
    print("\n" + "="*80)
    print(f" {title}")
    print("="*80)


def print_timestamp(label):
    """Print timestamp with label"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    print(f"[{timestamp}] {label}")


async def test_chat_baseline():
    """
    Test 1: Chat Tool (Baseline)
    
    This tool does NOT use expert analysis, so it should complete quickly.
    This establishes a baseline for comparison.
    """
    print_header("TEST 1: Chat Tool (Baseline - No Expert Analysis)")
    
    tool = ChatTool()
    
    arguments = {
        "prompt": "What is Python?",
        "model": "glm-4.5-flash",
    }
    
    print(f"\nüìã Test Parameters:")
    print(f"   - Tool: chat")
    print(f"   - Prompt: {arguments['prompt']}")
    print(f"   - Model: {arguments['model']}")
    print(f"   - Expert Analysis: N/A (SimpleTool)")
    
    print_timestamp("‚è±Ô∏è  Starting test...")
    start_time = time.time()
    
    try:
        result = await tool.execute(arguments)
        duration = time.time() - start_time
        
        print_timestamp(f"‚úÖ Test completed in {duration:.2f} seconds")
        
        # Validation
        print(f"\nüîç Validation:")
        
        if duration > 60.0:
            print(f"   ‚ö†Ô∏è  WARNING: Duration longer than expected ({duration:.2f}s)")
            print(f"   (Should be <60s for simple chat)")
        else:
            print(f"   ‚úÖ PASS: Duration appropriate ({duration:.2f}s)")
        
        if result and len(result) > 0:
            print(f"   ‚úÖ PASS: Result returned")
            print(f"\n‚úÖ TEST 1 PASSED!")
            return True, duration
        else:
            print(f"   ‚ùå FAIL: No result returned")
            print(f"\n‚ùå TEST 1 FAILED!")
            return False, duration
            
    except Exception as e:
        duration = time.time() - start_time
        print_timestamp(f"‚ùå Test failed after {duration:.2f} seconds")
        print(f"   Error: {e}")
        import traceback
        traceback.print_exc()
        print(f"\n‚ùå TEST 1 FAILED!")
        return False, duration


async def test_analyze_with_expert():
    """
    Test 2: Analyze Tool (With Expert Analysis)
    
    This tool uses expert analysis and was previously hanging.
    After the fix, it should complete within 30 seconds.
    """
    print_header("TEST 2: Analyze Tool (With Expert Analysis - FIXED)")
    
    tool = AnalyzeTool()
    
    arguments = {
        "step": "Analyze the architecture of a simple Python function",
        "step_number": 1,
        "total_steps": 1,
        "next_step_required": False,
        "findings": "Simple function with clear input/output",
        "use_assistant_model": True,  # Enable expert analysis
        "model": "glm-4.5-flash",
    }
    
    print(f"\nüìã Test Parameters:")
    print(f"   - Tool: analyze")
    print(f"   - Step: {arguments['step']}")
    print(f"   - Expert Analysis: Enabled")
    print(f"   - Model: {arguments['model']}")
    
    print_timestamp("‚è±Ô∏è  Starting test...")
    start_time = time.time()
    
    try:
        result = await tool.execute(arguments)
        duration = time.time() - start_time
        
        print_timestamp(f"‚úÖ Test completed in {duration:.2f} seconds")
        
        # Parse result
        if result and len(result) > 0:
            result_text = result[0].text if hasattr(result[0], 'text') else str(result[0])
            result_data = json.loads(result_text)
            
            print(f"\nüìä Result Analysis:")
            print(f"   - Status: {result_data.get('status', 'unknown')}")
            print(f"   - Has expert_analysis: {'expert_analysis' in result_data}")
            
            if 'expert_analysis' in result_data:
                expert = result_data['expert_analysis']
                print(f"   - Expert analysis status: {expert.get('status', 'unknown')}")
            
            # Validation
            print(f"\nüîç Validation:")
            
            if duration > 60.0:
                print(f"   ‚ö†Ô∏è  WARNING: Duration longer than expected ({duration:.2f}s)")
                print(f"   (Should be <60s for simple analysis)")
            else:
                print(f"   ‚úÖ PASS: Duration appropriate ({duration:.2f}s)")
            
            if 'expert_analysis' not in result_data:
                print(f"   ‚ùå FAIL: No expert_analysis field in response!")
                print(f"\n‚ùå TEST 2 FAILED!")
                return False, duration
            else:
                print(f"   ‚úÖ PASS: Expert analysis field present")
            
            if result_data.get('status') == 'error':
                print(f"   ‚ùå FAIL: Error status returned")
                print(f"   Error: {result_data.get('content', 'unknown')}")
                print(f"\n‚ùå TEST 2 FAILED!")
                return False, duration
            else:
                print(f"   ‚úÖ PASS: No error status")
            
            print(f"\n‚úÖ TEST 2 PASSED!")
            return True, duration
        else:
            print(f"\n‚ùå TEST 2 FAILED: No result returned")
            return False, duration
            
    except Exception as e:
        duration = time.time() - start_time
        print_timestamp(f"‚ùå Test failed after {duration:.2f} seconds")
        print(f"   Error: {e}")
        import traceback
        traceback.print_exc()
        print(f"\n‚ùå TEST 2 FAILED!")
        return False, duration


async def test_codereview_with_expert():
    """
    Test 3: Codereview Tool (With Expert Analysis)
    
    This tool uses expert analysis and should complete quickly.
    Verifies the fix works for multiple workflow tools.
    """
    print_header("TEST 3: Codereview Tool (With Expert Analysis - FIXED)")

    tool = CodeReviewTool()
    
    arguments = {
        "step": "Review a simple Python function for code quality",
        "step_number": 1,
        "total_steps": 1,
        "next_step_required": False,
        "findings": "Function follows PEP 8 style guidelines",
        "use_assistant_model": True,  # Enable expert analysis
        "model": "glm-4.5-flash",
    }
    
    print(f"\nüìã Test Parameters:")
    print(f"   - Tool: codereview")
    print(f"   - Step: {arguments['step']}")
    print(f"   - Expert Analysis: Enabled")
    print(f"   - Model: {arguments['model']}")
    
    print_timestamp("‚è±Ô∏è  Starting test...")
    start_time = time.time()
    
    try:
        result = await tool.execute(arguments)
        duration = time.time() - start_time
        
        print_timestamp(f"‚úÖ Test completed in {duration:.2f} seconds")
        
        # Parse result
        if result and len(result) > 0:
            result_text = result[0].text if hasattr(result[0], 'text') else str(result[0])
            result_data = json.loads(result_text)
            
            print(f"\nüìä Result Analysis:")
            print(f"   - Status: {result_data.get('status', 'unknown')}")
            print(f"   - Has expert_analysis: {'expert_analysis' in result_data}")
            
            # Validation
            print(f"\nüîç Validation:")
            
            if duration > 60.0:
                print(f"   ‚ö†Ô∏è  WARNING: Duration longer than expected ({duration:.2f}s)")
            else:
                print(f"   ‚úÖ PASS: Duration appropriate ({duration:.2f}s)")
            
            if 'expert_analysis' in result_data:
                print(f"   ‚úÖ PASS: Expert analysis field present")
            else:
                print(f"   ‚ùå FAIL: No expert_analysis field")
                print(f"\n‚ùå TEST 3 FAILED!")
                return False, duration
            
            print(f"\n‚úÖ TEST 3 PASSED!")
            return True, duration
        else:
            print(f"\n‚ùå TEST 3 FAILED: No result returned")
            return False, duration
            
    except Exception as e:
        duration = time.time() - start_time
        print_timestamp(f"‚ùå Test failed after {duration:.2f} seconds")
        print(f"   Error: {e}")
        import traceback
        traceback.print_exc()
        print(f"\n‚ùå TEST 3 FAILED!")
        return False, duration


async def main():
    """Run all tests and generate summary"""
    print_header("EXPERT ANALYSIS POLLING FIX VERIFICATION")
    print(f"Testing fix for 5-second polling interval bug")
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = []
    
    # Test 1: Chat (baseline)
    passed1, duration1 = await test_chat_baseline()
    results.append(("Chat (baseline)", passed1, duration1))
    
    # Test 2: Analyze (fixed)
    passed2, duration2 = await test_analyze_with_expert()
    results.append(("Analyze (with expert)", passed2, duration2))
    
    # Test 3: Codereview (fixed)
    passed3, duration3 = await test_codereview_with_expert()
    results.append(("Codereview (with expert)", passed3, duration3))
    
    # Summary
    print_header("TEST SUMMARY")
    
    for test_name, passed, duration in results:
        status = "‚úÖ PASSED" if passed else "‚ùå FAILED"
        print(f"{status}: {test_name} ({duration:.2f}s)")
    
    all_passed = all(result[1] for result in results)
    
    print(f"\n{'='*80}")
    if all_passed:
        print("üéâ ALL TESTS PASSED! Expert analysis polling fix verified!")
        print("\nThe fix successfully resolves the hanging issue:")
        print("  ‚úÖ Polling interval reduced from 5s to 0.1s")
        print("  ‚úÖ Task completion detected within 100ms")
        print("  ‚úÖ No more false timeouts/cancellations")
        return 0
    else:
        print("‚ùå SOME TESTS FAILED! Fix may not be working correctly.")
        print("\nPlease check:")
        print("  - Server was restarted after applying fix")
        print("  - expert_analysis.py changes are present")
        print("  - No other errors in logs")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

