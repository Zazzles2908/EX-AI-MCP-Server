"""
Test Critical Fixes - 2025-10-14

This script tests all 5 critical fixes:
1. Kimi finish_reason extraction
2. Response completeness validation
3. Parameter validation
4. Response structure validation
5. Timeout coordination (already fixed)

Run this after implementing all fixes to verify they work.
"""

import asyncio
import json
import logging
import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def test_kimi_finish_reason():
    """Test #1: Verify Kimi extracts finish_reason"""
    print("\n" + "="*80)
    print("TEST #1: Kimi finish_reason Extraction")
    print("="*80)
    
    try:
        from src.providers.kimi import KimiProvider
        from src.providers.registry_core import ModelProviderRegistry
        
        # Get Kimi provider
        provider = ModelProviderRegistry.get_provider_for_model("kimi-k2-0905-preview")
        if not provider:
            print("‚ùå SKIP: Kimi provider not configured")
            return False
        
        # Test with short prompt (should get finish_reason="stop")
        print("\nüìù Testing with short prompt...")
        response = provider.generate_content(
            prompt="Say 'hello' in one word.",
            model_name="kimi-k2-0905-preview",
            temperature=0.3,
            max_output_tokens=10
        )
        
        finish_reason = response.metadata.get("finish_reason", "NOT_FOUND")
        print(f"‚úÖ finish_reason extracted: {finish_reason}")
        
        if finish_reason == "NOT_FOUND":
            print("‚ùå FAIL: finish_reason not extracted!")
            return False
        
        print(f"‚úÖ PASS: finish_reason = {finish_reason}")
        return True
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_completeness_validation():
    """Test #2: Verify response completeness is validated"""
    print("\n" + "="*80)
    print("TEST #2: Response Completeness Validation")
    print("="*80)
    
    try:
        from tools.simple.base import SimpleTool
        from utils.model.response import ModelResponse
        
        # Create a mock SimpleTool
        class MockTool(SimpleTool):
            def get_name(self):
                return "mock_tool"
            
            def get_description(self):
                return "Mock tool for testing"
            
            def get_system_prompt(self):
                return "You are a test assistant."
        
        tool = MockTool()
        
        # Test 1: Response with finish_reason="length" should return error
        print("\nüìù Testing truncated response (finish_reason='length')...")
        mock_response = ModelResponse(
            content="This is a truncated respo",
            metadata={"finish_reason": "length"}
        )
        
        # We can't easily test this without running the full tool, so we'll just verify
        # the code path exists by checking the source
        import inspect
        source = inspect.getsource(SimpleTool.execute)
        
        if 'finish_reason in ["length", "content_filter"]' in source:
            print("‚úÖ PASS: Completeness check exists in code")
            return True
        else:
            print("‚ùå FAIL: Completeness check not found in code")
            return False
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_parameter_validation():
    """Test #3: Verify parameter validation works"""
    print("\n" + "="*80)
    print("TEST #3: Parameter Validation")
    print("="*80)
    
    try:
        from src.providers.kimi import KimiProvider
        from src.providers.registry_core import ModelProviderRegistry
        
        # Get Kimi provider
        provider = ModelProviderRegistry.get_provider_for_model("kimi-k2-0905-preview")
        if not provider:
            print("‚ùå SKIP: Kimi provider not configured")
            return False
        
        # Test 1: kimi-k2-0905-preview with thinking_mode should raise ValueError
        print("\nüìù Testing invalid parameter (thinking_mode on non-thinking model)...")
        try:
            provider.validate_parameters(
                model_name="kimi-k2-0905-preview",
                temperature=0.3,
                thinking_mode="high"
            )
            print("‚ùå FAIL: Should have raised ValueError!")
            return False
        except ValueError as e:
            if "does not support thinking_mode" in str(e):
                print(f"‚úÖ PASS: Correctly rejected thinking_mode: {e}")
            else:
                print(f"‚ùå FAIL: Wrong error message: {e}")
                return False
        
        # Test 2: kimi-thinking-preview with thinking_mode should succeed
        print("\nüìù Testing valid parameter (thinking_mode on thinking model)...")
        try:
            # Check if kimi-thinking-preview exists
            caps = provider.get_capabilities("kimi-thinking-preview")
            if caps.supports_extended_thinking:
                provider.validate_parameters(
                    model_name="kimi-thinking-preview",
                    temperature=0.3,
                    thinking_mode="high"
                )
                print("‚úÖ PASS: Correctly accepted thinking_mode on thinking model")
            else:
                print("‚ö†Ô∏è SKIP: kimi-thinking-preview doesn't support thinking_mode")
        except Exception as e:
            print(f"‚ùå FAIL: Should not have raised error: {e}")
            return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_structure_validation():
    """Test #4: Verify response structure validation"""
    print("\n" + "="*80)
    print("TEST #4: Response Structure Validation")
    print("="*80)
    
    try:
        # We can't easily test this without mocking the API response
        # So we'll verify the code exists
        from src.providers import kimi_chat
        import inspect
        
        source = inspect.getsource(kimi_chat.chat_completions_create)
        
        if "Invalid Kimi API response: missing 'choices' field" in source:
            print("‚úÖ PASS: Structure validation exists in code")
            return True
        else:
            print("‚ùå FAIL: Structure validation not found in code")
            return False
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_timeout_coordination():
    """Test #5: Verify timeout coordination works"""
    print("\n" + "="*80)
    print("TEST #5: Timeout Coordination")
    print("="*80)
    
    try:
        from config import TimeoutConfig
        
        # Verify TimeoutConfig exists and has correct hierarchy
        print("\nüìù Checking timeout hierarchy...")
        
        tool_timeout = TimeoutConfig.WORKFLOW_TOOL_TIMEOUT_SECS
        expert_timeout = TimeoutConfig.EXPERT_ANALYSIS_TIMEOUT_SECS
        daemon_timeout = TimeoutConfig.get_daemon_timeout()
        
        print(f"Tool timeout: {tool_timeout}s")
        print(f"Expert timeout: {expert_timeout}s")
        print(f"Daemon timeout: {daemon_timeout}s")
        
        # Verify hierarchy
        if expert_timeout < tool_timeout < daemon_timeout:
            print("‚úÖ PASS: Timeout hierarchy is correct")
            return True
        else:
            print("‚ùå FAIL: Timeout hierarchy is incorrect")
            return False
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run all tests"""
    print("\n" + "="*80)
    print("CRITICAL FIXES TEST SUITE - 2025-10-14")
    print("="*80)
    
    results = {
        "Kimi finish_reason": await test_kimi_finish_reason(),
        "Completeness validation": await test_completeness_validation(),
        "Parameter validation": await test_parameter_validation(),
        "Structure validation": await test_structure_validation(),
        "Timeout coordination": await test_timeout_coordination(),
    }
    
    print("\n" + "="*80)
    print("TEST RESULTS SUMMARY")
    print("="*80)
    
    for test_name, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{status}: {test_name}")
    
    total = len(results)
    passed = sum(1 for p in results.values() if p)
    
    print(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ ALL TESTS PASSED!")
        return 0
    else:
        print(f"\n‚ö†Ô∏è {total - passed} tests failed")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

