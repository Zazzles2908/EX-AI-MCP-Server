# EX-AI MCP Server - Main Project Environment Configuration
# TEMPLATE FILE - Copy to .env.docker and fill in your API keys
# Copy from .env.example and fill in your API keys
# Last Updated: 2025-10-09 (Phase 7 - .env Restructuring)

# ============================================================================
# CORE CONFIGURATION
# ============================================================================
DEFAULT_MODEL=glm-4.5-flash  # Default model for all tools (glm-4.5-flash recommended for speed)
ROUTER_ENABLED=true  # Enable intelligent model routing based on task complexity
GLM_ENABLE_WEB_BROWSING=true  # Enable GLM native web search capability

# Security Settings - File Path Validation
# Allow external paths (paths outside repo root) for file uploads
# This is needed for EXAI tools to access files in the project directory
EX_ALLOW_EXTERNAL_PATHS=true
EX_ALLOWED_EXTERNAL_PREFIXES=/app  # Docker container path

# Cross-Platform Path Mapping (Windows to Linux)
# Maps Windows drive letters to Linux container paths
# Format: C:/app,D:/data,E:/shared (comma-separated)
EX_DRIVE_MAPPINGS=C:/app

# Debug path conversion (set to true to log path conversions)
EX_DEBUG_PATH_CONVERSION=false

# ============================================================================
# SECURITY CONFIGURATION (CHANGED 2025-10-16: Defaults TRUE for security-by-default)
# ============================================================================
# Input validation and security controls
# SECURE_INPUTS_ENFORCED: Validates file paths to prevent directory traversal attacks
# STRICT_FILE_SIZE_REJECTION: Enforces file size limits to prevent DoS attacks
# Set to false only if you need to disable these protections (not recommended)
SECURE_INPUTS_ENFORCED=true
STRICT_FILE_SIZE_REJECTION=true

# ============================================================================
# SUPABASE STORAGE CONFIGURATION
# ============================================================================
# NOTE: Supabase credentials are configured in the SUPABASE CONFIGURATION section below (lines 386-393)
# This section is reserved for future Supabase-specific storage settings
# Enable native async providers for workflow tools (debug, analyze, thinkdeep, etc.)
# When enabled, uses AsyncGLMProvider and AsyncKimiProvider with native async SDK calls
# When disabled, uses sync providers with run_in_executor() pattern (backward compatible)
# Benefits: No thread blocking, better resource efficiency, improved scalability
USE_ASYNC_PROVIDERS=true  # Enable async providers (Phase 2 implementation)

# ============================================================================
# MODEL OUTPUT TOKEN LIMITS
# ============================================================================
# Controls maximum response length from AI models
# These are DEFAULT maximums used when max_output_tokens is not explicitly provided
# Official limits (as of 2025-10-14):
#   - Kimi K2 models: 16384 tokens (official Moonshot AI limit)
#   - GLM models: 8192 tokens (official ZhipuAI limit)
# Set to 0 to disable automatic max_tokens (let model use its default)
DEFAULT_MAX_OUTPUT_TOKENS=8192  # Default max output tokens for all models
KIMI_MAX_OUTPUT_TOKENS=16384    # Max output tokens for Kimi models
GLM_MAX_OUTPUT_TOKENS=8192      # Max output tokens for GLM models

# Whether to enforce max_tokens even when not explicitly requested
# true: Always use max_tokens (prevents truncation but may waste tokens)
# false: Only use max_tokens when explicitly provided by caller
ENFORCE_MAX_TOKENS=true

# ============================================================================
# EXPERT ANALYSIS CONFIGURATION
# ============================================================================
# Enable/disable expert analysis entirely (default: true with minimal thinking mode)
# Expert analysis adds 5-30s latency depending on thinking mode and prompt size
# With thinking_mode=minimal, expert analysis completes in ~5-7s
# Disable this to make workflow tools complete in <1 second (but lose AI analysis)
EXPERT_ANALYSIS_ENABLED=true

# Thinking mode for expert analysis when enabled (minimal, low, medium, high, max)
# - minimal: 0.5% model capacity, ~5-7s response time (DEFAULT for speed)
# - low: 8% model capacity, ~8-10s response time
# - medium: 33% model capacity, ~15-20s response time
# - high: 67% model capacity, ~25-30s response time
# - max: 100% model capacity, ~40-60s response time
# NOTE: Users can override this per-call by passing thinking_mode parameter
EXPERT_ANALYSIS_THINKING_MODE=minimal

# Auto-upgrade models for thinking mode support (default: true)
# When true, automatically upgrades models that don't support thinking mode:
#   - GLM: glm-4.5-flash → glm-4.6
#   - Kimi: kimi-k2-0905-preview → kimi-thinking-preview
# When false, keeps user-specified model even if it doesn't support thinking mode
# NOTE: Disabling may cause expert analysis to fail or hang with incompatible models
EXPERT_ANALYSIS_AUTO_UPGRADE=true

# Embed FULL file contents in expert analysis prompt (default: false)
# When true, embeds entire file contents into prompt, significantly increasing size and latency
# When false, only file paths/names are included in context (not full contents)
# NOTE: File paths/names are ALWAYS included regardless of this setting (Issue #9 clarification)
# Only enable if expert analysis truly needs to see full file contents
EXPERT_ANALYSIS_INCLUDE_FILES=false

# Maximum number of files to include in expert analysis (default: 20)
# Only applies when EXPERT_ANALYSIS_INCLUDE_FILES=true
# Prevents token bloat from embedding too many files (Issue #8)
EXPERT_ANALYSIS_MAX_FILE_COUNT=20

# Maximum file size to include in expert analysis (KB)
# Only applies when EXPERT_ANALYSIS_INCLUDE_FILES=true
# Prevents massive files from being embedded and causing timeouts
EXPERT_ANALYSIS_MAX_FILE_SIZE_KB=10

# Legacy settings (kept for backward compatibility)
# TRACK 1 FIX (2025-10-17): Disabled by default due to Augment Code MCP timeout issues
# Workflow tools with expert analysis take 30-60+ seconds, exceeding Augment's ~10-30s timeout
# See: docs/05_CURRENT_WORK/05_PROJECT_STATUS/CODEREVIEW_TIMEOUT_ROOT_CAUSE_2025-10-17.md
# Users can still enable expert analysis per-call with use_assistant_model=true parameter
DEFAULT_USE_ASSISTANT_MODEL=false
EXPERT_HEARTBEAT_INTERVAL_SECS=5

# ============================================================================
# WEBSOCKET DAEMON CONFIGURATION
# ============================================================================
# Alphabetically sorted for easy lookup

# Authentication (optional)
# Optional authentication token for WebSocket connections
EXAI_WS_TOKEN=test-token-12345

# Caching and deduplication
EXAI_WS_DISABLE_COALESCE_FOR_TOOLS=  # Comma-separated list of tools to disable request coalescing
EXAI_WS_INFLIGHT_TTL_SECS=180  # How long to cache in-flight requests (3 minutes)
EXAI_WS_RESULT_TTL=600  # How long to cache completed results (10 minutes)
EXAI_WS_RETRY_AFTER_SECS=1  # Delay before retrying failed requests

# Compatibility flags
EXAI_WS_COMPAT_TEXT=true  # Enable text compatibility mode for legacy clients
EX_ENSURE_NONEMPTY_FIRST=false  # Ensure first message is non-empty (legacy)

# Concurrency limits
EXAI_WS_GLM_MAX_INFLIGHT=4  # Max concurrent GLM requests per session
EXAI_WS_GLOBAL_MAX_INFLIGHT=24  # Max concurrent requests across all sessions
EXAI_WS_KIMI_MAX_INFLIGHT=6  # Max concurrent Kimi requests per session
EXAI_WS_SESSION_MAX_INFLIGHT=8  # Max concurrent requests per session

# PHASE 1 (2025-10-18): Connection limits for resilience
MAX_CONNECTIONS=1000  # Global connection limit (prevent resource exhaustion)
MAX_CONNECTIONS_PER_IP=10  # Per-IP connection limit (prevent abuse)

# PHASE 1 (2025-10-18): Rate limiting for abuse prevention
RATE_LIMIT_GLOBAL_CAPACITY=1000  # Global token bucket capacity
RATE_LIMIT_GLOBAL_REFILL_RATE=100  # Global tokens per second
RATE_LIMIT_IP_CAPACITY=100  # Per-IP token bucket capacity
RATE_LIMIT_IP_REFILL_RATE=10  # Per-IP tokens per second
RATE_LIMIT_USER_CAPACITY=50  # Per-user token bucket capacity
RATE_LIMIT_USER_REFILL_RATE=5  # Per-user tokens per second

# Connection timeouts
# CRITICAL FIX (2025-10-17): Use industry-standard conservative ping intervals
# WebSocket RFC 6455 recommends 30-second ping intervals for connection liveness
# Too-frequent pings (e.g., 3s) can cause client-side issues and race conditions
# Application-level progress frames are for user feedback, not connection keepalive
#
# WEBSOCKET PING INTERVAL HISTORY (2025-10-17 to 2025-10-18):
# - Original: 45 seconds (too long, clients disconnected during long operations)
# - First fix attempt: 3 seconds (TOO AGGRESSIVE, made disconnects WORSE - clients dropped after 2s instead of 5s)
# - Current: 30 seconds (industry standard per RFC 6455, CORRECT configuration)
#
# LESSON LEARNED: Aggressive ping intervals can cause timing issues and race conditions.
# The client's timeout mechanism is based on WebSocket protocol-level ping/pong frames,
# NOT application-level progress frames. Stick to industry standards (30s) for stability.
#
EXAI_WS_HELLO_TIMEOUT=15  # Timeout for initial handshake (seconds)
EXAI_WS_PING_INTERVAL=30  # Ping interval (seconds) - Industry standard (RFC 6455) - DO NOT CHANGE
EXAI_WS_PING_TIMEOUT=30  # Ping timeout (seconds)
# CRITICAL FIX (2025-10-18): Changed from 8.0 to 2.5 to prevent client drops after 8s of silence
EXAI_WS_PROGRESS_INTERVAL_SECS=2.5  # Progress update interval for long-running tasks (GLM-4.6 recommended)

# Message size limits
EXAI_WS_MAX_BYTES=33554432  # Max message size (32 MiB) - increase for large file uploads

# Network configuration
EXAI_WS_HOST=0.0.0.0  # WebSocket server host (0.0.0.0 for Docker container - accessible from host)
EXAI_WS_PORT=8079  # WebSocket server port

# PHASE 3 (2025-10-18): Real-time monitoring dashboard
MONITORING_PORT=8080  # Monitoring WebSocket port for dashboard
MONITORING_ENABLED=true  # Enable real-time monitoring dashboard
MONITORING_HOST=0.0.0.0  # Monitoring server host

# PHASE 3 CRITICAL GAPS (2025-10-18): Health check endpoint
# NOTE: Changed from 8081 to 8082 to avoid conflict with Redis Commander
HEALTH_CHECK_PORT=8082  # Health check HTTP port (changed from 8081)
HEALTH_CHECK_ENABLED=true  # Enable health check endpoint
HEALTH_CHECK_HOST=0.0.0.0  # Health check server host

# PHASE 3 CRITICAL GAPS (2025-10-18): Prometheus metrics
METRICS_PORT=8000  # Prometheus metrics HTTP port
METRICS_ENABLED=true  # Enable Prometheus metrics endpoint

# Diagnostics
DIAGNOSTICS=true  # Enable diagnostic logging and snapshots

# Logging
LOG_LEVEL=INFO  # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)

# ============================================================================
# KIMI API CONFIGURATION (Moonshot)
# ============================================================================
KIMI_API_KEY=your_kimi_api_key_here  # Moonshot AI API key
KIMI_BASE_URL=https://api.moonshot.ai/v1  # Kimi API base URL (OpenAI-compatible)
KIMI_API_URL=https://api.moonshot.ai/v1  # Kimi API URL (same as base URL)

# ============================================================================
# GLM API CONFIGURATION (ZhipuAI)
# ============================================================================
# Using z.ai international endpoint (3x faster than China endpoint)
# API Endpoint: https://api.z.ai/api/paas/v4 (recommended - international)
# Documentation: https://open.bigmodel.cn/dev/api (docs site only, not for API calls)
GLM_API_KEY=your_glm_api_key_here  # ZhipuAI API key
GLM_BASE_URL=https://api.z.ai/api/paas/v4  # GLM API base URL (z.ai proxy, 3x faster)
GLM_API_URL=https://api.z.ai/api/paas/v4  # GLM API URL (same as base URL)

# Legacy vars for backward compatibility (same as GLM_* above)
ZHIPUAI_API_KEY=your_glm_api_key_here  # Legacy: same as GLM_API_KEY
ZHIPUAI_API_URL=https://api.z.ai/api/paas/v4  # Legacy: same as GLM_API_URL
ZHIPUAI_BASE_URL=https://api.z.ai/api/paas/v4  # Legacy: same as GLM_BASE_URL

# ============================================================================
# TIMEOUT CONFIGURATION (Coordinated Hierarchy)
# ============================================================================
# These timeouts follow a coordinated hierarchy to ensure proper timeout behavior:
# HTTP Client → Tool Level → Daemon Level → Shim Level → Client Level
# Rule: Each outer timeout = 1.5x inner timeout (50% buffer)
#
# CRITICAL: HTTP Client timeout is the foundation - must be >= workflow tool timeout
# This was the root cause of the "SDK hanging" issue (was 60s, needed 300s)
EX_HTTP_TIMEOUT_SECONDS=300  # HTTP client timeout (foundation of timeout hierarchy)

# Tool-level timeouts (TRACK 2 FIX - 2025-10-16: Reduced from 300s to 30s)
# CRITICAL FIX (2025-10-17): Expert analysis with thinking mode needs longer timeout
SIMPLE_TOOL_TIMEOUT_SECS=30  # Timeout for simple tools (chat, listmodels, etc.)
WORKFLOW_TOOL_TIMEOUT_SECS=180  # Timeout for workflow tools (debug, analyze, etc.) - 3min for thinking mode
EXPERT_ANALYSIS_TIMEOUT_SECS=180  # Timeout for expert analysis validation (base timeout) - 3min for thinking mode

# Thinkdeep adaptive timeout (optional override)
# If not set, thinkdeep uses EXPERT_ANALYSIS_TIMEOUT_SECS with adaptive multipliers:
# - minimal: 0.5x base (e.g., 60s * 0.5 = 30s for quick validation)
# - low: 0.7x base (e.g., 60s * 0.7 = 42s for standard validation)
# - medium: 1.0x base (e.g., 60s * 1.0 = 60s for thorough analysis)
# - high: 1.5x base (e.g., 60s * 1.5 = 90s for deep analysis)
# - max: 2.0x base (e.g., 60s * 2.0 = 120s for exhaustive reasoning)
# Set this to override adaptive behavior with a fixed timeout for all thinking modes
# THINKDEEP_EXPERT_TIMEOUT_SECS=60  # Optional: Fixed timeout for thinkdeep (overrides adaptive)

# Provider-specific timeouts (TRACK 2 FIX - 2025-10-16: Synchronized with .env)
# CRITICAL FIX (2025-10-17): GLM and Kimi thinking mode needs longer timeout for expert analysis
GLM_TIMEOUT_SECS=120  # GLM API request timeout (2min for thinking mode)
KIMI_TIMEOUT_SECS=180  # Kimi API request timeout (3min for thinking mode)
KIMI_WEB_SEARCH_TIMEOUT_SECS=180  # Kimi web search timeout (3min for thinking mode with web search)

# Infrastructure timeouts (auto-calculated by TimeoutConfig class in config.py)
# IMPORTANT: These are NOT environment variables and CANNOT be overridden in .env
# They are always calculated based on WORKFLOW_TOOL_TIMEOUT_SECS:
#   - Daemon timeout: 270s (1.5x WORKFLOW_TOOL_TIMEOUT_SECS = 1.5 * 180)
#   - Shim timeout: 360s (2.0x WORKFLOW_TOOL_TIMEOUT_SECS = 2.0 * 180)
#   - Client timeout: 450s (2.5x WORKFLOW_TOOL_TIMEOUT_SECS = 2.5 * 180)
# To change these, modify WORKFLOW_TOOL_TIMEOUT_SECS above and they will auto-adjust.
# See: tool_validation_suite/docs/current/guides/TIMEOUT_CONFIGURATION_GUIDE.md
#
# TIMEOUT HIERARCHY (in seconds) - CRITICAL FIX (2025-10-17):
# Provider calls: 180s (KIMI_TIMEOUT_SECS for thinking mode)
# Workflow tools: 180s (WORKFLOW_TOOL_TIMEOUT_SECS)
# Expert analysis: 180s (EXPERT_ANALYSIS_TIMEOUT_SECS)
# Daemon: 270s (auto-calculated: 1.5x WORKFLOW_TOOL_TIMEOUT_SECS)
# Shim: 360s (auto-calculated: 2.0x WORKFLOW_TOOL_TIMEOUT_SECS)
# Client: 450s (auto-calculated: 2.5x WORKFLOW_TOOL_TIMEOUT_SECS)

# ============================================================================
# CIRCUIT BREAKER CONFIGURATION
# ============================================================================
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5  # Number of failures before circuit opens
CIRCUIT_BREAKER_RECOVERY_TIMEOUT_SECS=300  # Time before attempting recovery (5 minutes)

# ============================================================================
# SESSION MANAGEMENT CONFIGURATION
# ============================================================================
SESSION_TIMEOUT_SECS=3600  # Session timeout (1 hour)
SESSION_MAX_CONCURRENT=100  # Maximum concurrent sessions
SESSION_CLEANUP_INTERVAL=300  # Session cleanup interval (5 minutes)

# ============================================================================
# FILE PATH HANDLING
# ============================================================================
# Allow relative file paths in tool requests (auto-resolved to absolute paths)
EX_ALLOW_RELATIVE_PATHS=true

# ============================================================================
# KIMI (MOONSHOT) PROVIDER SETTINGS
# ============================================================================
AGENTIC_ENABLE_LOGGING=true  # Enable agentic logging for Kimi provider
KIMI_MAX_HEADER_LEN=4096  # Maximum header length for Kimi requests

# Kimi File Upload Configuration
KIMI_FILES_MAX_SIZE_MB=100  # Maximum file size for Kimi uploads (MB)
KIMI_FILES_PARALLEL_UPLOADS=false  # Enable parallel file uploads
KIMI_FILES_MAX_PARALLEL=3  # Maximum concurrent uploads
KIMI_FILES_UPLOAD_TIMEOUT_SECS=90  # Timeout for individual file upload
KIMI_FILES_FETCH_TIMEOUT_SECS=25  # Timeout for fetching file info
KIMI_FILES_MAX_COUNT=0  # Max files per upload (0 = no limit)
KIMI_FILES_BEHAVIOR_ON_OVERSIZE=skip  # Behavior on oversize: skip or fail
KIMI_FILES_FETCH_RETRIES=3  # Number of retries for file fetch
KIMI_FILES_FETCH_BACKOFF=0.8  # Backoff multiplier for retries
KIMI_FILES_FETCH_INITIAL_DELAY=0.5  # Initial delay for retries (seconds)
KIMI_MF_CHAT_TIMEOUT_SECS=180  # Timeout for multi-file chat operations

KIMI_DEFAULT_MODEL=kimi-k2-0905-preview  # Default Kimi model (balanced quality/speed)
KIMI_THINKING_MODEL=kimi-thinking-preview  # Kimi thinking model (extended reasoning)
KIMI_SPEED_MODEL=kimi-k2-turbo-preview  # Kimi speed model (fast responses)
KIMI_CACHE_TOKEN_TTL_SECS=1800  # Kimi cache token TTL (30 minutes)
KIMI_CACHE_TOKEN_LRU_MAX=256  # Kimi cache token LRU max size

# Model routing preferences (ordered by preference, comma-separated)
# Used by provider registry to select preferred model when multiple models are available
# First model in list that is available will be selected
KIMI_PREFERRED_MODELS=kimi-k2-0905-preview,kimi-k2-turbo-preview,kimi-k2-0711-preview  # Kimi model preference order
GLM_PREFERRED_MODELS=glm-4.5-flash,glm-4.6,glm-4.5,glm-4.5-air  # GLM model preference order

# ============================================================================
# EMBEDDINGS CONFIGURATION
# ============================================================================
# Provider selection: kimi, glm, or external
# - kimi: Use Kimi/Moonshot embeddings (OpenAI-compatible)
# - glm: Use GLM embeddings (ZhipuAI SDK, implemented 2025-10-09)
# - external: Use external embeddings service
EMBEDDINGS_PROVIDER=kimi  # Embeddings provider (kimi, glm, or external)

# Kimi embeddings model (OpenAI-compatible)
KIMI_EMBED_MODEL=text-embedding-3-large  # Kimi embeddings model (OpenAI-compatible)

# GLM embeddings model (implemented 2025-10-09 - Phase 5, fixed with ZAI SDK)
# IMPORTANT: Now uses ZAI SDK (zai-sdk) instead of zhipuai SDK
# The ZAI SDK works with z.ai proxy endpoint (3x faster than bigmodel.cn)
# Options: embedding-2 (1024 dims, recommended), embedding-3 (8192 dims)
GLM_EMBED_MODEL=embedding-2  # GLM embeddings model (embedding-2 or embedding-3)

# GLM embeddings endpoint (uses z.ai proxy with ZAI SDK)
# ZAI SDK (zai-sdk>=0.0.4) is designed for z.ai endpoints
# Endpoint: https://api.z.ai/api/paas/v4 (same as chat, 3x faster)
GLM_EMBEDDINGS_BASE_URL=https://api.z.ai/api/paas/v4  # GLM embeddings endpoint (z.ai proxy)

# GLM native web search configuration (2025-10-09 - Removed DuckDuckGo fallback)
# Documentation: https://docs.z.ai/api-reference/tools/web-search
# Endpoint: https://api.z.ai/api/paas/v4/web_search
GLM_WEBSEARCH_COUNT=10  # Number of search results to return
GLM_WEBSEARCH_ENGINE=search-prime  # Search engine (search-prime or search-pro)
GLM_WEBSEARCH_RECENCY=all  # Recency filter (oneDay, oneWeek, oneMonth, oneYear, all)
GLM_WEBSEARCH_TIMEOUT_SECS=30  # Web search timeout (seconds)
GLM_WEBSEARCH_DOMAIN_FILTER=  # Optional domain filter (e.g., "github.com")
GLM_ACCEPT_LANGUAGE=en-US,en  # Language preference for search results
GLM_DEFAULT_MODEL=glm-4.6  # Default GLM model (balanced quality/speed)  
# External embeddings service URL (only used when EMBEDDINGS_PROVIDER=external)
# EXTERNAL_EMBEDDINGS_URL=http://localhost:8080/embed  # External embeddings service URL

# ============================================================================
# TEST FILES DIRECTORY
# ============================================================================
# Absolute path(s) to folder(s) containing sample files for upload/extract tests
# Supports multiple roots separated by comma or semicolon
TEST_FILES_DIR=C:\Project # Test files directory (absolute path)

# ============================================================================
# STREAMING CONFIGURATION
# ============================================================================
GLM_STREAM_ENABLED=true  # Enable GLM streaming for chat tool (recommended for better UX)
GLM_STREAM_TIMEOUT=300  # GLM streaming timeout in seconds (5 minutes) - prevents indefinite hangs
GLM_TOOL_STREAM_ENABLED=true  # Enable GLM-4.6 tool call streaming (CRITICAL for web search tool calling)
KIMI_STREAM_ENABLED=false  # Enable Kimi streaming (not currently needed)
KIMI_STREAM_TIMEOUT=600  # Kimi streaming timeout in seconds (10 minutes) - Kimi can be slower than GLM
KIMI_STREAM_TIMEOUT_SECS=240  # DEPRECATED: Use KIMI_STREAM_TIMEOUT instead (kept for backward compatibility)
KIMI_STREAM_PRIME_CACHE=false  # Prime Kimi cache before streaming
KIMI_EXTRACT_REASONING=true  # Extract reasoning_content from kimi-thinking-preview model (default: true)

# ============================================================================
# KIMI CHAT TOOL TIMEOUTS
# ============================================================================
# Kimi chat tool has separate timeouts for web-enabled and non-web requests
# Web search requests need longer timeouts due to search latency
KIMI_CHAT_TOOL_TIMEOUT_SECS=180  # Kimi chat timeout for non-web requests (3 minutes)
KIMI_CHAT_TOOL_TIMEOUT_WEB_SECS=300  # Kimi chat timeout for web search requests (5 minutes)

# ============================================================================
# CONVERSATION STORAGE CONFIGURATION
# ============================================================================
# Conversation threads enable multi-turn conversations with continuation_id parameter
# Storage backend options: memory, supabase, dual
# - memory: In-memory only (LOST ON RESTART)
# - supabase: Supabase only (persistent)
# - dual: Both (writes to both, reads from Supabase with fallback to memory)
CONVERSATION_STORAGE_BACKEND=dual  # Storage backend (memory, supabase, dual)
CONVERSATION_TIMEOUT_HOURS=24  # How long to keep conversation threads (24 hours recommended)

# Redis Configuration (P0-9 Security Fix: Authentication enabled)
REDIS_PASSWORD=sk0yC6x_YAN1Z1ALmAgJOdVPuGZdF3gXX02q9dTi9xI  # Redis authentication password (CHANGE IN PRODUCTION)
REDIS_URL=redis://:sk0yC6x_YAN1Z1ALmAgJOdVPuGZdF3gXX02q9dTi9xI@redis:6379/0  # Redis URL for persistent storage (points to redis container)
# If REDIS_URL is not set and backend=memory, conversations use in-memory storage (LOST ON RESTART)

# File upload configuration
UPLOAD_FILES_IMMEDIATELY=true  # Upload files to Supabase immediately when provided
ENABLE_FALLBACK=true  # Enable fallback to in-memory storage if Supabase fails
KIMI_UPLOAD_TO_SUPABASE=true  # Upload Kimi files to Supabase Storage (in addition to Moonshot)
KIMI_SUPABASE_TIMEOUT=30.0  # Timeout for Supabase storage uploads (seconds)
KIMI_SEMAPHORE_TIMEOUT=0.001  # Timeout for semaphore acquisition (seconds)

# ============================================================================
# MESSAGE BUS CONFIGURATION - REMOVED (2025-10-18)
# ============================================================================
# Message bus code has been removed as it was disabled and causing code bloat.
# This section is kept as a placeholder for historical reference.
# If message bus functionality is needed in the future, implement it fresh
# rather than re-enabling old code.
#
# Previously: MESSAGE_BUS_ENABLED=false (disabled)

# ============================================================================
# SUPABASE CONFIGURATION (Track 3: Persistent Storage)
# ============================================================================
# Supabase project URL and API keys
# Get credentials from: https://supabase.com/dashboard/project/_/settings/api
SUPABASE_URL=https://your-project.supabase.co  # Supabase project URL
SUPABASE_ANON_KEY=your_supabase_anon_key_here  # Supabase anon key (client-side)
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key_here  # Supabase service role key (server-side) - REQUIRED for storage manager
SUPABASE_JWT_SECRET=your_supabase_jwt_secret_here  # JWT secret for signing tokens
SUPABASE_PROJECT_ID=your_project_id  # Supabase project ID

# Legacy variable for backward compatibility
SUPABASE_KEY=your_supabase_anon_key_here  # Legacy: same as SUPABASE_ANON_KEY

# ============================================================================
# CIRCUIT BREAKER CONFIGURATION (Supabase Fallback)
# ============================================================================
# Automatic fallback when Supabase is slow/unavailable
CIRCUIT_BREAKER_ENABLED=true  # Enable circuit breaker for Supabase
CIRCUIT_BREAKER_THRESHOLD=5  # Failure threshold before opening circuit
CIRCUIT_BREAKER_TIMEOUT_SECS=60  # Circuit breaker timeout (1 minute)
FALLBACK_TO_WEBSOCKET=true  # Fallback to WebSocket when Supabase unavailable

# ============================================================================
# PERFORMANCE METRICS CONFIGURATION
# ============================================================================
PERFORMANCE_METRICS_ENABLED=true
METRICS_WINDOW_SIZE=1000
METRICS_JSON_ENDPOINT_ENABLED=true
METRICS_JSON_PORT=9109

# ============================================================================
# ENVIRONMENT
# ============================================================================
ENVIRONMENT=development  # Environment (development, staging, production)

