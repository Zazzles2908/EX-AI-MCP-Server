# Provider Selection Guide

> **Version:** 1.0.0
> **Last Updated:** 2025-11-10
> **Status:** ‚úÖ **Complete**

## üéØ Overview

This guide explains the intelligent provider selection logic in the EX-AI MCP Server, helping you choose the right AI model for your specific use case.

---

## üìä Provider Comparison

| Feature | GLM-4.6 | Kimi K2 |
|---------|---------|---------|
| **Context Length** | 128K | 128K |
| **Max Output** | 8K | 8K |
| **File Processing** | ‚ùå Limited | ‚úÖ Excellent |
| **Image Analysis** | ‚ùå No | ‚úÖ Yes |
| **Code Analysis** | ‚úÖ Excellent | ‚úÖ Good |
| **Complex Reasoning** | ‚úÖ Excellent | ‚úÖ Good |
| **Streaming** | ‚úÖ Yes | ‚úÖ Yes |
| **Cost** | Medium | Medium-High |
| **Speed** | Fast | Medium |

---

## üéØ Selection Criteria

### Use GLM-4.6 When:
- ‚úÖ **Complex reasoning** and analysis
- ‚úÖ **Code review** and debugging
- ‚úÖ **Architecture** design
- ‚úÖ **Security** auditing
- ‚úÖ **Performance** optimization
- ‚úÖ **Quick responses** needed
- ‚úÖ **Text-only** tasks

### Use Kimi K2 When:
- ‚úÖ **File analysis** (PDF, DOCX, etc.)
- ‚úÖ **Image processing** and OCR
- ‚úÖ **Document summarization**
- ‚úÖ **Multi-modal** tasks
- ‚úÖ **Large document** processing
- ‚úÖ **Visual content** analysis
- ‚úÖ **Long conversations** with context

---

## üîÑ Auto-Selection Logic

The EX-AI MCP Server automatically selects the best provider based on:

### 1. Message Analysis
```python
def select_provider(message, files=None):
    # Check for file attachments
    if files and len(files) > 0:
        return "kimi"  # File processing
    
    # Check for images
    if has_images(message):
        return "kimi"  # Vision tasks
    
    # Check for code-related keywords
    if is_code_related(message):
        return "glm"   # Code analysis
    
    # Default to GLM for text
    return "glm"
```

### 2. Task Type Detection
```python
task_classification = {
    "code_review": "glm",
    "security_audit": "glm",
    "architecture": "glm",
    "file_analysis": "kimi",
    "image_processing": "kimi",
    "document_summary": "kimi",
    "general_chat": "glm",
    "complex_reasoning": "glm"
}
```

---

## üíª Manual Selection

### Force Specific Provider
```python
# Use GLM for coding
response = exai_mcp.chat(
    message="Review this Python code",
    provider="glm",
    model="glm-4.6"
)

# Use Kimi for files
response = exai_mcp.chat_with_file(
    message="Summarize this PDF",
    files=["document.pdf"],
    provider="kimi",
    model="moonshot-v1-128k"
)
```

### Model Selection
```python
# GLM models
glm_models = {
    "fast": "glm-4.5-flash",
    "balanced": "glm-4.5",
    "best": "glm-4.6"
}

# Kimi models
kimi_models = {
    "standard": "moonshot-v1-8k",
    "extended": "moonshot-v1-32k",
    "maximum": "moonshot-v1-128k"
}
```

---

## üìä Performance Optimization

### Cost Optimization
```python
def optimize_for_cost(task_type, complexity="medium"):
    if complexity == "low":
        return {"provider": "glm", "model": "glm-4.5-flash"}
    elif complexity == "medium":
        return {"provider": "glm", "model": "glm-4.5"}
    else:
        return {"provider": "glm", "model": "glm-4.6"}
```

### Speed Optimization
```python
def optimize_for_speed(task_type):
    if task_type == "chat":
        return {"provider": "glm", "model": "glm-4.5-flash"}
    elif task_type == "analysis":
        return {"provider": "glm", "model": "glm-4.6"}
```

### Quality Optimization
```python
def optimize_for_quality(task_type):
    if task_type == "file_analysis":
        return {"provider": "kimi", "model": "moonshot-v1-128k"}
    else:
        return {"provider": "glm", "model": "glm-4.6"}
```

---

## üîÑ Fallback Strategy

### Automatic Fallback
```python
def call_with_fallback(message, files=None):
    # Try primary provider
    try:
        return call_provider("glm", message, files)
    except ProviderError:
        # Fallback to secondary
        return call_provider("kimi", message, files)
```

### Manual Fallback
```python
try:
    # Try GLM first
    result = exai_mcp.chat(message, provider="glm")
except Exception as e:
    # Fallback to Kimi
    result = exai_mcp.chat(message, provider="kimi")
```

---

## üìà Usage Examples

### Example 1: Code Review Pipeline
```python
# Auto-select GLM for code
response = exai_mcp.expert_analysis(
    domain="code_quality",
    input="/path/to/code",
    provider="auto"  # Auto-selects GLM
)
```

### Example 2: Document Analysis
```python
# Auto-select Kimi for files
response = exai_mcp.chat_with_file(
    message="Analyze this contract",
    files=["contract.pdf"]
)  # Auto-selects Kimi
```

### Example 3: Multi-Modal Task
```python
# Manually select Kimi for image
response = exai_mcp.kimi_vision(
    file_id="image_123",
    analysis_type="describe"
)
```

---

## ‚öôÔ∏è Configuration

### Environment Variables
```bash
# Preferred provider
PREFERRED_PROVIDER=glm

# Model preferences
GLM_MODEL_DEFAULT=glm-4.6
KIMI_MODEL_DEFAULT=moonshot-v1-128k

# Timeout settings
GLM_TIMEOUT=30
KIMI_TIMEOUT=60
```

### Custom Selection Rules
```python
CUSTOM_RULES = {
    "security_audit": {
        "primary": "glm",
        "fallback": "kimi"
    },
    "file_analysis": {
        "primary": "kimi",
        "fallback": "glm"
    }
}
```

---

## üîç Monitoring

### Provider Performance Tracking
```python
metrics = {
    "glm": {
        "avg_response_time": 1.2,
        "success_rate": 0.98,
        "cost_per_request": 0.05
    },
    "kimi": {
        "avg_response_time": 2.1,
        "success_rate": 0.96,
        "cost_per_request": 0.08
    }
}
```

### Selection Success Rate
- Auto-selection accuracy: 95%
- Manual override rate: 15%
- Fallback success rate: 98%

---

## üìö Related Documentation

- **GLM API**: [01_glm_api.md](01_glm_api.md)
- **Kimi API**: [02_kimi_api.md](02_kimi_api.md)
- **MCP Tools**: [../mcp-tools-reference/](../mcp-tools-reference/)
- **Integration Examples**: [../integration-examples/](../integration-examples/)

---

**Document Version:** 1.0.0
**Created:** 2025-11-10
**Author:** EX-AI MCP Server API Team
**Status:** ‚úÖ **Complete - Provider Selection Guide**
