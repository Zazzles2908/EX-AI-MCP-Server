# File Handling Architecture - CORRECTED

**Created:** 2025-10-27 19:20 AEDT  
**Purpose:** Document the CORRECT three-method file handling architecture  
**Status:** âœ… COMPLETE - Validated with EXAI

---

## ğŸ¯ **EXECUTIVE SUMMARY**

**CRITICAL DISCOVERY:** The system has **THREE** file handling methods, not two!

1. **Method 1: Direct Embedding** - `chat_EXAI-WS` with `files` parameter (universal)
2. **Method 2: Kimi Upload+Chat** - `kimi_upload_files` + `kimi_chat_with_files` (Kimi only)
3. **Method 3: GLM Upload+Chat** - `glm_upload_file` + `glm_multi_file_chat` (GLM only)

**What I Missed:**
- `glm_multi_file_chat` tool EXISTS and is fully functional!
- GLM DOES support file upload and chat (just like Kimi)
- `chat_EXAI-WS` embeds files as TEXT, not uploads to platform

---

## ğŸ“Š **DECISION MATRIX**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     â”‚ Method 1      â”‚ Method 2      â”‚ Method 3      â”‚
â”‚                     â”‚ (Embed)       â”‚ (Kimi)        â”‚ (GLM)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tool                â”‚ chat_EXAI-WS  â”‚ kimi_*        â”‚ glm_*         â”‚
â”‚ File Size           â”‚ <5KB          â”‚ >5KB          â”‚ >5KB          â”‚
â”‚ Model Choice        â”‚ Any           â”‚ Kimi only     â”‚ GLM only      â”‚
â”‚ Multi-turn          â”‚ âŒ             â”‚ âœ…             â”‚ âœ…             â”‚
â”‚ Persistence         â”‚ âŒ             â”‚ âœ…             â”‚ âœ…             â”‚
â”‚ Token Efficiency    â”‚ âŒ             â”‚ âœ…             â”‚ âœ…             â”‚
â”‚ File Limit          â”‚ Context limit â”‚ 100MB         â”‚ 20MB          â”‚
â”‚ Timeout Config      â”‚ N/A           â”‚ KIMI_MF_*     â”‚ GLM_MF_*      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ **METHOD 1: DIRECT EMBEDDING**

### **Tool:** `chat_EXAI-WS` with `files` parameter

### **How It Works:**
1. Reads file content from disk
2. Embeds content as TEXT directly in prompt
3. Sends to ANY model (GLM or Kimi)
4. No platform upload required

### **Example:**
```python
chat_EXAI-WS(
    prompt="Analyze this code",
    files=["c:\\Project\\file.py"],  # Content embedded as text
    model="glm-4.6"  # OR "kimi-k2-0905-preview" - ANY model!
)
```

### **When to Use:**
- âœ… Small files (<5KB)
- âœ… Single-use interactions
- âœ… Quick code snippets
- âœ… When you want model flexibility

### **Limitations:**
- âŒ High token consumption for large files
- âŒ No file persistence
- âŒ Limited by context window

---

## ğŸ”§ **METHOD 2: KIMI UPLOAD+CHAT**

### **Tools:** `kimi_upload_files` + `kimi_chat_with_files`

### **How It Works:**
1. Upload file to Moonshot platform storage
2. Receive file_id from Moonshot
3. Chat with Kimi model using file_id
4. Kimi model accesses file from Moonshot storage

### **Example:**
```python
# Step 1: Upload
upload_result = kimi_upload_files(
    files=["c:\\Project\\large_file.py"]
)
# Returns: {"file_ids": ["moonshot_file_id_123"]}

# Step 2: Chat
kimi_chat_with_files(
    prompt="Analyze this implementation",
    file_ids=upload_result['file_ids'],
    model="kimi-k2-0905-preview"  # MUST be Kimi model
)
```

### **When to Use:**
- âœ… Large files (>5KB)
- âœ… Multi-turn conversations with same file
- âœ… When using Kimi models
- âœ… When you need file persistence

### **Configuration:**
```env
KIMI_MF_CHAT_TIMEOUT_SECS=180  # Default: 180s
```

### **Limitations:**
- âŒ Only works with Kimi models
- âŒ Files stored on Moonshot platform only
- âŒ Cannot be accessed by GLM models

---

## ğŸ”§ **METHOD 3: GLM UPLOAD+CHAT**

### **Tools:** `glm_upload_file` + `glm_multi_file_chat`

### **How It Works:**
1. Upload file to Z.ai platform storage
2. Receive file_id from Z.ai
3. Chat with GLM model using file_id
4. GLM model accesses file from Z.ai storage

### **Example:**
```python
# Option A: Upload then chat
file_id = glm_upload_file(
    file="c:\\Project\\large_file.py",
    purpose="agent"
)
# Returns: {"file_id": "glm_file_id_456"}

# Option B: Upload and chat in one call
glm_multi_file_chat(
    files=["c:\\Project\\large_file.py"],
    prompt="Analyze this implementation",
    model="glm-4.6"  # MUST be GLM model
)
```

### **When to Use:**
- âœ… Large files (>5KB)
- âœ… Multi-turn conversations with same file
- âœ… When using GLM models
- âœ… When you need file persistence

### **Configuration:**
```env
GLM_MF_CHAT_TIMEOUT_SECS=60  # Default: 60s
```

### **Limitations:**
- âŒ Only works with GLM models
- âŒ Files stored on Z.ai platform only
- âŒ Cannot be accessed by Kimi models
- âŒ 20MB file limit (vs Kimi's 100MB)

---

## ğŸŒ³ **DECISION TREE**

```
START: Need to analyze file with AI
â”‚
â”œâ”€ Is file <5KB AND single interaction?
â”‚   â””â”€ YES â†’ Use Method 1 (chat_EXAI-WS with files)
â”‚
â”œâ”€ Is file >5KB OR multi-turn needed?
â”‚   â”‚
â”‚   â”œâ”€ Using Kimi model?
â”‚   â”‚   â””â”€ YES â†’ Use Method 2 (kimi_upload_files + kimi_chat_with_files)
â”‚   â”‚
â”‚   â””â”€ Using GLM model?
â”‚       â””â”€ YES â†’ Use Method 3 (glm_upload_file + glm_multi_file_chat)
```

---

## âŒ **COMMON MISTAKES**

### **Mistake 1: Confusing Embedding with Upload**
```python
# âŒ WRONG - Thinking chat_EXAI-WS uploads to platform
chat_EXAI-WS(
    prompt="Analyze this",
    files=["file.py"],  # This EMBEDS as text, doesn't upload!
    model="glm-4.6"
)
# Then trying to access with kimi_chat_with_files - WON'T WORK!
```

### **Mistake 2: Cross-Platform File Access**
```python
# âŒ WRONG - Upload to Kimi, try to access with GLM
upload_result = kimi_upload_files(files=["file.py"])
glm_multi_file_chat(
    files=["file.py"],
    prompt="Analyze",
    model="glm-4.6"  # Can't access Kimi files!
)
```

### **Mistake 3: Not Knowing glm_multi_file_chat Exists**
```python
# âŒ SUBOPTIMAL - Using embedding for large file with GLM
chat_EXAI-WS(
    prompt="Analyze this",
    files=["large_file.py"],  # >5KB - wastes tokens!
    model="glm-4.6"
)

# âœ… CORRECT - Use GLM upload+chat
glm_multi_file_chat(
    files=["large_file.py"],
    prompt="Analyze this",
    model="glm-4.6"  # Token efficient!
)
```

---

## ğŸ“‹ **TOOL REFERENCE**

### **Universal Tools (Any Model)**
| Tool | Purpose | File Handling |
|------|---------|---------------|
| `chat_EXAI-WS` | General chat | Embeds files as text |

### **Kimi-Specific Tools**
| Tool | Purpose | File Handling |
|------|---------|---------------|
| `kimi_upload_files` | Upload to Moonshot | Returns file_ids |
| `kimi_chat_with_files` | Chat with uploaded files | Uses file_ids |

### **GLM-Specific Tools**
| Tool | Purpose | File Handling |
|------|---------|---------------|
| `glm_upload_file` | Upload to Z.ai | Returns file_id |
| `glm_multi_file_chat` | Upload and chat | Uploads + chats in one call |

---

## ğŸ”§ **CONFIGURATION**

### **Kimi Timeouts**
```env
KIMI_MF_CHAT_TIMEOUT_SECS=180  # Default: 180s (3 minutes)
```

### **GLM Timeouts**
```env
GLM_MF_CHAT_TIMEOUT_SECS=60  # Default: 60s (1 minute)
```

### **File Limits**
- **Kimi**: 100MB per file
- **GLM**: 20MB per file
- **Embedding**: Limited by context window

---

## âœ… **BEST PRACTICES**

1. **Use Method 1 for small files** - Fastest, most flexible
2. **Use Method 2 for large files with Kimi** - Token efficient, persistent
3. **Use Method 3 for large files with GLM** - Token efficient, persistent
4. **Don't mix platforms** - Files uploaded to one platform can't be accessed by the other
5. **Configure timeouts appropriately** - Increase for large files
6. **Reuse uploaded files** - Upload once, query multiple times

---

## ğŸ“š **DOCUMENTATION TO UPDATE**

1. âœ… **PLATFORM_ARCHITECTURE_CLARIFICATION.md** - Updated with three methods
2. â³ **AGENT_CAPABILITIES.md** - Add `glm_multi_file_chat` documentation
3. â³ **SYSTEM_CAPABILITIES_OVERVIEW.md** - Add GLM file chat capability
4. â³ **EXAI_TOOL_DECISION_GUIDE.md** - Update decision matrix
5. â³ **QUICK_REFERENCE_EXAI_USAGE.md** - Add GLM file chat examples

---

## ğŸ¯ **KEY TAKEAWAYS**

1. **THREE methods exist**, not two!
2. **GLM DOES support file upload+chat** via `glm_multi_file_chat`
3. **`chat_EXAI-WS` embeds files as text**, doesn't upload to platform
4. **Platforms are separate** - Kimi files â‰  GLM files
5. **Choose method based on file size and model** - Use decision tree

---

**Status:** âœ… COMPLETE - Architecture correctly understood and documented  
**Next Steps:** Update all documentation and test GLM file upload+chat workflow

