# Phase 3.2: Kimi Provider Separation Plan

**Date**: 2025-09-30  
**File**: `src/providers/kimi.py` (550 lines)  
**Target**: Split into 4 focused modules  
**Status**: PLAN READY (Based on EXAI analyze_EXAI-WS analysis)

---

## üìä EXAI Analysis Results

**Analysis Tool**: `analyze_EXAI-WS` (3-step systematic analysis)  
**Continuation ID**: d51fc448-c7f0-493a-a93d-6efd131b3018  
**Confidence**: CERTAIN  
**Recommendation**: 4-module split with 88% reduction in main file

---

## üéØ Current State Analysis

### File Structure (550 lines)
1. **SUPPORTED_MODELS** (lines 28-187, ~160 lines) - 11 model configurations
2. **__init__** (lines 189-211, ~23 lines) - Timeout configuration
3. **Standard provider methods** (lines 213-277, ~65 lines)
4. **Cache token management** (lines 279-326, ~48 lines) - LRU cache with TTL
5. **File upload** (lines 328-364, ~37 lines)
6. **Chat functionality** (lines 365-527, ~163 lines) - Custom wrapper with cache
7. **generate_content** (lines 529-550, ~22 lines) - Delegates to base

### Key Characteristics
- **Inherits from**: `OpenAICompatibleProvider` (different from glm.py)
- **Unique Feature**: Cache token management (LRU + TTL)
- **Chat Pattern**: Custom wrapper with idempotency and cache headers
- **File Upload**: Similar to glm.py but with Moonshot-specific purpose tags

---

## üéØ Proposed Module Split

### Module 1: kimi_config.py (~225 lines)
**Purpose**: Model configuration and standard provider methods

**Contents**:
- `SUPPORTED_MODELS` dictionary (11 models, ~160 lines)
- `get_all_model_aliases()` - Extract model aliases
- `get_capabilities()` - Get model capabilities with fallback
- `count_tokens()` - Language-aware token counting (CJK support)
- Helper functions for model validation

**Key Features**:
- Centralized model configuration
- Language-aware token estimation
- Graceful fallback for unknown models
- Clean separation of configuration logic

### Module 2: kimi_cache.py (~50 lines)
**Purpose**: Cache token management (Kimi-specific feature)

**Contents**:
- `_lru_key()` - Generate cache key
- `save_cache_token()` - Save token with TTL
- `get_cache_token()` - Retrieve cached token
- `_purge_cache_tokens()` - LRU + TTL cleanup

**Key Features**:
- LRU cache with configurable max size
- TTL-based expiration
- Thread-safe token management
- Automatic cleanup

### Module 3: kimi_files.py (~40 lines)
**Purpose**: File upload functionality

**Contents**:
- `upload_file()` - Upload files to Moonshot API

**Key Features**:
- Path resolution (relative ‚Üí absolute)
- Client-side size validation (KIMI_FILES_MAX_SIZE_MB)
- Moonshot-specific purpose tags
- Robust error handling with helpful messages

### Module 4: kimi_chat.py (~170 lines)
**Purpose**: Chat wrapper with cache integration

**Contents**:
- `_prefix_hash()` - Generate message prefix hash
- `chat_completions_create()` - Chat with cache token handling

**Key Features**:
- Idempotency key support
- Cache token attachment and capture
- Header size validation
- Tool/tool_choice sanitization
- Raw response handling for header extraction

### Module 5: kimi.py (main file, ~65 lines)
**Purpose**: Main provider class and delegation

**Contents**:
- `KimiModelProvider` class definition
- `__init__` method with timeout configuration
- `get_provider_type` method
- `validate_model_name` method
- `supports_thinking_mode` method
- `list_models` method
- `get_model_configurations` method
- Delegation to specialized modules
- `generate_content` - delegates to OpenAI-compatible base

**Key Features**:
- Thin wrapper maintaining backward compatibility
- Clean imports and delegation
- Inherits from OpenAICompatibleProvider
- All public methods preserved

---

## üîß Implementation Steps

### Step 1: Create kimi_config.py
1. Extract `SUPPORTED_MODELS` dictionary
2. Extract standard provider methods
3. Extract `count_tokens` method
4. Create module with all configuration logic

### Step 2: Create kimi_cache.py
1. Extract cache token management methods
2. Include LRU and TTL logic
3. Maintain class-level cache storage
4. Preserve thread-safety

### Step 3: Create kimi_files.py
1. Extract `upload_file` method
2. Include path resolution logic
3. Maintain size validation
4. Preserve error handling

### Step 4: Create kimi_chat.py
1. Extract `_prefix_hash` method
2. Extract `chat_completions_create` method
3. Include cache token integration
4. Maintain header handling

### Step 5: Refactor kimi.py
1. Create backup: `kimi_BACKUP.py`
2. Import from new modules
3. Update class to delegate to modules
4. Remove extracted code
5. Maintain backward compatibility

### Step 6: Testing
1. Restart server
2. Test Kimi chat functionality
3. Test Kimi file upload
4. Test cache token management
5. Verify model configuration queries

---

## üìã Expected Results

| Metric | Before | After | Change |
|--------|--------|-------|--------|
| **Main File Lines** | 550 | ~65 | -485 (-88.2%) |
| **kimi_config.py** | - | ~225 | +225 |
| **kimi_cache.py** | - | ~50 | +50 |
| **kimi_files.py** | - | ~40 | +40 |
| **kimi_chat.py** | - | ~170 | +170 |
| **Total Lines** | 550 | ~550 | ~0 (+0%) |
| **Modules** | 1 | 5 | +4 |

**Note**: Total lines remain similar but organization is significantly better.

---

## ‚úÖ Benefits

### 1. Separation of Concerns ‚úÖ
- Configuration isolated
- Cache management isolated
- File upload isolated
- Chat wrapper isolated
- Each module has single responsibility

### 2. Maintainability ‚úÖ
- Easier to understand each module
- Easier to test individual concerns
- Easier to modify without affecting others
- Clear module boundaries

### 3. Testability ‚úÖ
- Can test cache management independently
- Can test file upload independently
- Can test chat wrapper independently
- Easier to mock dependencies

### 4. Scalability ‚úÖ
- Easy to add new models
- Easy to enhance cache logic
- Easy to add new file operations
- Room for future growth

---

## ‚ö†Ô∏è Considerations

### Backward Compatibility
- **CRITICAL**: All existing imports must continue to work
- Main `KimiModelProvider` class must maintain same interface
- All public methods must remain accessible
- No breaking changes to external callers

### Dependencies
- kimi_chat.py depends on kimi_cache.py (for cache token management)
- kimi_config.py is independent
- kimi_files.py is independent
- kimi.py imports all modules and delegates

### Testing Requirements
- Test chat generation with cache tokens
- Test file upload
- Test model configuration queries
- Test cache token LRU and TTL
- Test backward compatibility

---

## üéØ Success Criteria

- ‚úÖ Main file reduced to ~65 lines (88.2% reduction)
- ‚úÖ 4 focused modules created
- ‚úÖ All functionality tested and working
- ‚úÖ Zero breaking changes
- ‚úÖ 100% backward compatibility
- ‚úÖ Clean separation of concerns

---

**Status**: PLAN READY - Ready for implementation  
**Estimated Time**: 40-50 minutes  
**Risk Level**: LOW (clear boundaries, EXAI-validated strategy)  
**Based On**: EXAI analyze_EXAI-WS 3-step analysis (continuation_id: d51fc448-c7f0-493a-a93d-6efd131b3018)

