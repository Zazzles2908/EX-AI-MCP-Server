# Raw output: kimi_upload_and_extract (docs/validation/provider_params.md)

Returned messages array (exact):

[
  {
    "role": "system",
    "content": "{\"content\":\"# Provider Parameter Shapes (Validated)\\n\\nDate: 2025-09-28\\n\\nThis document records minimal JSON payloads and key links for validated provider calls used by EXAI WS daemon + manager.\\n\\n## GLM Agents\\n\\n### 1) Agent Chat (glm_agent_chat)\\n- Endpoint: POST /api/v1/agents\\n- Minimal arguments (tool):\\n```json\\n{\\n  \\\"agent_id\\\": \\\"general_translation\\\",\\n  \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"hello\\\"}],\\n  \\\"temperature\\\": 0.2\\n}\\n```\\n- Response contains: `id` (often usable as `conversation_id`), `agent_id`, `status`, `choices`, `usage`.\\n\\n### 2) Conversation History (glm_agent_conversation)\\n- Endpoint: POST /api/v1/agents/conversation\\n- Minimal arguments (tool):\\n```json\\n{\\n  \\\"agent_id\\\": \\\"general_translation\\\",\\n  \\\"conversation_id\\\": \\\"\\u003cfrom agent_chat id or conversation_id\\u003e\\\",\\n  \\\"page\\\": 1,\\n  \\\"page_size\\\": 10\\n}\\n```\\n- Notes: prefer `conversation_id` if present; some responses only include `id`. We treat `id` as fallback for `conversation_id`.\\n\\n### 3) Async Result (glm_agent_get_result)\\n- Endpoint: POST /api/v1/agents/async-result\\n- Minimal arguments (tool):\\n```json\\n{\\n  \\\"agent_id\\\": \\\"\\u003cagent\\\"\\\",\\n  \\\"async_id\\\": \\\"\\u003creturned async id\\\"\\n}\\n```\\n\\n## GLM Chat (manager path with web search)\\n- Endpoint: POST /api/paas/v4/chat/completions (provider HTTP)\\n- Manager tool: `chat` with `use_websearch=true` triggers provider web browsing.\\n- Minimal payload concept (effective fields; manager abstracts this):\\n```json\\n{\\n  \\\"model\\\": \\\"glm-4.5-flash\\\",\\n  \\\"messages\\\": [{\\\"role\\\":\\\"user\\\",\\\"content\\\":\\\"...\\\"}],\\n  \\\"tools\\\": [{\\\"type\\\":\\\"web_search\\\", \\\"params\\\": {\\\"search_query\\\":\\\"...\\\"}}],\\n  \\\"tool_choice\\\": \\\"auto\\\"\\n}\\n```\\n\\n## GLM Web Search (direct tool, optional)\\n- Endpoint: POST https://api.z.ai/api/paas/v4/web_search\\n- Minimal arguments (tool):\\n```json\\n{\\n  \\\"search_engine\\\": \\\"search-prime\\\",\\n  \\\"search_query\\\": \\\"\\u003cquery\\\"\\\",\\n  \\\"count\\\": 5\\n}\\n```\\n- Returns: JSON with search results array. (Manager path via `chat(use_websearch=true)` is preferred.)\\n\\n## Kimi Files\\n\\n### 1) Upload + Extract (kimi_upload_and_extract)\\n- Endpoints: POST /v1/files (upload), GET /v1/files/{id}/content (download/extract)\\n- Minimal arguments (tool):\\n```json\\n{\\n  \\\"file_path\\\": \\\"\\u003cabsolute or repo-relative\\\"\\\",\\n  \\\"extract\\\": true\\n}\\n```\\n- Response: array of assistant/system messages including extracted content.\\n\\n## Manager-side Conversation Meta Envelope\\n- The GLM Agent Chat tool appends a second text output:\\n```json\\n{\\\"hint\\\":\\\"conversation_meta\\\",\\\"agent_id\\\":\\\"...\\\",\\\"conversation_id\\\":\\\"...\\\"}\\n```\\n- Manager may, when user intent indicates, lazily fetch a small window of prior messages with `glm_agent_conversation`.\\n\\n## Links (add official references)\\n- ZhipuAI GLM Chat Completions: https://open.bigmodel.cn/api/paas/v4/chat/completions\\n- ZhipuAI Agents (platform): /api/v1/agents, /api/v1/agents/async-result, /api/v1/agents/conversation (see ZhipuAI docs)\\n- Moonshot Kimi Files: https://api.moonshot.ai/v1/files\\n\\n\\n\\n\\n## Implementation Locations (Functions \\u0026 Scripts)\\n\\n- server.py\\n  - register_provider_specific_tools: registers provider tools when GLM/Kimi available (glm_agent_chat, glm_agent_get_result, glm_agent_conversation, kimi_upload_and_extract, kimi_multi_file_chat)\\n- tools/providers/glm/glm_agents.py\\n  - GLMAgentChatTool: posts to /api/v1/agents and appends conversation_meta envelope\\n  - GLMAgentConversationTool: posts to /api/v1/agents/conversation\\n  - GLMAgentGetResultTool: posts to /api/v1/agents/async-result\\n- tools/providers/glm/glm_web_search.py (optional direct wrapper)\\n  - GLMWebSearchTool: POST https://api.z.ai/api/paas/v4/web_search (manager path via chat(use_websearch=true) preferred)\\n- scripts/diagnostics/ws_probe.py\\n  - Connects to WS daemon, enumerates tools, runs smoke calls; captures conversation_id organically and verifies glm_agent_conversation; saves artifacts\\n- docs/external_review/\\n  - 20250928_ws_probe_run.md: live probe outputs\\n  - 20250928_glm_agent_session.json: captured agent_id + conversation_id + raw response\\n  - 20250928_exai_websearch_raw.md: exact EXAI-WS websearch output snapshot\\n","file_type":"text/plain","filename":"provider_params.md","title":"","type":"file"}",
    "_file_id": "d3c7mlf37oq66hiolveg"
  }
]

