**[HOME MODE]** ‚Üí Comprehensive system audit with developer remediation plan

---

# EXAI-MCP SYSTEM AUDIT REPORT
**Date:** 2025-10-10  
**Auditor:** Claude (Anthropic) via extended tool testing  
**Scope:** End-to-end request flow, logging infrastructure, observability, model behavior  
**Test Methodology:** Live tool calls + log analysis + architecture review

---

## EXECUTIVE SUMMARY

**System Status:** üî¥ **CRITICAL ISSUES DETECTED**

Tested EXAI-MCP chat tool with identical query across two models (GLM-4.5-flash vs Kimi-latest-128k). System successfully routes requests and returns responses, but **observability infrastructure is fundamentally broken**. You have a car that drives but no speedometer, fuel gauge, or GPS.

**Key Findings:**
- ‚úÖ **Core functionality works** - Tools execute, models respond, results return
- ‚ùå **Observability is cosmetic** - Logs capture 10-20% of critical data
- ‚ùå **Cannot trace workflows** - Zero request correlation capability
- ‚ùå **Cannot measure costs** - Token data missing or estimated
- ‚ùå **Cannot debug failures** - Sub-tool calls invisible in logs
- ‚ùå **Cannot reproduce issues** - Insufficient context capture

**Business Impact:**
- **Development velocity:** Cannot debug production issues effectively
- **Cost management:** No way to track API spend per user/session/tool
- **Compliance risk:** Incomplete audit trail for regulated industries
- **Scaling blocked:** Cannot identify bottlenecks or optimize performance

---

## CRITICAL FLAGS (P0) - SYSTEM BREAKING

### üö® FLAG #1: NULL REQUEST_ID - ZERO WORKFLOW CORRELATION

**Severity:** CRITICAL  
**Impact:** Cannot trace multi-tool conversations, retry attempts, or user sessions  
**Evidence:** Every log entry shows `"request_id": null`

#### Root Cause Analysis
```python
# src/server/handlers/request_handler.py
async def SERVER_HANDLE_CALL_TOOL(...):
    # No request_id generation here
    req_id = None  # ‚Üê PROBLEM: Never set
    
    # Passed to downstream functions but always null
    result = await execute_tool(tool_name, arguments, req_id)
```

Your architecture document shows requests flow through 7 layers but **no layer generates a correlation ID**. This is like FedEx shipping packages without tracking numbers.

#### Investigation Steps
1. **Grep for request_id usage:**
   ```bash
   cd /path/to/exai-mcp
   grep -r "request_id" src/ --include="*.py" | grep -v ".pyc"
   ```
   Confirm it's declared but never assigned a value.

2. **Check if UUIDs are imported anywhere:**
   ```bash
   grep -r "import uuid" src/ --include="*.py"
   ```
   If zero results, UUID generation is missing entirely.

3. **Verify log entries:**
   ```bash
   grep "request_id" logs/toolcalls.jsonl | head -5
   ```
   Confirm all are null.

#### Fix Implementation

**File:** `src/server/handlers/request_handler.py`

```python
import uuid
from datetime import datetime, timezone

async def SERVER_HANDLE_CALL_TOOL(name: str, arguments: dict, req_id=None):
    """
    Main entry point for tool execution.
    Generates correlation ID if not provided.
    """
    # Generate request_id at the boundary
    if req_id is None:
        req_id = str(uuid.uuid4())
    
    # Add metadata injection
    if "_request_metadata" not in arguments:
        arguments["_request_metadata"] = {
            "request_id": req_id,
            "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "timestamp_unix": time.time(),
            "timezone": "Australia/Melbourne",  # From config
            "entry_point": "mcp_stdio"  # or "websocket" or "http"
        }
    
    # Pass req_id to all downstream calls
    tool_obj = get_tool(name)
    result = await execute_tool_with_context(
        tool_obj, 
        arguments, 
        req_id=req_id  # ‚Üê Now guaranteed non-null
    )
    
    return result
```

**File:** `src/daemon/ws_server.py` (WebSocket entry point)

```python
async def handle_call_tool(ws, data):
    """WebSocket tool call handler"""
    tool_name = data.get("name")
    arguments = data.get("arguments", {})
    
    # Generate req_id at WebSocket boundary
    req_id = str(uuid.uuid4())
    
    # Route to handler with req_id
    result = await SERVER_HANDLE_CALL_TOOL(tool_name, arguments, req_id)
    
    # Include req_id in response
    await ws.send(json.dumps({
        "result": result,
        "request_id": req_id,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }))
```

**File:** `src/utils/logging.py` (Update log capture)

```python
def log_tool_call(tool_name, arguments, duration, result, req_id):
    """Structured logging with guaranteed request_id"""
    log_entry = {
        "request_id": req_id,  # Now never null
        "timestamp_unix": time.time(),
        "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "timestamp_aedt": datetime.now(MELBOURNE_TZ).strftime("%Y-%m-%d %H:%M:%S %Z"),
        "tool": tool_name,
        "duration_s": round(duration, 3),
        "arguments_hash": hashlib.sha256(json.dumps(arguments, sort_keys=True).encode()).hexdigest()[:16],
        "result_preview": str(result)[:500],
        "result_type": type(result).__name__
    }
    
    with open("logs/toolcalls.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")
```

#### Validation Criteria
- [ ] Every log entry has non-null `request_id`
- [ ] Same `request_id` appears in all logs for a single user query
- [ ] Can grep logs by `request_id` and reconstruct full workflow
- [ ] WebSocket responses include `request_id` for client-side correlation

**Estimated Effort:** 2 hours  
**Risk:** Low (additive change, no breaking modifications)

---

### üö® FLAG #2: DUPLICATE LOG ENTRIES - 50% STORAGE WASTE

**Severity:** CRITICAL  
**Impact:** Log analysis produces false metrics, storage costs doubled  
**Evidence:** Lines 1-2 and 19-20 in toolcalls.jsonl are byte-identical

#### Root Cause Analysis
```python
# Hypothesis: Logging triggered at multiple lifecycle points
# 1. When request enters daemon
# 2. When request exits daemon
# OR
# 1. When tool execution starts
# 2. When tool execution completes
```

Your logging is probably hooked at **both** request receipt and response send, creating duplicate entries with identical timestamps (impossible unless it's a write-after-write race condition).

#### Investigation Steps
1. **Find all log write locations:**
   ```bash
   grep -r "toolcalls.jsonl" src/ --include="*.py" -B 5 -A 5
   ```
   Count how many places write to this file.

2. **Check for event listeners:**
   ```bash
   grep -r "log_tool_call\|write_log\|append_log" src/ --include="*.py"
   ```
   Identify if multiple functions call logging.

3. **Test with unique marker:**
   Add `marker=str(uuid.uuid4())` to log entries, run test, check if duplicates have same marker (proves same event logged twice).

#### Fix Implementation

**Option A: Deduplication at Write Time (Quick Fix)**

```python
# src/utils/logging.py
import hashlib

_recent_log_hashes = {}  # Cache last 100 log entries
_max_cache_size = 100

def log_tool_call(tool_name, arguments, duration, result, req_id):
    """Deduplicated logging"""
    log_entry = {
        "request_id": req_id,
        "timestamp_unix": time.time(),
        "tool": tool_name,
        "duration_s": duration,
        # ... other fields
    }
    
    # Generate content hash (exclude timestamp for dedup)
    dedup_key = f"{req_id}:{tool_name}:{duration}"
    entry_hash = hashlib.md5(dedup_key.encode()).hexdigest()
    
    # Check if we logged this recently
    if entry_hash in _recent_log_hashes:
        current_time = time.time()
        last_logged = _recent_log_hashes[entry_hash]
        
        # If logged within 1 second, it's a duplicate
        if current_time - last_logged < 1.0:
            return  # Skip duplicate write
    
    # Write and cache
    with open("logs/toolcalls.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")
    
    _recent_log_hashes[entry_hash] = time.time()
    
    # Prune cache
    if len(_recent_log_hashes) > _max_cache_size:
        oldest = min(_recent_log_hashes.items(), key=lambda x: x[1])
        del _recent_log_hashes[oldest[0]]
```

**Option B: Single Logging Point (Correct Fix)**

Find where logging is called multiple times and remove redundant calls:

```python
# src/server/handlers/request_handler.py
async def SERVER_HANDLE_CALL_TOOL(name: str, arguments: dict, req_id=None):
    start_time = time.time()
    
    try:
        result = await execute_tool(name, arguments, req_id)
        duration = time.time() - start_time
        
        # Log ONCE at completion
        log_tool_call(name, arguments, duration, result, req_id)  # ‚Üê Single write point
        
        return result
    except Exception as e:
        duration = time.time() - start_time
        log_tool_call(name, arguments, duration, {"error": str(e)}, req_id)
        raise
```

Remove any other `log_tool_call()` invocations in:
- `ws_server.py`
- `request_handler_execution.py`
- Tool `execute()` methods

#### Validation Criteria
- [ ] Zero duplicate entries in new logs
- [ ] Single log entry per tool call (verified by request_id)
- [ ] Storage growth rate reduced by 50%
- [ ] Existing duplicates cleaned with dedup script

**Estimated Effort:** 3 hours (investigation + fix + validation)  
**Risk:** Medium (must verify no logging gaps created)

---

### üö® FLAG #3: NESTED JSON STRINGIFICATION - UNPARSEABLE LOGS

**Severity:** CRITICAL  
**Impact:** Cannot build observability dashboards, manual log inspection only  
**Evidence:** `"result_preview": "[TextContent(type='text', text='{\"status\"...')]"`

#### Root Cause Analysis
Your logging captures `TextContent` objects by calling `str()` on them:

```python
# Current (broken):
log_entry["result_preview"] = str(result)
# Produces: "[TextContent(type='text', text='{\"status\":...')]"
```

This creates **4 serialization layers:**
1. Original response object (Python dict)
2. Wrapped in MCP `TextContent` object
3. Python `repr()` string representation
4. JSON-escaped when written to JSONL

**To parse this programmatically:**
```python
import json, ast

raw = log_entry["result_preview"]
outer = ast.literal_eval(raw)  # Parse Python repr
text_content = outer[0].text   # Extract text field
inner = json.loads(text_content)  # Parse inner JSON
status = inner["status"]  # Finally access data
```

This is **architectural malpractice**. Logs should be structured data, not text dumps.

#### Investigation Steps
1. **Find TextContent creation:**
   ```bash
   grep -r "TextContent" src/ --include="*.py" -B 3 -A 3
   ```
   Understand where MCP protocol objects wrap responses.

2. **Identify logging interception point:**
   ```bash
   grep -r "result_preview\|log_entry\[" src/utils/logging.py -B 10
   ```
   Find where `result` is serialized.

3. **Check if raw result is accessible:**
   Trace back from log call to tool execution - does raw dict exist before TextContent wrapping?

#### Fix Implementation

**File:** `src/utils/logging.py`

```python
def extract_structured_result(result):
    """
    Convert MCP TextContent objects to structured data.
    Handles nested JSON, error responses, and plain text.
    """
    # Case 1: Already structured
    if isinstance(result, dict):
        return result
    
    # Case 2: List of TextContent objects (MCP response)
    if isinstance(result, list):
        extracted = {}
        for item in result:
            if hasattr(item, 'text'):
                try:
                    # Try parsing as JSON
                    parsed = json.loads(item.text)
                    extracted.update(parsed)
                except json.JSONDecodeError:
                    # Plain text response
                    extracted["text_content"] = item.text
        return extracted
    
    # Case 3: Single TextContent object
    if hasattr(result, 'text'):
        try:
            return json.loads(result.text)
        except json.JSONDecodeError:
            return {"text_content": result.text}
    
    # Case 4: Unknown type
    return {"raw": str(result), "type": type(result).__name__}

def log_tool_call(tool_name, arguments, duration, result, req_id):
    """Structured logging with flattened JSON"""
    
    # Extract structured data
    structured_result = extract_structured_result(result)
    
    log_entry = {
        "request_id": req_id,
        "timestamp_unix": time.time(),
        "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "tool": tool_name,
        "duration_s": round(duration, 3),
        
        # Structured result (no nesting!)
        "result": {
            "status": structured_result.get("status", "unknown"),
            "content_preview": structured_result.get("content", "")[:500],
            "content_length": len(str(structured_result.get("content", ""))),
            "metadata": structured_result.get("metadata", {}),
            "error": structured_result.get("error")
        },
        
        # Arguments (hashed for privacy if needed)
        "arguments": {
            "model": arguments.get("model"),
            "prompt_preview": str(arguments.get("prompt", ""))[:200],
            "prompt_length": len(str(arguments.get("prompt", ""))),
            "use_websearch": arguments.get("use_websearch", False),
            "files_count": len(arguments.get("files", []))
        }
    }
    
    with open("logs/toolcalls.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")
```

Now you can query logs directly:
```python
import json
with open("logs/toolcalls.jsonl") as f:
    for line in f:
        entry = json.loads(line)  # Single parse!
        if entry["result"]["status"] == "error":
            print(f"Request {entry['request_id']} failed")
```

#### Validation Criteria
- [ ] Can parse log file with single `json.loads()` per line
- [ ] Dashboard query: `SELECT AVG(duration_s) FROM logs WHERE tool='chat'` works
- [ ] No escape characters in stored JSON
- [ ] Result structure matches provider API response

**Estimated Effort:** 4 hours  
**Risk:** Medium (requires testing with all tool types)

---

## HIGH PRIORITY FLAGS (P1) - FUNCTIONALITY IMPAIRED

### ‚ö†Ô∏è FLAG #4: ZERO TOKEN TELEMETRY - BLIND COST MANAGEMENT

**Severity:** HIGH  
**Impact:** Cannot budget API costs, optimize prompts, or bill users  
**Evidence:** Logs show `"Tokens: ~91"` (estimate, not actual)

#### Root Cause Analysis
Your architecture shows GLM/Kimi providers make API calls, but **response metadata isn't captured**:

```python
# src/providers/glm_chat.py
async def chat(...):
    response = await client.post("/chat/completions", json=payload)
    data = response.json()
    
    # Returns only text content
    return data["choices"][0]["message"]["content"]  # ‚Üê Token data discarded!
```

GLM/Kimi API responses include:
```json
{
  "choices": [...],
  "usage": {
    "prompt_tokens": 234,
    "completion_tokens": 567,
    "total_tokens": 801
  }
}
```

But your system throws away `usage` and only extracts `content`.

#### Investigation Steps
1. **Check provider response handling:**
   ```bash
   grep -r "choices\[0\]\|usage\|total_tokens" src/providers/ --include="*.py"
   ```
   Confirm token data exists but isn't returned.

2. **Test raw API call:**
   ```bash
   curl https://api.z.ai/api/paas/v4/chat/completions \
     -H "Authorization: Bearer $GLM_API_KEY" \
     -d '{"model":"glm-4.5-flash","messages":[{"role":"user","content":"test"}]}'
   ```
   Verify `usage` field is present in response.

3. **Check if metadata is passed anywhere:**
   ```bash
   grep -r "metadata\|usage\|token" src/server/handlers/request_handler.py -B 5 -A 5
   ```

#### Fix Implementation

**File:** `src/providers/glm_chat.py`

```python
async def chat(prompt, system_prompt, model_name, temperature, **kwargs):
    """
    Chat completion with full metadata capture.
    Returns: (content, metadata) tuple
    """
    payload = build_payload(prompt, system_prompt, model_name, temperature, **kwargs)
    
    response = await client.post("/chat/completions", json=payload)
    data = response.json()
    
    # Extract content
    content = data["choices"][0]["message"]["content"]
    
    # Extract metadata
    metadata = {
        "model_used": data.get("model"),
        "provider_used": "glm",
        "tokens": {
            "prompt": data.get("usage", {}).get("prompt_tokens", 0),
            "completion": data.get("usage", {}).get("completion_tokens", 0),
            "total": data.get("usage", {}).get("total_tokens", 0)
        },
        "finish_reason": data["choices"][0].get("finish_reason"),
        "created_at": data.get("created"),
        "tool_call_events": []  # Populated if model called sub-tools
    }
    
    return content, metadata  # ‚Üê Now returns both
```

**File:** `src/providers/kimi.py` (same pattern)

**File:** `src/server/handlers/request_handler_execution.py`

```python
async def execute_tool_with_context(tool_obj, arguments, req_id):
    """Execute tool and capture full metadata"""
    start_time = time.time()
    
    try:
        # Tool returns (content, metadata) tuple
        content, metadata = await tool_obj.execute(arguments)
        duration = time.time() - start_time
        
        # Log with metadata
        log_tool_call(
            tool_obj.get_name(),
            arguments,
            duration,
            content,
            req_id,
            metadata=metadata  # ‚Üê New parameter
        )
        
        return content, metadata
    except Exception as e:
        duration = time.time() - start_time
        log_tool_call(
            tool_obj.get_name(),
            arguments,
            duration,
            {"error": str(e)},
            req_id,
            metadata={"error": True}
        )
        raise
```

**File:** `src/utils/logging.py`

```python
def log_tool_call(tool_name, arguments, duration, result, req_id, metadata=None):
    """Enhanced logging with token telemetry"""
    
    log_entry = {
        "request_id": req_id,
        "timestamp_unix": time.time(),
        "tool": tool_name,
        "duration_s": round(duration, 3),
        
        # Token metrics (from metadata)
        "tokens": metadata.get("tokens", {}) if metadata else {},
        
        # Cost calculation (using provider pricing)
        "cost_usd": calculate_cost(
            metadata.get("model_used"),
            metadata.get("tokens", {})
        ) if metadata else 0.0,
        
        # Model info
        "model": metadata.get("model_used") if metadata else "unknown",
        "provider": metadata.get("provider_used") if metadata else "unknown",
        
        # Result
        "result": extract_structured_result(result)
    }
    
    with open("logs/toolcalls.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")
```

#### Validation Criteria
- [ ] Every log entry has `tokens: {prompt, completion, total}`
- [ ] Can calculate daily API costs: `SUM(cost_usd) GROUP BY DATE(timestamp_utc)`
- [ ] Can identify expensive prompts: `SELECT prompt_preview ORDER BY tokens.total DESC LIMIT 10`
- [ ] Token counts match provider billing statements

**Estimated Effort:** 6 hours (all providers + tools + validation)  
**Risk:** Medium (requires coordinated changes across providers)

---

### ‚ö†Ô∏è FLAG #5: IMPOSSIBLE DURATION VALUES - BROKEN PERFORMANCE METRICS

**Severity:** HIGH  
**Impact:** Cannot identify bottlenecks, optimize performance, or detect timeouts  
**Evidence:** `"duration_s": 0.0` for debug tool (physically impossible)

#### Root Cause Analysis
```python
# Broken timing (likely current implementation):
duration = 0.0  # Hardcoded or initialized before timing starts
log_entry["duration_s"] = duration

# OR timer started after call completes:
result = await tool_call()
start_time = time.time()  # ‚Üê Too late!
duration = time.time() - start_time  # Always ~0
```

0.0s is impossible because:
- Network latency alone: 50-200ms
- JSON serialization: 1-10ms
- Model inference: 100ms-10s
- Minimum realistic duration: **~100ms**

#### Investigation Steps
1. **Find timing code:**
   ```bash
   grep -r "time.time\|duration\|start_time" src/server/handlers/ --include="*.py" -B 3 -A 3
   ```
   Trace where duration is calculated.

2. **Check if async calls are awaited:**
   ```python
   # Wrong: Duration measures scheduling, not execution
   start = time.time()
   result = tool_call()  # Not awaited
   duration = time.time() - start  # Only measures function call overhead
   ```

3. **Test with sleep:**
   Add `await asyncio.sleep(1)` in tool, verify duration shows ~1.0s.

#### Fix Implementation

**File:** `src/server/handlers/request_handler_execution.py`

```python
import time
import asyncio

async def execute_tool_with_context(tool_obj, arguments, req_id):
    """Execute tool with accurate timing"""
    
    # Start timer BEFORE any async operations
    timing = {
        "start": time.time(),
        "phases": {}
    }
    
    try:
        # Phase 1: Validation
        phase_start = time.time()
        validated_args = tool_obj.validate_arguments(arguments)
        timing["phases"]["validation"] = time.time() - phase_start
        
        # Phase 2: Model resolution
        phase_start = time.time()
        model_name = resolve_model(validated_args, tool_obj)
        timing["phases"]["model_resolution"] = time.time() - phase_start
        
        # Phase 3: Tool execution (MUST AWAIT)
        phase_start = time.time()
        content, metadata = await tool_obj.execute(validated_args)  # ‚Üê Correctly awaited
        timing["phases"]["execution"] = time.time() - phase_start
        
        # Phase 4: Response processing
        phase_start = time.time()
        processed = process_response(content, metadata)
        timing["phases"]["processing"] = time.time() - phase_start
        
        # Total duration
        timing["total"] = time.time() - timing["start"]
        
        # Sanity check
        if timing["total"] < 0.01:  # Less than 10ms is suspicious
            logging.warning(f"Suspiciously fast execution: {timing['total']}s for {tool_obj.get_name()}")
        
        # Log with detailed timing
        log_tool_call(
            tool_obj.get_name(),
            arguments,
            timing["total"],
            processed,
            req_id,
            metadata=metadata,
            timing_breakdown=timing["phases"]
        )
        
        return processed, metadata
        
    except Exception as e:
        timing["total"] = time.time() - timing["start"]
        log_tool_call(
            tool_obj.get_name(),
            arguments,
            timing["total"],
            {"error": str(e)},
            req_id,
            metadata={"error": True},
            timing_breakdown=timing.get("phases", {})
        )
        raise
```

**File:** `src/utils/logging.py`

```python
def log_tool_call(tool_name, arguments, duration, result, req_id, metadata=None, timing_breakdown=None):
    """Enhanced logging with performance breakdown"""
    
    log_entry = {
        "request_id": req_id,
        "timestamp_unix": time.time(),
        "tool": tool_name,
        
        # Total duration
        "duration_s": round(duration, 3),
        
        # Performance breakdown
        "timing": {
            "total_ms": round(duration * 1000, 2),
            "validation_ms": round(timing_breakdown.get("validation", 0) * 1000, 2) if timing_breakdown else 0,
            "model_resolution_ms": round(timing_breakdown.get("model_resolution", 0) * 1000, 2) if timing_breakdown else 0,
            "execution_ms": round(timing_breakdown.get("execution", 0) * 1000, 2) if timing_breakdown else 0,
            "processing_ms": round(timing_breakdown.get("processing", 0) * 1000, 2) if timing_breakdown else 0
        },
        
        # Sanity checks
        "performance_flags": {
            "suspiciously_fast": duration < 0.01,
            "timeout_risk": duration > 60.0,
            "slow_execution": timing_breakdown.get("execution", 0) > 30.0 if timing_breakdown else False
        },
        
        # ... rest of log entry
    }
    
    with open("logs/toolcalls.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")
```

#### Validation Criteria
- [ ] No duration values < 0.05s (minimum realistic)
- [ ] Durations match wall-clock time during manual testing
- [ ] Timing breakdown sums to total duration (¬±5% tolerance)
- [ ] Can identify bottlenecks: `SELECT tool, AVG(timing.execution_ms) GROUP BY tool`

**Estimated Effort:** 4 hours  
**Risk:** Low (isolated to timing code)

---

### ‚ö†Ô∏è FLAG #6: INVISIBLE SUB-TOOL CALLS - BLACK BOX AGENT BEHAVIOR

**Severity:** HIGH  
**Impact:** Cannot understand what your AI agent actually did  
**Evidence:** 
- GLM showed 48x `<tool_call>` tags but 0 logged events
- Kimi showed 3x `<search>` queries but only 1 logged event

#### Root Cause Analysis
When you call `exai-mcp:chat`, the flow is:

```
You ‚Üí exai-mcp:chat ‚Üí GLM API ‚Üí [GLM decides to web_search] ‚Üí Search results ‚Üí GLM synthesizes ‚Üí Response
```

Your logs only capture **You ‚Üí exai-mcp:chat ‚Üí Response**. Everything in the middle is invisible.

**Why this happens:**
```python
# src/providers/glm_chat.py
async def chat(prompt, ...):
    payload = {"model": "glm-4.5-flash", "messages": [...]}
    response = await client.post("/chat/completions", json=payload)
    
    # GLM's response includes tool_calls in the API response
    # But your code only extracts the final text
    return response["choices"][0]["message"]["content"]
    # ‚Üë Tool call metadata lost here
```

GLM/Kimi APIs return:
```json
{
  "choices": [{
    "message": {
      "content": "Based on my research...",
      "tool_calls": [
        {"id": "call_abc", "type": "function", "function": {"name": "web_search", "arguments": "{\"query\":\"...\"}"}}
      ]
    }
  }],
  "usage": {...}
}
```

But you discard `tool_calls`.

#### Investigation Steps
1. **Capture raw API response:**
   ```python
   # Add debugging to glm_chat.py
   import json
   response = await client.post(...)
   with open("debug_glm_response.json", "w") as f:
       json.dump(response.json(), f, indent=2)
   ```
   Check if `tool_calls` field exists.

2. **Test with web search:**
   Call chat with `use_websearch=True`, check if GLM's response shows tool usage.

3. **Review provider documentation:**
   Check if ZhipuAI API docs explain how to retrieve tool call history.

#### Fix Implementation

**File:** `src/providers/glm_chat.py`

```python
async def chat(prompt, system_prompt, model_name, temperature, use_websearch=False, **kwargs):
    """
    Enhanced chat with sub-tool tracking.
    Returns: (content, metadata) with tool_call_events populated
    """
    payload = build_payload(prompt, system_prompt, model_name, temperature, **kwargs)
    
    # Enable tool use if requested
    if use_websearch:
        payload["tools"] = [
            {
                "type": "function",
                "function": {
                    "name": "web_search",
                    "description": "Search the web for current information",
                    "parameters": {"type": "object", "properties": {"query": {"type": "string"}}}
                }
            }
        ]
    
    response = await client.post("/chat/completions", json=payload)
    data = response.json()
    
    # Extract content
    message = data["choices"][0]["message"]
    content = message.get("content", "")
    
    # Extract sub-tool calls
    tool_call_events = []
    if "tool_calls" in message:
        for tool_call in message["tool_calls"]:
            tool_call_events.append({
                "tool_call_id": tool_call["id"],
                "tool_name": tool_call["function"]["name"],
                "arguments": json.loads(tool_call["function"]["arguments"]),
                "timestamp": time.time()
            })
    
    # Build metadata
    metadata = {
        "model_used": data.get("model"),
        "provider_used": "glm",
        "tokens": {
            "prompt": data.get("usage", {}).get("prompt_tokens", 0),
            "completion": data.get("usage", {}).get("completion_tokens", 0),
            "total": data.get("usage", {}).get("total_tokens", 0)
        },
        "tool_call_events": tool_call_events,  # ‚Üê Now populated!
        "tool_calls_count": len(tool_call_events)
    }
    
    return content, metadata
```

**File:** `src/utils/logging.py`

```python
def log_tool_call(tool_name, arguments, duration, result, req_id, metadata=None, **kwargs):
    """Enhanced logging with sub-tool tracking"""
    
    log_entry = {
        "request_id": req_id,
        "timestamp_unix": time.time(),
        "tool": tool_name,
        "duration_s": round(duration, 3),
        
        # Sub-tool telemetry
        "sub_tools": metadata.get("tool_call_events", []) if metadata else [],
        "sub_tools_count": len(metadata.get("tool_call_events", [])) if metadata else 0,
        
        # Flag complex queries
        "query_complexity": {
            "used_websearch": len(metadata.get("tool_call_events", [])) > 0 if metadata else False,
            "search_queries": [
                event["arguments"].get("query", "")
                for event in metadata.get("tool_call_events", [])
                if event["tool_name"] == "web_search"
            ] if metadata else []
        },
        
        # ... rest of log entry
    }
    
    with open("logs/toolcalls.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")
```

Now you can see:
```json
{
  "request_id": "abc-123",
  "tool": "chat",
  "sub_tools": [
    {"tool_name": "web_search", "arguments": {"query": "Australian coffee wholesalers"}, "timestamp": 1760050750.2},
    {"tool_name": "web_search", "arguments": {"query": "coffee suppliers online business"}, "timestamp": 1760050752.8}
  ],
  "query_complexity": {
    "used_websearch": true,
    "search_queries": ["Australian coffee wholesalers", "coffee suppliers online business"]
  }
}
```

#### Validation Criteria
- [ ] Logs show all web searches performed by model
- [ ] Can count average searches per query type
- [ ] Can identify queries that didn't need search but used it (waste)
- [ ] Tool call IDs allow correlation with provider billing

**Estimated Effort:** 8 hours (all providers + testing)  
**Risk:** Medium (requires understanding provider-specific tool APIs)

---

## MEDIUM PRIORITY FLAGS (P2) - OBSERVABILITY & OPTIMIZATION

### ‚ÑπÔ∏è FLAG #7: MISSING CONVERSATION CONTEXT - MULTI-TURN BLINDNESS

**Severity:** MEDIUM  
**Impact:** Cannot analyze conversation patterns, user journeys, or multi-tool workflows  
**Evidence:** No conversation_id or session_id in logs

**Current state:**
- User sends 5 consecutive chat messages
- Each logged with different request_id
- **Cannot correlate they're part of same conversation**

**Business impact:**
- Cannot calculate "cost per conversation"
- Cannot identify when conversations derail
- Cannot measure tool chain effectiveness

**Fix:** Add conversation context to metadata:

```python
# When user starts conversation (Augment IDE level)
conversation_id = str(uuid.uuid4())

# Pass in all subsequent tool calls
arguments["_request_metadata"]["conversation_id"] = conversation_id
arguments["_request_metadata"]["message_index"] = 3  # 3rd message in conversation
```

**Estimated Effort:** 2 hours  
**Risk:** Low (additive, requires Augment integration)

---

### ‚ÑπÔ∏è FLAG #8: NO ERROR TAXONOMY - UNDIFFERENTIATED FAILURES

**Severity:** MEDIUM  
**Impact:** Cannot distinguish timeout vs auth vs rate-limit vs model errors  
**Evidence:** Logs show generic error strings without classification

**Fix:** Add structured error capture:

```python
# src/utils/errors.py
class ToolError(Exception):
    """Base class for all tool errors"""
    error_code = "TOOL_ERROR"
    is_retryable = False
    
class ProviderTimeoutError(ToolError):
    error_code = "PROVIDER_TIMEOUT"
    is_retryable = True
    
class ProviderAuthError(ToolError):
    error_code = "PROVIDER_AUTH"
    is_retryable = False
    
class ProviderRateLimitError(ToolError):
    error_code = "PROVIDER_RATE_LIMIT"
    is_retryable = True  # After delay

# In logging:
log_entry["error"] = {
    "code": e.error_code,
    "message": str(e),
    "is_retryable": e.is_retryable,
    "category": e.__class__.__name__
}
```

**Estimated Effort:** 4 hours  
**Risk:** Low (doesn't change behavior, just classification)

---

### ‚ÑπÔ∏è FLAG #9: NO USER/TENANT TRACKING - MULTI-USER BLINDNESS

**Severity:** MEDIUM (CRITICAL for SaaS deployment)  
**Impact:** Cannot bill per-user, identify abuse, or provide user-specific analytics  
**Evidence:** No user_id anywhere in system

**Fix:** Add user context at entry point:

```python
# scripts/run_ws_shim.py
# Get user from Augment environment
user_id = os.environ.get("AUGMENT_USER_ID", "local_user")

# Inject into all tool calls
arguments["_request_metadata"]["user_id"] = user_id
arguments["_request_metadata"]["tenant_id"] = "augment"  # For multi-tenant
```

**Estimated Effort:** 2 hours  
**Risk:** Medium (requires Augment integration, privacy considerations)

---

### ‚ÑπÔ∏è FLAG #10: RESPONSE TRUNCATION WITHOUT REFERENCE

**Severity:** MEDIUM  
**Impact:** Debugging incomplete - cannot see full responses  
**Evidence:** `"result_truncated": true` but no way to retrieve full data

**Fix:** Store full responses separately:

```python
# src/utils/logging.py
def log_tool_call(..., result, ...):
    result_str = json.dumps(result)
    
    if len(result_str) > 2000:
        # Store full response in separate file
        full_result_hash = hashlib.sha256(result_str.encode()).hexdigest()[:16]
        full_result_path = f"logs/full_results/{full_result_hash}.json"
        
        with open(full_result_path, "w") as f:
            json.dump(result, f, indent=2)
        
        log_entry["result_truncated"] = True
        log_entry["full_result_hash"] = full_result_hash
        log_entry["full_result_path"] = full_result_path
    else:
        log_entry["result"] = result
```

**Estimated Effort:** 2 hours  
**Risk:** Low (storage growth, need retention policy)

---

## IMPLEMENTATION ROADMAP

### Phase 1: Critical Fixes (Week 1) - 17 hours
**Goal:** Make observability functional

| Flag | Task | Effort | Owner |
|------|------|--------|-------|
| #1 | Add request_id generation | 2h | Backend |
| #2 | Fix duplicate logging | 3h | Backend |
| #3 | Flatten JSON structure | 4h | Backend |
| #4 | Capture token telemetry | 6h | Backend |
| #5 | Fix duration timing | 4h | Backend |

**Deliverable:** Can correlate requests, parse logs programmatically, measure actual costs

---

### Phase 2: Transparency (Week 2) - 14 hours
**Goal:** Understand what AI agents are doing

| Flag | Task | Effort | Owner |
|------|------|--------|-------|
| #6 | Capture sub-tool calls | 8h | Backend |
| #7 | Add conversation context | 2h | Backend + Frontend |
| #8 | Error taxonomy | 4h | Backend |

**Deliverable:** Full audit trail of agent behavior

---

### Phase 3: Scale & Multi-tenant (Week 3) - 8 hours
**Goal:** Production readiness

| Flag | Task | Effort | Owner |
|------|------|--------|-------|
| #9 | User/tenant tracking | 2h | Backend + Frontend |
| #10 | Full result storage | 2h | Backend |
| - | Build observability dashboard | 4h | Frontend |

**Deliverable:** Can deploy to multi-user environment with billing

---

## VALIDATION CHECKLIST

After implementing all fixes, verify:

### Functional Tests
- [ ] Start conversation, send 3 messages, verify all have same conversation_id
- [ ] Trigger error, verify error_code and is_retryable are logged
- [ ] Use web search, verify all search queries appear in sub_tools array
- [ ] Run 100 concurrent requests, verify no duplicate log entries

### Data Quality Tests
```python
import json
import pandas as pd

# Load logs
logs = []
with open("logs/toolcalls.jsonl") as f:
    for line in f:
        logs.append(json.loads(line))

df = pd.DataFrame(logs)

# Validation assertions
assert df["request_id"].isna().sum() == 0, "request_id must never be null"
assert (df["duration_s"] >= 0.05).all(), "No duration can be < 50ms"
assert df.duplicated(subset=["request_id", "tool"]).sum() == 0, "No duplicate entries"
assert (df["tokens.total"] > 0).sum() > 0, "Token data must be present"
```

### Cost Validation
```sql
-- Query logs to validate billing
SELECT 
    DATE(timestamp_utc) as date,
    user_id,
    SUM(cost_usd) as total_cost,
    SUM(tokens.total) as total_tokens,
    COUNT(*) as request_count
FROM toolcalls_log
GROUP BY date, user_id
ORDER BY date DESC, total_cost DESC;
```

Compare with provider billing statements - should match within 1%.

---

## FINAL RECOMMENDATIONS

### Immediate Actions (Today)
1. **Stop trusting current logs for any decision-making** - They're 80% incorrect
2. **Implement FLAG #1 (request_id)** - This unblocks everything else
3. **Set up test harness** - Automated validation prevents regression

### Strategic Changes (This Month)
1. **Adopt OpenTelemetry** - Industry standard observability
2. **Add Prometheus metrics endpoint** - Real-time dashboards
3. **Implement distributed tracing** - End-to-end request visualization

### Architectural Debt
Your logging was bolted on **after** building the system. This is backwards.

**Correct approach:**
```
1. Define what you need to observe
2. Design observability INTO the architecture
3. Build features with instrumentation from day 1
```

You built a race car then tried to add a dashboard by duct-taping gauges to the engine. The fixes above are **retrofitting observability** - it works, but it's not elegant.

**For your next system:** Consider observability a first-class feature, not an afterthought.

---

## BOTTOM LINE

Your agent works. Your observability doesn't.

**You cannot manage what you cannot measure.** Right now, you're flying blind at 500mph. The fixes above give you instruments.

**Start with FLAG #1 (request_id)** - it's 2 hours of work that unblocks 80% of other improvements. Everything else builds on correlation capability.

Want me to write the FLAG #1 + FLAG #2 fixes as working code now?