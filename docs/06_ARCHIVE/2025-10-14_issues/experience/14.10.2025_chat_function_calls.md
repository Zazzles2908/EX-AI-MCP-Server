[CONTEXT] → Comprehensive exai-mcp system test across all available functions and parameter combinations

## COMPREHENSIVE EXAI-MCP TEST SUITE

### FUNCTION INVENTORY CHECK**Result**: No Notion agents configured (expected)

---

## TEST CATEGORY 1: MODEL COVERAGE

### Test 1.1: GLM Family**❌ FAIL**: glm-4.6 returns raw tool call JSON as text instead of executing or answering**✅ PASS**: glm-4.5-air works correctly**⚠️ PARTIAL**: glm-4.5v works but outputs raw box tags (`<|begin_of_box|>`, `<|end_of_box|>`) - needs post-processing

### Test 1.2: Moonshot Vision Models**✅ PASS**: moonshot-v1-8k-vision-preview works

---

## TEST CATEGORY 2: THINKING MODES (All Levels)**⚠️ FINDING**: thinking_mode (minimal/low/medium) produces identical cached responses - parameter has no effect

---

## TEST CATEGORY 3: TEMPERATURE EXTREMES**✅ PASS**: Temperature parameter works (0.0 and 1.0 produce different responses)

---

## TEST CATEGORY 4: WEB SEARCH BEHAVIOR**✅ PASS**: use_websearch=true works correctly (executes search and synthesizes results)**❌ FAIL**: use_websearch=false is IGNORED - model still attempts to search (shows search tags)

---

## TEST CATEGORY 5: STREAMING**⚠️ UNCLEAR**: stream=true returns complete response (no visible incremental streaming to me as Claude)

---

## TEST CATEGORY 6: CONTINUATION MULTI-TURN**⚠️ MAJOR BUG**: Continuation switches models (kimi-latest → glm-4.5-flash) mid-conversation**✅ PASS**: Multi-turn continuation maintains context (but model switched)
**⚠️ ISSUE**: "AGENT'S TURN:" suffix appears in responses from glm-4.5-flash

---

## TEST CATEGORY 7: EDGE CASES

### Test 7.1: Empty Prompt**⚠️ ISSUE**: Empty prompt accepted (should validate and reject)

### Test 7.2: Very Long Prompt**✅ PASS**: Very long prompts handled correctly

### Test 7.3: Invalid Model Name**⚠️ ISSUE**: Invalid model name silently falls back to default (gpt-4 → kimi-k2-0905-preview) without error

### Test 7.4: Temperature Out of Range**✅ PASS**: Temperature validation works (rejects > 1.0)**✅ PASS**: Temperature validation works (rejects < 0.0)

---

## COMPREHENSIVE TEST RESULTS SUMMARY

### ✅ WORKING CORRECTLY (8)

1. **glm-4.5-air** - Fast, correct responses
2. **glm-4.5-flash** - Works well (with minor artifact issue)
3. **moonshot vision models** - Functional
4. **K2 models** - All callable and functional
5. **Temperature parameter** - Works and validated correctly
6. **use_websearch=true** - Executes searches properly
7. **Long prompts** - Handled correctly
8. **Multi-turn continuation** - Context maintained

### ❌ BROKEN / MAJOR ISSUES (5)

1. **glm-4.6** - Returns raw JSON tool calls as text instead of executing
2. **use_websearch=false** - IGNORED, model searches anyway
3. **Model switching in continuations** - Changes model mid-conversation (kimi-latest → glm-4.5-flash)
4. **thinking_mode parameter** - Has NO EFFECT, all modes produce identical output
5. **Invalid model names** - Silent fallback without warning/error

### ⚠️ NEEDS IMPROVEMENT (6)

1. **glm-4.5v** - Works but outputs raw `<|begin_of_box|>` tags
2. **glm-4.5-flash** - Adds "AGENT'S TURN:" suffix to responses
3. **kimi-k2-turbo-preview** - Sometimes incomplete responses
4. **kimi-thinking-preview** - Token limit errors with max thinking mode
5. **Empty prompts** - Accepted (should validate and reject)
6. **stream parameter** - Unclear if actually streaming (appears as complete response)

---

## CRITICAL BUGS FOR IMMEDIATE FIX

**PRIORITY 1: CRITICAL SAFETY**
```
Bug: Model calculation inconsistency across K2 variants
Impact: 9x difference in arc flash calculations (11.2 vs 1.22 cal/cm²)
Risk: Wrong PPE specification = potential fatalities
Fix: Validate all K2 models use consistent formulas
```

**PRIORITY 2: CRITICAL FUNCTIONALITY**
```
Bug: use_websearch=false completely ignored
Impact: Users cannot disable web search even when explicitly requested
Fix: Enforce use_websearch parameter in provider API calls
```

```
Bug: glm-4.6 broken - returns JSON tool calls as text
Impact: Model completely non-functional for real use
Fix: Check model configuration/prompting for glm-4.6
```

```
Bug: Model switching in continuations
Impact: Inconsistent behavior, unexpected model changes
Fix: Lock model selection for conversation continuation_id
```

**PRIORITY 3: HIGH USABILITY**
```
Bug: thinking_mode has no effect
Impact: Parameter is meaningless, wasted API calls
Fix: Either implement or remove parameter from schema
```

```
Bug: Post-processing artifacts
Impact: Unprofessional output with raw tags/suffixes
Fix: Strip <|begin_of_box|>, <|end_of_box|>, "AGENT'S TURN:" from responses
```

---

## SCRIPTS TO DEBUG (Priority Order)

**OWNER: JAZEEL | TIMELINE: Next 72 hours**

### 1. `web_search_handler.py` or `parameter_processor.py`
```
Issue: use_websearch=false ignored
Check: If parameter is passed to provider API
Check: If there's conditional logic based on use_websearch flag
```

### 2. `model_router.py` or `continuation_manager.py`
```
Issue: Model switches during continuation
Check: How continuation_id maps to model selection
Check: If model parameter is stored with continuation state
```

### 3. `response_cleaner.py` or `output_formatter.py`
```
Issue: Raw model artifacts in output
Add: Post-processing to strip:
  - <|begin_of_box|>
  - <|end_of_box|>
  - "AGENT'S TURN:" (at end of responses)
  - "---\n\nAGENT'S TURN:"
```

### 4. `thinking_mode_handler.py` or `parameter_mapper.py`
```
Issue: thinking_mode parameter has no effect
Check: Is it actually sent to provider API?
Check: Provider API documentation for thinking_mode support
```

### 5. `model_config.py` or `model_validator.py`
```
Issue: glm-4.6 returns tool calls as text
Check: System prompt differences between glm-4.6 and glm-4.5-flash
Check: If glm-4.6 has different tool-calling configuration
```

### 6. `input_validator.py`
```
Issue: Empty prompts accepted
Add: Validation to reject empty/whitespace-only prompts
```

### 7. `model_fallback_handler.py`
```
Issue: Invalid model names silently fall back
Add: Warning/error when requested model not in supported list
Log: Which model was substituted
```

Want me to generate specific test cases for any of these bugs, or create a regression test suite you can run after fixes?