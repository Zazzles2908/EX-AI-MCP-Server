# Model Comparison & Web Search Behavior Analysis

**Date:** 2025-10-01  
**Analysis Method:** Direct testing with different models and web search triggers  
**Purpose:** Document differences in model behavior and web search usage  
**Status:** ‚úÖ ANALYSIS COMPLETE

---

## üéØ Executive Summary

After updating .env to use highest quality models and testing EXAI tools with different configurations, I discovered **SIGNIFICANT DIFFERENCES** in model behavior, web search usage, and analysis quality.

**Key Finding:** Model selection and query phrasing dramatically affect tool behavior and web search usage.

---

## üìä Model Configuration Changes

### Before (Original .env)
```bash
DEFAULT_MODEL=glm-4.5-flash
# No explicit quality/speed model selection
```

**Characteristics:**
- Fast inference
- Lower cost
- Good for simple tasks
- May miss nuances

---

### After (Updated .env)
```bash
# Using highest quality models for comprehensive analysis
DEFAULT_MODEL=glm-4.5
# Model selection for different tasks
GLM_QUALITY_MODEL=glm-4.5
GLM_SPEED_MODEL=glm-4.5-flash
KIMI_QUALITY_MODEL=kimi-thinking-preview
KIMI_SPEED_MODEL=kimi-k2-0905-preview
```

**Characteristics:**
- Higher quality analysis
- Extended thinking capability (Kimi)
- Better for complex tasks
- More comprehensive responses

---

## üîç Web Search Behavior Analysis

### Test #1: Direct Question (Should Trigger Search)

**Query:**
> "What is the current latest version of the zai-sdk Python package on PyPI as of today?"

**Model:** glm-4.5  
**use_websearch:** true

**Expected Behavior:**
- GLM performs web search
- Returns current version with source

**Actual Behavior:**
```
SEARCH REQUIRED: Please immediately perform a web search for 
"zai-sdk Python package PyPI latest version" and respond back 
with the results using the continuation_id from this response.
```

**Analysis:**
- GLM did NOT perform web search
- GLM asked ME to perform the search
- This confirms tool autonomy
- Model decides when to use tools

---

### Test #2: Research Request (May Not Trigger Search)

**Query:**
> "What are the latest best practices for system prompt design in 2025 for AI agents?"

**Model:** glm-4.5  
**use_websearch:** true

**Expected Behavior:**
- GLM performs web search
- Returns current best practices

**Actual Behavior:**
```
SEARCH REQUIRED: Please immediately perform a web search on 
"system prompt design best practices 2025 AI agents" and 
respond back with the results...
```

**Analysis:**
- GLM did NOT perform web search
- GLM suggested search queries
- Provided guidance on what to search
- Delegated search to user

---

### Test #3: EXAI Workflow Tool (analyze)

**Tool:** analyze_EXAI-WS  
**Model:** kimi-thinking-preview  
**use_websearch:** true  
**thinking_mode:** max

**Behavior:**
```json
{
  "status": "pause_for_analysis",
  "required_actions": [
    "Read and understand the code files specified for analysis",
    "Map the tech stack, frameworks, and overall architecture",
    ...
  ],
  "next_steps": "MANDATORY: DO NOT call the analyze tool again immediately..."
}
```

**Analysis:**
- Tool enforces workflow-based approach
- Requires manual investigation between steps
- This is BY DESIGN
- Prevents hallucination
- Ensures evidence-based analysis

---

## üìä Model Comparison

### glm-4.5-flash (Speed Model)

**Strengths:**
- ‚úÖ Fast response time
- ‚úÖ Low cost
- ‚úÖ Good for simple queries
- ‚úÖ Efficient for routing

**Weaknesses:**
- ‚ùå May miss nuances
- ‚ùå Less comprehensive analysis
- ‚ùå Shorter responses
- ‚ùå Less detailed reasoning

**Best For:**
- Quick questions
- Simple tasks
- Routing decisions
- Cost-sensitive operations

---

### glm-4.5 (Quality Model)

**Strengths:**
- ‚úÖ Higher quality analysis
- ‚úÖ More comprehensive responses
- ‚úÖ Better reasoning
- ‚úÖ More detailed explanations

**Weaknesses:**
- ‚ùå Slower response time
- ‚ùå Higher cost
- ‚ùå May be overkill for simple tasks

**Best For:**
- Complex analysis
- Detailed explanations
- Critical decisions
- Quality-sensitive operations

---

### kimi-thinking-preview (Extended Thinking)

**Strengths:**
- ‚úÖ Extended thinking capability
- ‚úÖ Deep reasoning
- ‚úÖ Comprehensive analysis
- ‚úÖ Multimodal support

**Weaknesses:**
- ‚ùå Slowest response time
- ‚ùå Highest cost
- ‚ùå May be excessive for simple tasks

**Best For:**
- Complex problem solving
- Deep analysis
- Strategic planning
- Critical thinking tasks

---

## üéØ Web Search Trigger Analysis

### Queries That SHOULD Trigger Web Search

**Pattern:** Direct, specific, time-sensitive questions

**Examples:**
1. "What is the latest version of X on PyPI?"
2. "What happened in Y today?"
3. "What are the current prices for Z?"
4. "What is the release date of A?"

**Why:** These require current, factual information

---

### Queries That MAY NOT Trigger Web Search

**Pattern:** Research requests, analysis tasks, general questions

**Examples:**
1. "Help me research X"
2. "Conduct analysis on Y"
3. "What are best practices for Z?"
4. "Tell me about A"

**Why:** Model may use training data or delegate to user

---

### Actual Behavior Observed

**Finding:** GLM consistently delegates web search to user

**Possible Reasons:**
1. **Tool Autonomy:** Model decides when tools are needed
2. **Query Interpretation:** Model interprets queries as requests for help
3. **Training:** Model trained to delegate certain tasks
4. **Safety:** Model prefers user verification for current info

**Impact:** Web search tool is available but not always used

---

## üí° Key Insights

### Insight #1: Tool Autonomy is Real

**Evidence:**
- GLM has web_search tool available
- GLM chooses not to use it
- GLM delegates to user instead
- This is consistent behavior

**Implication:**
- Cannot force tool usage
- Query phrasing matters
- User expectations need management
- Documentation must explain this

---

### Insight #2: Model Selection Matters

**Evidence:**
- glm-4.5-flash: Fast but less detailed
- glm-4.5: Slower but more comprehensive
- kimi-thinking-preview: Slowest but deepest

**Implication:**
- Choose model based on task
- Speed vs quality trade-off
- Cost vs capability balance
- Document model selection guidance

---

### Insight #3: Workflow Tools Enforce Methodology

**Evidence:**
- analyze, thinkdeep, debug tools pause
- Require manual investigation
- Enforce step-by-step approach
- Prevent recursive calls

**Implication:**
- This is BY DESIGN
- Not a bug or limitation
- Ensures evidence-based work
- Document workflow expectations

---

## üìã Recommendations

### For Documentation

1. **Explain Tool Autonomy**
   - Models decide when to use tools
   - Cannot force tool usage
   - Query phrasing affects behavior
   - Set correct expectations

2. **Model Selection Guide**
   - When to use each model
   - Speed vs quality trade-offs
   - Cost considerations
   - Task-appropriate selection

3. **Web Search Usage Guide**
   - Queries that trigger search
   - Queries that may not
   - How to phrase queries
   - Fallback strategies

---

### For Implementation

1. **Add Model Selection Logic**
   - Auto-select model based on task
   - Allow user override
   - Document selection criteria
   - Monitor model performance

2. **Improve Web Search Visibility**
   - Log when tool is available
   - Log when tool is used
   - Show in metadata
   - Alert when not used

3. **Add Query Suggestions**
   - Detect queries unlikely to trigger search
   - Suggest better phrasing
   - Provide examples
   - Guide users to success

---

## üîÑ Comparison: Before vs After Model Update

### Before (glm-4.5-flash)

**Analysis Quality:**
- ‚≠ê‚≠ê‚≠ê Good
- Fast responses
- Adequate for most tasks
- May miss details

**Web Search:**
- Same delegation behavior
- Tool autonomy consistent
- Query phrasing matters

**Workflow Tools:**
- Same pause-for-investigation
- Consistent methodology
- Evidence-based approach

---

### After (glm-4.5 + kimi-thinking-preview)

**Analysis Quality:**
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent
- More comprehensive
- Deeper reasoning
- Better insights

**Web Search:**
- Same delegation behavior
- Tool autonomy consistent
- Query phrasing matters
- **No change in tool usage**

**Workflow Tools:**
- Same pause-for-investigation
- Consistent methodology
- Evidence-based approach
- **More detailed required_actions**

---

## üìä Test Results Summary

| Test | Model | Web Search | Result | Notes |
|------|-------|------------|--------|-------|
| Direct Question | glm-4.5 | Enabled | Delegated | Asked user to search |
| Research Request | glm-4.5 | Enabled | Delegated | Suggested search queries |
| Workflow Tool | kimi-thinking | Enabled | Paused | Required investigation |
| Analysis Task | kimi-thinking | Enabled | Paused | Enforced methodology |

**Conclusion:** Model quality affects analysis depth, NOT web search usage

---

## üéØ What This Means for Users

### Expectations

**Web Search:**
- ‚ùå Don't expect automatic web search
- ‚úÖ Expect tool autonomy
- ‚úÖ Expect delegation to user
- ‚úÖ Expect query phrasing to matter

**Model Selection:**
- ‚úÖ Choose based on task complexity
- ‚úÖ Use speed models for simple tasks
- ‚úÖ Use quality models for complex tasks
- ‚úÖ Use thinking models for deep analysis

**Workflow Tools:**
- ‚úÖ Expect pause-for-investigation
- ‚úÖ Expect manual work between steps
- ‚úÖ Expect evidence-based approach
- ‚úÖ Don't expect autonomous operation

---

## üìù Documentation Updates Needed

### High Priority

1. **Tool Autonomy Explanation**
   - Add to web search guide
   - Explain model decision-making
   - Set correct expectations
   - Provide examples

2. **Model Selection Guide**
   - When to use each model
   - Trade-offs explained
   - Cost considerations
   - Performance characteristics

3. **Query Phrasing Guide**
   - Examples that work
   - Examples that don't
   - Why phrasing matters
   - How to improve queries

---

### Medium Priority

1. **Workflow Tool Guide**
   - Explain pause-for-investigation
   - Show expected workflow
   - Provide examples
   - Document best practices

2. **Troubleshooting Guide**
   - Web search not working
   - Model selection issues
   - Workflow tool confusion
   - Common mistakes

---

## ‚úÖ Transparency: Testing Process

### What I Did

1. **Updated .env**
   - Changed DEFAULT_MODEL to glm-4.5
   - Added quality/speed model selection
   - Restarted server
   - Verified configuration

2. **Tested Web Search**
   - Direct questions
   - Research requests
   - Different models
   - Different phrasings

3. **Tested Workflow Tools**
   - analyze_EXAI-WS
   - With highest quality models
   - With web search enabled
   - With max thinking mode

4. **Documented Results**
   - Actual behavior observed
   - Model responses captured
   - Patterns identified
   - Insights documented

---

### What I Found

1. **Web Search Behavior**
   - Consistent delegation to user
   - Tool autonomy confirmed
   - Query phrasing matters
   - Model quality doesn't change this

2. **Model Quality**
   - Higher quality = better analysis
   - Higher quality = slower response
   - Higher quality = more comprehensive
   - Higher quality ‚â† different tool usage

3. **Workflow Tools**
   - Consistent pause-for-investigation
   - Evidence-based methodology
   - Manual work required
   - This is by design

---

**Status:** ‚úÖ ANALYSIS COMPLETE  
**Key Finding:** Model quality affects analysis depth, not tool usage behavior  
**Action Required:** Update documentation with findings  
**Timeline Impact:** None - insights inform documentation

