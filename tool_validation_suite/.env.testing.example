# Tool Validation Suite - Environment Configuration
# Copy this file to .env.testing and fill in your API keys

# ============================================================================
# KIMI API CONFIGURATION (Moonshot)
# ============================================================================
# Used for testing Kimi-specific tools and general tool testing
KIMI_API_KEY=sk-your_kimi_api_key_here
KIMI_BASE_URL=https://api.moonshot.ai/v1

# ============================================================================
# GLM API CONFIGURATION (ZhipuAI)
# ============================================================================
# Used for testing GLM-specific tools and general tool testing
GLM_API_KEY=your_glm_api_key_here.your_glm_secret_here
GLM_BASE_URL=https://api.z.ai/api/paas/v4

# ============================================================================
# GLM WATCHER CONFIGURATION (Independent Observer)
# ============================================================================
# Separate GLM API key for independent test observation
# This should be a DIFFERENT account/key from GLM_API_KEY
GLM_WATCHER_KEY=your_watcher_glm_key_here.your_watcher_secret_here
GLM_WATCHER_MODEL=glm-4.5-flash
GLM_WATCHER_ENABLED=true

# ============================================================================
# TEST CONFIGURATION
# ============================================================================
# Directory for test files
TEST_FILES_DIR=./tool_validation_suite/fixtures/sample_files

# Directory for test results
TEST_RESULTS_DIR=./tool_validation_suite/results

# Maximum timeout for any single test (seconds)
TEST_TIMEOUT_SECS=300

# Maximum retries for failed tests
TEST_MAX_RETRIES=3

# Delay between tests (seconds) to avoid rate limiting
TEST_DELAY_SECS=1

# ============================================================================
# COST TRACKING
# ============================================================================
# Track API costs during testing
TRACK_API_COSTS=true

# Maximum cost per individual test (USD)
MAX_COST_PER_TEST=0.50

# Maximum total cost for entire test run (USD)
MAX_TOTAL_COST=10.00

# Alert if cost exceeds threshold
COST_ALERT_THRESHOLD=5.00

# ============================================================================
# CONVERSATION ID CACHING
# ============================================================================
# Cache conversation IDs for multi-turn testing
CACHE_CONVERSATION_IDS=true

# Conversation cache TTL (seconds)
CONVERSATION_CACHE_TTL=3600

# Separate caches for Kimi and GLM
KIMI_CONVERSATION_CACHE_DIR=./tool_validation_suite/cache/kimi
GLM_CONVERSATION_CACHE_DIR=./tool_validation_suite/cache/glm

# ============================================================================
# FILE UPLOAD CONFIGURATION
# ============================================================================
# Maximum file size for upload tests (bytes)
MAX_FILE_SIZE_BYTES=10485760  # 10 MB

# Supported file types for testing
SUPPORTED_FILE_TYPES=.txt,.py,.json,.md,.csv,.log

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
# Log level for test execution
TEST_LOG_LEVEL=INFO

# Save detailed logs
SAVE_DETAILED_LOGS=true

# Log directory
TEST_LOG_DIR=./tool_validation_suite/results/latest/test_logs

# ============================================================================
# PERFORMANCE MONITORING
# ============================================================================
# Monitor memory usage during tests
MONITOR_MEMORY=true

# Monitor CPU usage during tests
MONITOR_CPU=true

# Performance alert thresholds
MEMORY_ALERT_MB=500
CPU_ALERT_PERCENT=80

# ============================================================================
# RESULT STORAGE
# ============================================================================
# Keep historical test results
KEEP_HISTORY=true

# Maximum number of historical runs to keep
MAX_HISTORY_RUNS=10

# Compress old results
COMPRESS_OLD_RESULTS=true

# ============================================================================
# GLM WATCHER SETTINGS
# ============================================================================
# Watcher observation detail level (low, medium, high)
WATCHER_DETAIL_LEVEL=high

# Save watcher observations
SAVE_WATCHER_OBSERVATIONS=true

# Watcher observation directory
WATCHER_OBSERVATION_DIR=./tool_validation_suite/results/latest/watcher_observations

# Watcher timeout (seconds)
WATCHER_TIMEOUT_SECS=30

# ============================================================================
# WEB SEARCH TESTING
# ============================================================================
# Enable web search testing
TEST_WEB_SEARCH=true

# Web search test queries
WEB_SEARCH_TEST_QUERY=latest AI news

# ============================================================================
# INTEGRATION TESTING
# ============================================================================
# Test conversation ID persistence
TEST_CONVERSATION_PERSISTENCE=true

# Test conversation ID isolation between platforms
TEST_CONVERSATION_ISOLATION=true

# Test file upload integration
TEST_FILE_UPLOAD=true

# ============================================================================
# REPORTING
# ============================================================================
# Generate markdown reports
GENERATE_MARKDOWN_REPORTS=true

# Generate JSON reports
GENERATE_JSON_REPORTS=true

# Generate HTML reports (future)
GENERATE_HTML_REPORTS=false

# Report output directory
REPORT_OUTPUT_DIR=./tool_validation_suite/results/reports

# ============================================================================
# DEBUGGING
# ============================================================================
# Save API request/response for debugging
SAVE_API_REQUESTS=true
SAVE_API_RESPONSES=true

# API request/response directory
API_DEBUG_DIR=./tool_validation_suite/results/latest/api_responses

# Verbose output during testing
VERBOSE_OUTPUT=true

# ============================================================================
# SAFETY LIMITS
# ============================================================================
# Stop testing if failure rate exceeds threshold
STOP_ON_FAILURE_RATE=0.5  # Stop if 50% of tests fail

# Stop testing if cost exceeds limit
STOP_ON_COST_LIMIT=true

# Require confirmation before running all tests
REQUIRE_CONFIRMATION=true

