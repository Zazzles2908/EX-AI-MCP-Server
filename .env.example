# EX-AI MCP Server - Environment Template (clean, minimal)
# Copy this file to .env and fill in secrets/values. Keep .env out of VCS.

# -------- Core --------
# Default model to use for Chat and general tools
DEFAULT_MODEL=glm-4.5-flash

# Keep simple single-provider routing on; advanced agents are de-scoped
ROUTER_ENABLED=true
GLM_ENABLE_WEB_BROWSING=true

# -------- Expert Analysis Configuration --------
# DEFAULT_USE_ASSISTANT_MODEL: Controls whether workflow tools use expert analysis by default
# When true, tools like thinkdeep, debug, analyze will call expert models for validation
# When false, tools rely only on their own analysis (faster but less comprehensive)
# Default: true (recommended for comprehensive analysis)
# Note: Can be overridden per-tool using tool-specific environment variables below
DEFAULT_USE_ASSISTANT_MODEL=true

# Tool-specific overrides for expert analysis (optional)
# These override DEFAULT_USE_ASSISTANT_MODEL for specific tools
# Useful when you want expert analysis for some tools but not others
# Example: Enable for codereview but disable for thinkdeep
# THINKDEEP_USE_ASSISTANT_MODEL_DEFAULT=false
# DEBUG_USE_ASSISTANT_MODEL_DEFAULT=false
# ANALYZE_USE_ASSISTANT_MODEL_DEFAULT=false
# CODEREVIEW_USE_ASSISTANT_MODEL_DEFAULT=false
# TESTGEN_USE_ASSISTANT_MODEL_DEFAULT=false
# REFACTOR_USE_ASSISTANT_MODEL_DEFAULT=false
# SECAUDIT_USE_ASSISTANT_MODEL_DEFAULT=false
# PRECOMMIT_USE_ASSISTANT_MODEL_DEFAULT=false
# TRACER_USE_ASSISTANT_MODEL_DEFAULT=false
# DOCGEN_USE_ASSISTANT_MODEL_DEFAULT=false
# PLANNER_USE_ASSISTANT_MODEL_DEFAULT=false
# CONSENSUS_USE_ASSISTANT_MODEL_DEFAULT=false

# EXPERT_ANALYSIS_TIMEOUT_SECS: Maximum time for expert analysis provider calls
# CRITICAL: Must be LESS than EXAI_WS_CALL_TIMEOUT to prevent daemon timeouts
# Default: 300 seconds (5 minutes) - but should be set to 90s for production
# Recommended: 90 seconds (provides safe buffer under 180s WebSocket timeout)
EXPERT_ANALYSIS_TIMEOUT_SECS=90

# EXPERT_HEARTBEAT_INTERVAL_SECS: How often to emit progress during expert analysis
# Default: 10 seconds - but should be set to 5s to keep WebSocket connection alive
# Recommended: 5 seconds (prevents idle timeout disconnects)
EXPERT_HEARTBEAT_INTERVAL_SECS=5

# WebSocket daemon configuration
EXAI_WS_HOST=127.0.0.1
EXAI_WS_PORT=8079
# LOG_LEVEL can be DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# File path handling
# EX_ALLOW_RELATIVE_PATHS=true
#   Purpose: Allow relative file paths in tool requests (auto-resolved to absolute paths)
#   Default: true (enabled for better UX)
#   Security: Relative paths are resolved against project root and validated to prevent escaping
#   Example: "src/file.py" → "c:\Project\EX-AI-MCP-Server\src\file.py"
#   Note: Set to false to require absolute paths only (stricter but less convenient)

# WebSocket daemon advanced settings
# EXAI_WS_DISABLE_COALESCE_FOR_TOOLS=
#   Purpose: Comma-separated list of tool names to disable request coalescing for
#   Default: Empty (coalescing enabled for all tools)
#   Example: EXAI_WS_DISABLE_COALESCE_FOR_TOOLS=chat,analyze
#   Note: Coalescing deduplicates identical concurrent requests to save resources.
#         Disable for tools where each call should execute independently.
#
# KIMI_CHAT_TOOL_TIMEOUT_SECS=180
#   Purpose: Timeout in seconds for kimi_chat_with_tools without web search
#   Default: 180 (3 minutes)
#
# KIMI_CHAT_TOOL_TIMEOUT_WEB_SECS=300
#   Purpose: Timeout in seconds for kimi_chat_with_tools with web search enabled
#   Default: 300 (5 minutes)
#   Note: Web search requires more time for search execution and result processing

# -------- Providers (GLM + Kimi) --------
# GLM (ZhipuAI/Z.ai) access
# Prefer GLM_API_URL; aliases ZHIPUAI_API_URL and ZHIPUAI_BASE_URL are also read by the server
GLM_API_KEY=
GLM_API_URL=https://api.z.ai/api/paas/v4
ZHIPUAI_API_KEY=
ZHIPUAI_API_URL=https://open.bigmodel.cn/api/paas/v4
ZHIPUAI_BASE_URL=https://open.bigmodel.cn/api/paas/v4

# Kimi (Moonshot) access
# Prefer KIMI_API_URL; alias KIMI_BASE_URL is also read by the server
KIMI_API_KEY=
KIMI_API_URL=https://api.moonshot.ai/v1
KIMI_BASE_URL=https://api.moonshot.ai/v1


# -------- Web Search Configuration --------
# -------- Web Search Configuration --------
# Both GLM and Kimi support SERVER-SIDE web search
# GLM: Uses native web_search tool (search executed by GLM API)
# Kimi: Uses builtin_function $web_search (search executed by Kimi API)
# No client-side search execution needed!
GLM_ENABLE_WEB_BROWSING=true
KIMI_ENABLE_INTERNET_SEARCH=true

# -------- Kimi (Moonshot) Provider Settings --------
# Enable agentic logging
AGENTIC_ENABLE_LOGGING=true
# Cap header size to avoid 400 Request Header Too Large (NGINX/gateway)
KIMI_MAX_HEADER_LEN=4096
# Client-side file upload guardrail (MB)
KIMI_FILES_MAX_SIZE_MB=20
# Preferred Kimi models (used by boundary auto-router in request handler)
KIMI_DEFAULT_MODEL=kimi-k2-0711-preview
KIMI_THINKING_MODEL=kimi-thinking-preview
KIMI_SPEED_MODEL=kimi-k2-turbo-preview
# Context-cache controls (attach/reuse tokens via headers)
KIMI_CACHE_TOKEN_TTL_SECS=1800
KIMI_CACHE_TOKEN_LRU_MAX=256
# Optional provider timeouts (set only if you need overrides)
KIMI_CONNECT_TIMEOUT_SECS=10
KIMI_READ_TIMEOUT_SECS=300
KIMI_WRITE_TIMEOUT_SECS=60
KIMI_POOL_TIMEOUT_SECS=10
KIMI_DEFAULT_READ_TIMEOUT_SECS=300


# -------- Test Files (Local) --------
# Absolute path(s) to folder(s) containing sample files for upload/extract tests
# Supports multiple roots separated by comma or semicolon (e.g., C:\A;C:\B or C:\A,C:\B)
# Example (Windows): C:\Project\EX-AI-MCP-Server\test_files
# Example (Unix): /home/you/Project/EX-AI-MCP-Server/test_files
TEST_FILES_DIR=

# -------- Streaming --------
# Provider-level streaming for GLM Chat (SSE/SDK aggregation)
# true: Chat tool requests streaming from GLM provider (env-gated)
# false: standard non-streaming responses
GLM_STREAM_ENABLED=false
# Provider-level streaming for Kimi Chat (text/event-stream)
KIMI_STREAM_ENABLED=false

# Kimi streaming timeout in seconds (applies to stream mode)
KIMI_STREAM_TIMEOUT_SECS=240
# Prime Kimi context-cache before streaming (best-effort; true/false)
KIMI_STREAM_PRIME_CACHE=false

# -------- Optional timeouts (sane defaults if unset) --------
# HTTP_CONNECT_TIMEOUT=10
# HTTP_READ_TIMEOUT=60
# HTTP_TOTAL_TIMEOUT=90

# -------- Client-Specific Configuration (MCP Client Behavior) --------
# Generic client configuration (works with all MCP clients: Augment, Claude Code, Cline, etc.)
# These control default behavior when MCP clients call tools
#
# IMPORTANT FOR DEVELOPMENT: Leave these variables unset/empty to enable FULL system capabilities.
# Setting CLIENT_TOOL_ALLOWLIST or CLIENT_TOOL_DENYLIST RESTRICTS tool visibility.
# Setting CLIENT_MAX_WORKFLOW_STEPS LIMITS workflow tool functionality.

# Tool filtering (comma-separated list of tool names)
# CLIENT_TOOL_ALLOWLIST=chat,analyze,debug
#   Purpose: Restrict which tools are visible to MCP clients (empty = all allowed)
#   Default: Empty (all tools allowed based on TOOL_VISIBILITY in tools/registry.py)
#   WARNING: Setting this RESTRICTS functionality - leave empty during development
#
# CLIENT_TOOL_DENYLIST=
#   Purpose: Block specific tools from being visible to MCP clients (empty = none denied)
#   Default: Empty (no tools denied)
#   WARNING: Setting this RESTRICTS functionality - leave empty during development

# Default web search behavior for tools that support it
# CLIENT_DEFAULTS_USE_WEBSEARCH=false
#   Purpose: Control whether web search is enabled by default
#   Default: false (web search must be explicitly requested)
#   Note: Individual tools may override this setting

# Default thinking mode for thinkdeep tool (minimal|low|medium|high|max)
# CLIENT_DEFAULT_THINKING_MODE=medium
#   Purpose: Set default reasoning depth for workflow tools
#   Default: medium
#   Options: minimal (0.5% of model max), low (8%), medium (33%), high (67%), max (100%)

# Maximum workflow steps for multi-step tools (0 = no limit)
# CLIENT_MAX_WORKFLOW_STEPS=0
#   Purpose: Limit the number of steps in workflow tools
#   Default: 0 (unlimited)
#   WARNING: Setting this RESTRICTS functionality - leave at 0 during development

# Legacy variables (deprecated - use CLIENT_* instead for clarity)
# The server maintains backward compatibility by falling back to CLAUDE_* variables if CLIENT_* are not set
# CLAUDE_TOOL_ALLOWLIST=
# CLAUDE_TOOL_DENYLIST=
# CLAUDE_DEFAULTS_USE_WEBSEARCH=false
# CLAUDE_DEFAULT_THINKING_MODE=medium
# CLAUDE_MAX_WORKFLOW_STEPS=0

# -------- Provider Gating (Advanced) --------
# These variables control which providers are available to the system
# DISABLED_PROVIDERS=
#   Purpose: Comma-separated list of providers to disable (e.g., KIMI,GLM)
#   Default: Empty (all providers enabled)
#   WARNING: Setting this RESTRICTS functionality - leave empty during development
#
# ALLOWED_PROVIDERS=
#   Purpose: Comma-separated list of providers to allow (empty = all allowed)
#   Default: Empty (all providers allowed)
#   WARNING: Setting this RESTRICTS functionality - leave empty during development
#
# Note: The system intentionally disables GOOGLE, OPENAI, XAI, DIAL providers
#       as this is a GLM/Kimi-only deployment. This is hardcoded in provider_detection.py.

# -------- Tool Gating (Advanced) --------
# DISABLED_TOOLS=
#   Purpose: Comma-separated list of tool names to completely disable
#   Default: Empty (all tools enabled)
#   WARNING: Setting this RESTRICTS functionality - leave empty during development
#
# Note: Tool visibility is controlled by TOOL_VISIBILITY in tools/registry.py
#       During development, all tools are set to 'core' or 'advanced' for full accessibility.

# -------- Feature Flags (All Enabled by Default) --------
# The following feature flags control major system capabilities.
# All are ENABLED by default for full system functionality.
#
# ROUTER_ENABLED=true
#   Purpose: Enable intelligent routing between GLM and Kimi providers
#   Default: true
#   Note: Disabling this restricts the system to single-provider mode
#
# ENABLE_INTELLIGENT_ROUTING=true
#   Purpose: Enable AI manager (GLM-4.5-flash) for intelligent task routing
#   Default: true
#   Note: Disabling this restricts the system to simple routing
#
# GLM_STREAM_ENABLED=true
#   Purpose: Enable streaming responses for GLM provider
#   Default: true
#   Note: Disabling this forces synchronous responses only
#
# KIMI_STREAM_ENABLED=true
#   Purpose: Enable streaming responses for Kimi provider
#   Default: true
#   Note: Disabling this forces synchronous responses only
#
# GLM_ENABLE_WEB_BROWSING=true
#   Purpose: Enable native web search for GLM provider
#   Default: true
#   Note: Disabling this removes web search capability from GLM
#
# KIMI_ENABLE_INTERNET_SEARCH=true
#   Purpose: Enable native web search for Kimi provider
#   Default: true
#   Note: Disabling this removes web search capability from Kimi
#
# EX_WEB_ENABLED=true
#   Purpose: Enable web search backend (DuckDuckGo)
#   Default: true
#   Note: Disabling this removes all web search capabilities

# -------- Model Pricing Configuration (Optional) --------
# Pricing in USD per million tokens
# Format: {"model-name": price_per_million, ...}
# MODEL_INPUT_PRICE_JSON={"glm-4.6": 0.60, "glm-4.5": 0.50, "glm-4.5-flash": 0.10, "glm-4.5-air": 0.10, "kimi-k2-0905-preview": 0.10}
# MODEL_OUTPUT_PRICE_JSON={"glm-4.6": 2.20, "glm-4.5": 2.00, "glm-4.5-flash": 0.30, "glm-4.5-air": 0.30, "kimi-k2-0905-preview": 0.10}

# -------- Notes --------
# - Only include secrets in your local .env (never commit).
# - Keep this template minimal; add variables here only when they are essential
#   to run the core MCP server, GLM provider, streaming, and the essential tools.
# - For DEVELOPMENT: Leave all restriction variables (DISABLED_*, ALLOWED_*, CLIENT_TOOL_*)
#   unset or empty to enable FULL system capabilities and unrestricted access to all tools.
