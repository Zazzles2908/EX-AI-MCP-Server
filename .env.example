# EX-AI MCP Server - Main Project Environment Configuration Template
# Copy this file to .env and fill in your API keys
# Keep .env out of version control
# Last Updated: 2025-10-09 (Phase 7 - .env Restructuring)

# ============================================================================
# CORE CONFIGURATION
# ============================================================================
DEFAULT_MODEL=glm-4.5-flash  # Default model for all tools (glm-4.5-flash recommended for speed)
ROUTER_ENABLED=true  # Enable intelligent model routing based on task complexity
GLM_ENABLE_WEB_BROWSING=true  # Enable GLM native web search capability

# ============================================================================
# EXPERT ANALYSIS CONFIGURATION
# ============================================================================
# Enable/disable expert analysis entirely (default: true with minimal thinking mode)
# Expert analysis adds 5-30s latency depending on thinking mode and prompt size
# With thinking_mode=minimal, expert analysis completes in ~5-7s
# Disable this to make workflow tools complete in <1 second (but lose AI analysis)
EXPERT_ANALYSIS_ENABLED=true

# Thinking mode for expert analysis when enabled (minimal, low, medium, high, max)
# - minimal: 0.5% model capacity, ~5-7s response time (DEFAULT for speed)
# - low: 8% model capacity, ~8-10s response time
# - medium: 33% model capacity, ~15-20s response time
# - high: 67% model capacity, ~25-30s response time
# - max: 100% model capacity, ~40-60s response time
# NOTE: Users can override this per-call by passing thinking_mode parameter
EXPERT_ANALYSIS_THINKING_MODE=minimal

# Auto-upgrade models for thinking mode support (default: true)
# When true, automatically upgrades models that don't support thinking mode:
#   - GLM: glm-4.5-flash → glm-4.6
#   - Kimi: kimi-k2-0905-preview → kimi-thinking-preview
# When false, keeps user-specified model even if it doesn't support thinking mode
# NOTE: Disabling may cause expert analysis to fail or hang with incompatible models
EXPERT_ANALYSIS_AUTO_UPGRADE=true

# Embed FULL file contents in expert analysis prompt (default: false)
# When true, embeds entire file contents into prompt, significantly increasing size and latency
# When false, only file paths/names are included in context (not full contents)
# NOTE: File paths/names are ALWAYS included regardless of this setting (Issue #9 clarification)
# Only enable if expert analysis truly needs to see full file contents
EXPERT_ANALYSIS_INCLUDE_FILES=false

# Maximum number of files to include in expert analysis (default: 20)
# Only applies when EXPERT_ANALYSIS_INCLUDE_FILES=true
# Prevents token bloat from embedding too many files (Issue #8)
EXPERT_ANALYSIS_MAX_FILE_COUNT=20

# Maximum file size to include in expert analysis (KB)
# Only applies when EXPERT_ANALYSIS_INCLUDE_FILES=true
# Prevents massive files from being embedded and causing timeouts
EXPERT_ANALYSIS_MAX_FILE_SIZE_KB=10

# Legacy settings (kept for backward compatibility)
DEFAULT_USE_ASSISTANT_MODEL=true
EXPERT_HEARTBEAT_INTERVAL_SECS=5

# ============================================================================
# WEBSOCKET DAEMON CONFIGURATION
# ============================================================================
# Alphabetically sorted for easy lookup

# Authentication (optional)
EXAI_WS_TOKEN=  # Optional authentication token for WebSocket connections

# Caching and deduplication
EXAI_WS_DISABLE_COALESCE_FOR_TOOLS=  # Comma-separated list of tools to disable request coalescing
EXAI_WS_INFLIGHT_TTL_SECS=180  # How long to cache in-flight requests (3 minutes)
EXAI_WS_RESULT_TTL=600  # How long to cache completed results (10 minutes)
EXAI_WS_RETRY_AFTER_SECS=1  # Delay before retrying failed requests

# Compatibility flags
EXAI_WS_COMPAT_TEXT=true  # Enable text compatibility mode for legacy clients
EX_ENSURE_NONEMPTY_FIRST=false  # Ensure first message is non-empty (legacy)

# Concurrency limits
EXAI_WS_GLM_MAX_INFLIGHT=4  # Max concurrent GLM requests per session
EXAI_WS_GLOBAL_MAX_INFLIGHT=24  # Max concurrent requests across all sessions
EXAI_WS_KIMI_MAX_INFLIGHT=6  # Max concurrent Kimi requests per session
EXAI_WS_SESSION_MAX_INFLIGHT=8  # Max concurrent requests per session

# Connection timeouts
EXAI_WS_HELLO_TIMEOUT=15  # Timeout for initial handshake (seconds)
EXAI_WS_PING_INTERVAL=45  # Ping interval (seconds) - lower = faster deadlock detection
EXAI_WS_PING_TIMEOUT=30  # Ping timeout (seconds)
EXAI_WS_PROGRESS_INTERVAL_SECS=8.0  # Progress update interval for long-running tasks

# Message size limits
EXAI_WS_MAX_BYTES=33554432  # Max message size (32 MiB) - increase for large file uploads

# Network configuration
EXAI_WS_HOST=127.0.0.1  # WebSocket server host (localhost only for security)
EXAI_WS_PORT=8079  # WebSocket server port

# Diagnostics
DIAGNOSTICS=false  # Enable diagnostic logging and snapshots (set to true for debugging)

# Logging
LOG_LEVEL=INFO  # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)

# ============================================================================
# KIMI API CONFIGURATION (Moonshot)
# ============================================================================
KIMI_API_KEY=  # Moonshot AI API key (get from https://platform.moonshot.cn)
KIMI_BASE_URL=https://api.moonshot.ai/v1  # Kimi API base URL (OpenAI-compatible)
KIMI_API_URL=https://api.moonshot.ai/v1  # Kimi API URL (same as base URL)

# ============================================================================
# GLM API CONFIGURATION (ZhipuAI)
# ============================================================================
# Using z.ai international endpoint (3x faster than China endpoint)
# API Endpoint: https://api.z.ai/api/paas/v4 (recommended - international)
# Documentation: https://open.bigmodel.cn/dev/api (docs site only, not for API calls)
GLM_API_KEY=  # ZhipuAI API key (get from https://open.bigmodel.cn)
GLM_BASE_URL=https://api.z.ai/api/paas/v4  # GLM API base URL (z.ai proxy, 3x faster)
GLM_API_URL=https://api.z.ai/api/paas/v4  # GLM API URL (same as base URL)

# Legacy vars for backward compatibility (same as GLM_* above)
ZHIPUAI_API_KEY=  # Legacy: same as GLM_API_KEY
ZHIPUAI_API_URL=https://api.z.ai/api/paas/v4  # Legacy: same as GLM_API_URL
ZHIPUAI_BASE_URL=https://api.z.ai/api/paas/v4  # Legacy: same as GLM_BASE_URL

# ============================================================================
# TIMEOUT CONFIGURATION (Coordinated Hierarchy)
# ============================================================================
# These timeouts follow a coordinated hierarchy to ensure proper timeout behavior:
# HTTP Client → Tool Level → Daemon Level → Shim Level → Client Level
# Rule: Each outer timeout = 1.5x inner timeout (50% buffer)
#
# CRITICAL: HTTP Client timeout is the foundation - must be >= workflow tool timeout
# This was the root cause of the "SDK hanging" issue (was 60s, needed 300s)
EX_HTTP_TIMEOUT_SECONDS=300  # HTTP client timeout (foundation of timeout hierarchy)

# Tool-level timeouts
SIMPLE_TOOL_TIMEOUT_SECS=60  # Timeout for simple tools (chat, listmodels, etc.)
WORKFLOW_TOOL_TIMEOUT_SECS=300  # Timeout for workflow tools (debug, analyze, etc.)
EXPERT_ANALYSIS_TIMEOUT_SECS=180  # Timeout for expert analysis validation (base timeout)

# Thinkdeep adaptive timeout (optional override)
# If not set, thinkdeep uses EXPERT_ANALYSIS_TIMEOUT_SECS with adaptive multipliers:
# - minimal: 0.5x base (e.g., 180s * 0.5 = 90s for quick validation)
# - low: 0.7x base (e.g., 180s * 0.7 = 126s for standard validation)
# - medium: 1.0x base (e.g., 180s * 1.0 = 180s for thorough analysis)
# - high: 1.5x base (e.g., 180s * 1.5 = 270s for deep analysis)
# - max: 2.0x base (e.g., 180s * 2.0 = 360s for exhaustive reasoning)
# Set this to override adaptive behavior with a fixed timeout for all thinking modes
# THINKDEEP_EXPERT_TIMEOUT_SECS=180  # Optional: Fixed timeout for thinkdeep (overrides adaptive)

# Provider-specific timeouts
GLM_TIMEOUT_SECS=90  # GLM API request timeout
KIMI_TIMEOUT_SECS=120  # Kimi API request timeout
KIMI_WEB_SEARCH_TIMEOUT_SECS=150  # Kimi web search timeout (longer for search)

# Infrastructure timeouts (auto-calculated by TimeoutConfig class in config.py)
# IMPORTANT: These are NOT environment variables and CANNOT be overridden in .env
# They are always calculated based on WORKFLOW_TOOL_TIMEOUT_SECS:
#   - Daemon timeout: 450s (1.5x WORKFLOW_TOOL_TIMEOUT_SECS)
#   - Shim timeout: 600s (2.0x WORKFLOW_TOOL_TIMEOUT_SECS)
#   - Client timeout: 750s (2.5x WORKFLOW_TOOL_TIMEOUT_SECS)
# To change these, modify WORKFLOW_TOOL_TIMEOUT_SECS above and they will auto-adjust.
# See: tool_validation_suite/docs/current/guides/TIMEOUT_CONFIGURATION_GUIDE.md

# ============================================================================
# CIRCUIT BREAKER CONFIGURATION
# ============================================================================
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5  # Number of failures before circuit opens
CIRCUIT_BREAKER_RECOVERY_TIMEOUT_SECS=300  # Time before attempting recovery (5 minutes)

# ============================================================================
# SESSION MANAGEMENT CONFIGURATION
# ============================================================================
SESSION_TIMEOUT_SECS=3600  # Session timeout (1 hour)
SESSION_MAX_CONCURRENT=100  # Maximum concurrent sessions
SESSION_CLEANUP_INTERVAL=300  # Session cleanup interval (5 minutes)

# ============================================================================
# FILE PATH HANDLING
# ============================================================================
# Allow relative file paths in tool requests (auto-resolved to absolute paths)
# EX_ALLOW_RELATIVE_PATHS=true  # Enable relative path resolution (disabled by default)

# ============================================================================
# KIMI (MOONSHOT) PROVIDER SETTINGS
# ============================================================================
AGENTIC_ENABLE_LOGGING=true  # Enable agentic logging for Kimi provider
KIMI_MAX_HEADER_LEN=4096  # Maximum header length for Kimi requests
KIMI_FILES_MAX_SIZE_MB=20  # Maximum file size for Kimi uploads (MB)
KIMI_DEFAULT_MODEL=kimi-k2-0905-preview  # Default Kimi model (balanced quality/speed)
KIMI_THINKING_MODEL=kimi-thinking-preview  # Kimi thinking model (extended reasoning)
KIMI_SPEED_MODEL=kimi-k2-turbo-preview  # Kimi speed model (fast responses)
KIMI_CACHE_TOKEN_TTL_SECS=1800  # Kimi cache token TTL (30 minutes)
KIMI_CACHE_TOKEN_LRU_MAX=256  # Kimi cache token LRU max size

# Model routing preferences (ordered by preference, comma-separated)
# Used by provider registry to select preferred model when multiple models are available
# First model in list that is available will be selected
KIMI_PREFERRED_MODELS=kimi-k2-0905-preview,kimi-k2-turbo-preview,kimi-k2-0711-preview  # Kimi model preference order
GLM_PREFERRED_MODELS=glm-4.5-flash,glm-4.6,glm-4.5,glm-4.5-air  # GLM model preference order

# ============================================================================
# EMBEDDINGS CONFIGURATION
# ============================================================================
# Provider selection: kimi, glm, or external
# - kimi: Use Kimi/Moonshot embeddings (OpenAI-compatible)
# - glm: Use GLM embeddings (ZhipuAI SDK, implemented 2025-10-09)
# - external: Use external embeddings service
EMBEDDINGS_PROVIDER=kimi  # Embeddings provider (kimi, glm, or external)

# Kimi embeddings model (OpenAI-compatible)
KIMI_EMBED_MODEL=text-embedding-3-large  # Kimi embeddings model (OpenAI-compatible)

# GLM embeddings model (implemented 2025-10-09 - Phase 5, fixed with ZAI SDK)
# IMPORTANT: Now uses ZAI SDK (zai-sdk) instead of zhipuai SDK
# The ZAI SDK works with z.ai proxy endpoint (3x faster than bigmodel.cn)
# Options: embedding-2 (1024 dims, recommended), embedding-3 (8192 dims)
GLM_EMBED_MODEL=embedding-2  # GLM embeddings model (embedding-2 or embedding-3)

# GLM embeddings endpoint (uses z.ai proxy with ZAI SDK)
# ZAI SDK (zai-sdk>=0.0.4) is designed for z.ai endpoints
# Endpoint: https://api.z.ai/api/paas/v4 (same as chat, 3x faster)
GLM_EMBEDDINGS_BASE_URL=https://api.z.ai/api/paas/v4  # GLM embeddings endpoint (z.ai proxy)

# GLM native web search configuration (2025-10-09 - Removed DuckDuckGo fallback)
# Documentation: https://docs.z.ai/api-reference/tools/web-search
# Endpoint: https://api.z.ai/api/paas/v4/web_search
GLM_WEBSEARCH_COUNT=10  # Number of search results to return
GLM_WEBSEARCH_ENGINE=search-prime  # Search engine (search-prime or search-pro)
GLM_WEBSEARCH_RECENCY=all  # Recency filter (oneDay, oneWeek, oneMonth, oneYear, all)
GLM_WEBSEARCH_TIMEOUT_SECS=30  # Web search timeout (seconds)
GLM_WEBSEARCH_DOMAIN_FILTER=  # Optional domain filter (e.g., "github.com")
GLM_ACCEPT_LANGUAGE=en-US,en  # Language preference for search results

# External embeddings service URL (only used when EMBEDDINGS_PROVIDER=external)
# EXTERNAL_EMBEDDINGS_URL=http://localhost:8080/embed  # External embeddings service URL

# ============================================================================
# TEST FILES DIRECTORY
# ============================================================================
# Absolute path(s) to folder(s) containing sample files for upload/extract tests
# Supports multiple roots separated by comma or semicolon
TEST_FILES_DIR=  # Test files directory (absolute path, e.g., C:\Project\EX-AI-MCP-Server\test_files)

# ============================================================================
# STREAMING CONFIGURATION
# ============================================================================
# Streaming provides real-time token-by-token response delivery for better UX
# Benefits: Lower perceived latency, immediate feedback, progressive display
# Currently only supported for 'chat' tool with GLM provider
GLM_STREAM_ENABLED=true  # Enable GLM streaming for chat tool (recommended for better UX)
KIMI_STREAM_ENABLED=false  # Enable Kimi streaming (not currently needed)
KIMI_STREAM_TIMEOUT_SECS=240  # Kimi streaming timeout (4 minutes)
KIMI_STREAM_PRIME_CACHE=false  # Prime Kimi cache before streaming
KIMI_EXTRACT_REASONING=true  # Extract reasoning_content from kimi-thinking-preview model (default: true)

# ============================================================================
# KIMI CHAT TOOL TIMEOUTS
# ============================================================================
# Kimi chat tool has separate timeouts for web-enabled and non-web requests
# Web search requests need longer timeouts due to search latency
KIMI_CHAT_TOOL_TIMEOUT_SECS=180  # Kimi chat timeout for non-web requests (3 minutes)
KIMI_CHAT_TOOL_TIMEOUT_WEB_SECS=300  # Kimi chat timeout for web search requests (5 minutes)

# ============================================================================
# CONVERSATION STORAGE CONFIGURATION
# ============================================================================
# Conversation threads enable multi-turn conversations with continuation_id parameter
#
# CRITICAL: Without Redis, conversations are stored in-memory and LOST on server restart!
# For production use, configure Redis for persistent conversation storage.
#
# In-Memory Storage (Development Only):
# - Conversations lost on restart
# - Conversations lost on crash
# - Not suitable for production
#
# Redis Storage (Production):
# - Conversations persist across restarts
# - Conversations survive crashes
# - Scalable and reliable
#
CONVERSATION_TIMEOUT_HOURS=24  # How long to keep conversation threads (default: 3 hours, recommended: 24 hours)
REDIS_URL=  # Redis URL for persistent storage (e.g., redis://localhost:6379/0)
# Leave REDIS_URL empty for in-memory storage (development only)
# Set REDIS_URL for persistent storage (production recommended)

# ============================================================================
# MESSAGE BUS CONFIGURATION (Phase 2A - Supabase Integration)
# ============================================================================
# Enable Supabase message bus for guaranteed message integrity
MESSAGE_BUS_ENABLED=false  # Enable Supabase message bus (experimental)
MESSAGE_BUS_TTL_HOURS=48  # Message TTL in hours (2 days)
MESSAGE_BUS_MAX_PAYLOAD_MB=100  # Maximum payload size (MB)
MESSAGE_BUS_COMPRESSION=gzip  # Compression algorithm (gzip, zstd, none)
MESSAGE_BUS_CHECKSUM_ENABLED=true  # Enable checksum validation

# ============================================================================
# SUPABASE CONFIGURATION
# ============================================================================
# Supabase project URL and API key (required when MESSAGE_BUS_ENABLED=true)
SUPABASE_URL=  # Supabase project URL (e.g., https://xxx.supabase.co)
SUPABASE_KEY=  # Supabase anon key (get from Supabase dashboard)
SUPABASE_PROJECT_ID=  # Supabase project ID (get from Supabase dashboard)

# ============================================================================
# CIRCUIT BREAKER CONFIGURATION (Supabase Fallback)
# ============================================================================
# Automatic fallback when Supabase is slow/unavailable
CIRCUIT_BREAKER_ENABLED=true  # Enable circuit breaker for Supabase
CIRCUIT_BREAKER_THRESHOLD=5  # Failure threshold before opening circuit
CIRCUIT_BREAKER_TIMEOUT_SECS=60  # Circuit breaker timeout (1 minute)
FALLBACK_TO_WEBSOCKET=true  # Fallback to WebSocket when Supabase unavailable

# ============================================================================
# ENVIRONMENT
# ============================================================================
ENVIRONMENT=development  # Environment (development, staging, production)

