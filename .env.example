# EX-AI MCP Server - Environment Template (clean, minimal)
# Copy this file to .env and fill in secrets/values. Keep .env out of VCS.

# -------- Core --------
# Default model to use for Chat and general tools
DEFAULT_MODEL=glm-4.5-flash

# Keep simple single-provider routing on; advanced agents are de-scoped
ROUTER_ENABLED=true

# WebSocket daemon configuration
EXAI_WS_HOST=127.0.0.1
EXAI_WS_PORT=8079
# LOG_LEVEL can be DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# -------- Providers (GLM + Kimi) --------
# GLM (ZhipuAI/Z.ai) access
# Prefer GLM_API_URL; aliases ZHIPUAI_API_URL and ZHIPUAI_BASE_URL are also read by the server
GLM_API_KEY=
GLM_API_URL=https://api.z.ai/api/paas/v4
ZHIPUAI_API_KEY=
ZHIPUAI_API_URL=https://open.bigmodel.cn/api/paas/v4
ZHIPUAI_BASE_URL=https://open.bigmodel.cn/api/paas/v4

# Kimi (Moonshot) access
# Prefer KIMI_API_URL; alias KIMI_BASE_URL is also read by the server
KIMI_API_KEY=
KIMI_API_URL=https://api.moonshot.ai/v1
KIMI_BASE_URL=https://api.moonshot.ai/v1

# -------- Streaming --------
# Provider-level streaming for GLM Chat (SSE/SDK aggregation)
# true: Chat tool requests streaming from GLM provider (env-gated)
# false: standard non-streaming responses
GLM_STREAM_ENABLED=false
# Provider-level streaming for Kimi Chat (text/event-stream)
KIMI_STREAM_ENABLED=false

# Kimi streaming timeout in seconds (applies to stream mode)
KIMI_STREAM_TIMEOUT_SECS=240
# Prime Kimi context-cache before streaming (best-effort; true/false)
KIMI_STREAM_PRIME_CACHE=false

# -------- Optional timeouts (sane defaults if unset) --------
# HTTP_CONNECT_TIMEOUT=10
# HTTP_READ_TIMEOUT=60
# HTTP_TOTAL_TIMEOUT=90

# -------- Notes --------
# - Only include secrets in your local .env (never commit).
# - Keep this template minimal; add variables here only when they are essential
#   to run the core MCP server, GLM provider, streaming, and the essential tools.
