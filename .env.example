# EX-AI MCP Server - Main Project Environment Configuration Template
# Copy this file to .env and fill in your API keys
# Keep .env out of version control

# ============================================================================
# CORE CONFIGURATION
# ============================================================================
DEFAULT_MODEL=glm-4.5-flash
ROUTER_ENABLED=true
GLM_ENABLE_WEB_BROWSING=true

# ============================================================================
# EXPERT ANALYSIS CONFIGURATION
# ============================================================================
# Enable/disable expert analysis entirely (default: true with minimal thinking mode)
# Expert analysis adds 5-30s latency depending on thinking mode and prompt size
# With thinking_mode=minimal, expert analysis completes in ~5-7s
# Disable this to make workflow tools complete in <1 second (but lose AI analysis)
EXPERT_ANALYSIS_ENABLED=true

# Thinking mode for expert analysis when enabled (minimal, low, medium, high, max)
# - minimal: 0.5% model capacity, ~5-7s response time (DEFAULT for speed)
# - low: 8% model capacity, ~8-10s response time
# - medium: 33% model capacity, ~15-20s response time
# - high: 67% model capacity, ~25-30s response time
# - max: 100% model capacity, ~40-60s response time
# NOTE: Users can override this per-call by passing thinking_mode parameter
EXPERT_ANALYSIS_THINKING_MODE=minimal

# Include files in expert analysis prompt (default: false)
# When true, embeds entire file contents into prompt, significantly increasing size and latency
# Only enable if expert analysis truly needs to see file contents
EXPERT_ANALYSIS_INCLUDE_FILES=false

# Maximum file size to include in expert analysis (KB)
# Only applies when EXPERT_ANALYSIS_INCLUDE_FILES=true
# Prevents massive files from being embedded and causing timeouts
EXPERT_ANALYSIS_MAX_FILE_SIZE_KB=10

# Legacy settings (kept for backward compatibility)
DEFAULT_USE_ASSISTANT_MODEL=true
EXPERT_HEARTBEAT_INTERVAL_SECS=5

# ============================================================================
# WEBSOCKET DAEMON CONFIGURATION
# ============================================================================
# Alphabetically sorted for easy lookup

# Authentication (optional)
EXAI_WS_TOKEN=

# Caching and deduplication
EXAI_WS_DISABLE_COALESCE_FOR_TOOLS=
EXAI_WS_INFLIGHT_TTL_SECS=180
EXAI_WS_RESULT_TTL=600
EXAI_WS_RETRY_AFTER_SECS=1

# Compatibility flags
EXAI_WS_COMPAT_TEXT=true
EX_ENSURE_NONEMPTY_FIRST=false

# Concurrency limits
EXAI_WS_GLM_MAX_INFLIGHT=4
EXAI_WS_GLOBAL_MAX_INFLIGHT=24
EXAI_WS_KIMI_MAX_INFLIGHT=6
EXAI_WS_SESSION_MAX_INFLIGHT=8

# Connection timeouts
EXAI_WS_HELLO_TIMEOUT=15
EXAI_WS_PING_INTERVAL=45  # seconds; lower = faster deadlock detection, higher = less traffic
EXAI_WS_PING_TIMEOUT=30
EXAI_WS_PROGRESS_INTERVAL_SECS=8.0

# Message size limits
EXAI_WS_MAX_BYTES=33554432  # 32 MiB - raise if you upload large files

# Network configuration
EXAI_WS_HOST=127.0.0.1
EXAI_WS_PORT=8079

# Diagnostics
DIAGNOSTICS=false

# Logging
LOG_LEVEL=INFO

# ============================================================================
# KIMI API CONFIGURATION (Moonshot)
# ============================================================================
KIMI_API_KEY=
KIMI_BASE_URL=https://api.moonshot.ai/v1
KIMI_API_URL=https://api.moonshot.ai/v1

# ============================================================================
# GLM API CONFIGURATION (ZhipuAI)
# ============================================================================
# Using z.ai proxy (3x faster than open.bigmodel.cn according to user testing)
# Alternative: https://open.bigmodel.cn/api/paas/v4 (official but slower)
GLM_API_KEY=
GLM_BASE_URL=https://api.z.ai/api/paas/v4
GLM_API_URL=https://api.z.ai/api/paas/v4

# Legacy vars for backward compatibility (same as GLM_* above)
ZHIPUAI_API_KEY=
ZHIPUAI_API_URL=https://api.z.ai/api/paas/v4
ZHIPUAI_BASE_URL=https://api.z.ai/api/paas/v4

# ============================================================================
# TIMEOUT CONFIGURATION (Coordinated Hierarchy)
# ============================================================================
# These timeouts follow a coordinated hierarchy to ensure proper timeout behavior:
# HTTP Client → Tool Level → Daemon Level → Shim Level → Client Level
# Rule: Each outer timeout = 1.5x inner timeout (50% buffer)
#
# CRITICAL: HTTP Client timeout is the foundation - must be >= workflow tool timeout
# This was the root cause of the "SDK hanging" issue (was 60s, needed 300s)
EX_HTTP_TIMEOUT_SECONDS=300

# Tool-level timeouts
SIMPLE_TOOL_TIMEOUT_SECS=60
WORKFLOW_TOOL_TIMEOUT_SECS=300
EXPERT_ANALYSIS_TIMEOUT_SECS=180

# Provider-specific timeouts
GLM_TIMEOUT_SECS=90
KIMI_TIMEOUT_SECS=120
KIMI_WEB_SEARCH_TIMEOUT_SECS=150

# Infrastructure timeouts (auto-calculated by TimeoutConfig class in config.py)
# IMPORTANT: These are NOT environment variables and CANNOT be overridden in .env
# They are always calculated based on WORKFLOW_TOOL_TIMEOUT_SECS:
#   - Daemon timeout: 450s (1.5x WORKFLOW_TOOL_TIMEOUT_SECS)
#   - Shim timeout: 600s (2.0x WORKFLOW_TOOL_TIMEOUT_SECS)
#   - Client timeout: 750s (2.5x WORKFLOW_TOOL_TIMEOUT_SECS)
# To change these, modify WORKFLOW_TOOL_TIMEOUT_SECS above and they will auto-adjust.
# See: tool_validation_suite/docs/current/guides/TIMEOUT_CONFIGURATION_GUIDE.md

# ============================================================================
# CIRCUIT BREAKER CONFIGURATION
# ============================================================================
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT_SECS=300

# ============================================================================
# SESSION MANAGEMENT CONFIGURATION
# ============================================================================
SESSION_TIMEOUT_SECS=3600
SESSION_MAX_CONCURRENT=100
SESSION_CLEANUP_INTERVAL=300

# ============================================================================
# FILE PATH HANDLING
# ============================================================================
# Allow relative file paths in tool requests (auto-resolved to absolute paths)
# EX_ALLOW_RELATIVE_PATHS=true

# ============================================================================
# KIMI (MOONSHOT) PROVIDER SETTINGS
# ============================================================================
AGENTIC_ENABLE_LOGGING=true
KIMI_MAX_HEADER_LEN=4096
KIMI_FILES_MAX_SIZE_MB=20
KIMI_DEFAULT_MODEL=kimi-k2-0905-preview
KIMI_THINKING_MODEL=kimi-thinking-preview
KIMI_SPEED_MODEL=kimi-k2-turbo-preview
KIMI_CACHE_TOKEN_TTL_SECS=1800
KIMI_CACHE_TOKEN_LRU_MAX=256

# ============================================================================
# EMBEDDINGS CONFIGURATION
# ============================================================================
# Provider selection: kimi, glm, or external
# - kimi: Use Kimi/Moonshot embeddings (OpenAI-compatible, recommended)
# - glm: Use GLM embeddings (not yet implemented)
# - external: Use external embeddings service
EMBEDDINGS_PROVIDER=kimi

# Kimi embeddings model (OpenAI-compatible)
KIMI_EMBED_MODEL=text-embedding-3-large

# GLM embeddings model (not yet implemented - placeholder for future)
# When implemented, will use same base_url as GLM chat: https://api.z.ai/api/paas/v4
# GLM_EMBED_MODEL=embedding-2

# External embeddings service URL (only used when EMBEDDINGS_PROVIDER=external)
# EXTERNAL_EMBEDDINGS_URL=http://localhost:8080/embed

# ============================================================================
# TEST FILES DIRECTORY
# ============================================================================
# Absolute path(s) to folder(s) containing sample files for upload/extract tests
# Supports multiple roots separated by comma or semicolon
TEST_FILES_DIR=

# ============================================================================
# STREAMING CONFIGURATION
# ============================================================================
GLM_STREAM_ENABLED=false
KIMI_STREAM_ENABLED=false
KIMI_STREAM_TIMEOUT_SECS=240
KIMI_STREAM_PRIME_CACHE=false

# ============================================================================
# MESSAGE BUS CONFIGURATION (Phase 2A - Supabase Integration)
# ============================================================================
# Enable Supabase message bus for guaranteed message integrity
MESSAGE_BUS_ENABLED=false
MESSAGE_BUS_TTL_HOURS=48
MESSAGE_BUS_MAX_PAYLOAD_MB=100
MESSAGE_BUS_COMPRESSION=gzip
MESSAGE_BUS_CHECKSUM_ENABLED=true

# ============================================================================
# SUPABASE CONFIGURATION
# ============================================================================
# Supabase project URL and API key (required when MESSAGE_BUS_ENABLED=true)
SUPABASE_URL=
SUPABASE_KEY=
SUPABASE_PROJECT_ID=

# ============================================================================
# CIRCUIT BREAKER CONFIGURATION
# ============================================================================
# Automatic fallback when Supabase is slow/unavailable
CIRCUIT_BREAKER_ENABLED=true
CIRCUIT_BREAKER_THRESHOLD=5
CIRCUIT_BREAKER_TIMEOUT_SECS=60
FALLBACK_TO_WEBSOCKET=true

# ============================================================================
# ENVIRONMENT
# ============================================================================
ENVIRONMENT=development

