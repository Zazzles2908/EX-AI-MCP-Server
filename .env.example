# EX-AI MCP Server - Environment Template (clean, minimal)
# Copy this file to .env and fill in secrets/values. Keep .env out of VCS.

# -------- Core --------
# Default model to use for Chat and general tools
DEFAULT_MODEL=glm-4.5-flash

# Keep simple single-provider routing on; advanced agents are de-scoped
ROUTER_ENABLED=true
GLM_ENABLE_WEB_BROWSING=true

# WebSocket daemon configuration
EXAI_WS_HOST=127.0.0.1
EXAI_WS_PORT=8079
# LOG_LEVEL can be DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# -------- Providers (GLM + Kimi) --------
# GLM (ZhipuAI/Z.ai) access
# Prefer GLM_API_URL; aliases ZHIPUAI_API_URL and ZHIPUAI_BASE_URL are also read by the server
GLM_API_KEY=
GLM_API_URL=https://api.z.ai/api/paas/v4
ZHIPUAI_API_KEY=
ZHIPUAI_API_URL=https://open.bigmodel.cn/api/paas/v4
ZHIPUAI_BASE_URL=https://open.bigmodel.cn/api/paas/v4

# Kimi (Moonshot) access
# Prefer KIMI_API_URL; alias KIMI_BASE_URL is also read by the server
KIMI_API_KEY=
KIMI_API_URL=https://api.moonshot.ai/v1
KIMI_BASE_URL=https://api.moonshot.ai/v1


# -------- Kimi (Moonshot) Provider Settings --------
# Enable Kimi internet search tool-use when supported by the model
KIMI_ENABLE_INTERNET_SEARCH=true
# Schema for tool-use on /chat/completions: function | builtin | native | both
# Use 'function' for OpenAI-compatible endpoint compliance
KIMI_WEBSEARCH_SCHEMA=function
# Cap header size to avoid 400 Request Header Too Large (NGINX/gateway)
KIMI_MAX_HEADER_LEN=4096
# Client-side file upload guardrail (MB)
KIMI_FILES_MAX_SIZE_MB=20
# Preferred Kimi models (used by boundary auto-router in request handler)
KIMI_DEFAULT_MODEL=kimi-k2-0711-preview
KIMI_THINKING_MODEL=kimi-thinking-preview
KIMI_SPEED_MODEL=kimi-k2-turbo-preview
# Context-cache controls (attach/reuse tokens via headers)
KIMI_CACHE_TOKEN_TTL_SECS=1800
KIMI_CACHE_TOKEN_LRU_MAX=256
# Optional provider timeouts (set only if you need overrides)
KIMI_CONNECT_TIMEOUT_SECS=10
KIMI_READ_TIMEOUT_SECS=300
KIMI_WRITE_TIMEOUT_SECS=60
KIMI_POOL_TIMEOUT_SECS=10
KIMI_DEFAULT_READ_TIMEOUT_SECS=300


# -------- Test Files (Local) --------
# Absolute path(s) to folder(s) containing sample files for upload/extract tests
# Supports multiple roots separated by comma or semicolon (e.g., C:\A;C:\B or C:\A,C:\B)
# Example (Windows): C:\Project\EX-AI-MCP-Server\test_files
# Example (Unix): /home/you/Project/EX-AI-MCP-Server/test_files
TEST_FILES_DIR=

# -------- Streaming --------
# Provider-level streaming for GLM Chat (SSE/SDK aggregation)
# true: Chat tool requests streaming from GLM provider (env-gated)
# false: standard non-streaming responses
GLM_STREAM_ENABLED=false
# Provider-level streaming for Kimi Chat (text/event-stream)
KIMI_STREAM_ENABLED=false

# Kimi streaming timeout in seconds (applies to stream mode)
KIMI_STREAM_TIMEOUT_SECS=240
# Prime Kimi context-cache before streaming (best-effort; true/false)
KIMI_STREAM_PRIME_CACHE=false

# -------- Optional timeouts (sane defaults if unset) --------
# HTTP_CONNECT_TIMEOUT=10
# HTTP_READ_TIMEOUT=60
# HTTP_TOTAL_TIMEOUT=90

# -------- Notes --------
# - Only include secrets in your local .env (never commit).
# - Keep this template minimal; add variables here only when they are essential
#   to run the core MCP server, GLM provider, streaming, and the essential tools.
