# EX-AI MCP Server - Environment Template (clean, minimal)
# Copy this file to .env and fill in secrets/values. Keep .env out of VCS.

# -------- Core --------
# Default model to use for Chat and general tools
DEFAULT_MODEL=glm-4.5-flash

# Keep simple single-provider routing on; advanced agents are de-scoped
ROUTER_ENABLED=true

# WebSocket daemon configuration
EXAI_WS_HOST=127.0.0.1
EXAI_WS_PORT=8079
# LOG_LEVEL can be DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# -------- Providers (GLM + Kimi) --------
# GLM (ZhipuAI/Z.ai) access
ZHIPUAI_API_KEY=
ZHIPUAI_BASE_URL=https://open.bigmodel.cn/api/paas/v4

# Kimi (Moonshot) access
KIMI_API_KEY=
KIMI_BASE_URL=https://api.moonshot.ai/v1

# -------- Streaming --------
# Provider-level streaming for GLM Chat (SSE/SDK aggregation)
# true: Chat tool requests streaming from GLM provider
# false: standard non-streaming responses
GLM_STREAM_ENABLED=false

# -------- Optional timeouts (sane defaults if unset) --------
# HTTP_CONNECT_TIMEOUT=10
# HTTP_READ_TIMEOUT=60
# HTTP_TOTAL_TIMEOUT=90

# -------- Notes --------
# - Only include secrets in your local .env (never commit).
# - Keep this template minimal; add variables here only when they are essential
#   to run the core MCP server, GLM provider, streaming, and the essential tools.
